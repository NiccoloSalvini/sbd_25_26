[["index.html", "Statistics &amp; Big Data 25-26 Labs Chapter 1  About 1.1 🔧 Logistics 1.2 👥 Team 1.3 🗒 labs’ contents 1.4 Exam 📝 1.5 📚 Suggested reading list 1.6 📜 Honor Code 1.7 QR code time! 1.8 Colophon", " Statistics &amp; Big Data 25-26 Labs Dr. Niccolò Salvini 2025-10-21 Chapter 1  About This is the official course website for Statistics &amp; Big data2 2025 - 2026 for laboratories. This website augments lecture topics and provides exercises for home and class assignments. Additional theory wrt slides and textbook will not be part of the exam, indeed they are for your growth and hopefully in the future as quicksilver resource to recover R proficiency from lethargy. 1.1 🔧 Logistics Lectures: Mondays: 14:00 - 17:00 CET Tuesdays: 10:00 - 13:00 CET Location: Campus Gemelli, Room 20 (and optionally remote) Office hours: Dr. Niccolò Salvini: Available via email and for questions before/after lectures Prof. Sophie Dabo-Niang: Available during intensive session (week of November 17th) here’s the shared drive (slides notebooks extra in class) 1.2 👥 Team .instructor { display: inline-block; width: 160px; text-align: center; margin-right: 20px; margin-bottom: 10px; font-size: 17px; vertical-align: top; } .instructorphoto img { width: 150px; border-radius: 140px; margin-bottom: 10px; } .col-md-2, .col-md-6 { position: relative; min-height: 1px; padding-right: 15px; padding-left: 15px; } Instructor Dr. Niccolò Salvini Instructor Prof. Sophie Dabo-Niang 1.3 🗒 labs’ contents 1.3.1 Part 1: The Foundations (Dr. Niccolò Salvini) Introduction to the R ecosystem Install R and RStudio R tricks for research and professional life Data wrangling with R Hypothesis Testing Fundamentals Alternative hypothesis testing How to calculate p-values Hypothesis testing with null hypothesis Hypothesis testing on averages Analysis of Variance (ANOVA) Testing more than 2 means Chi-Square Tests Testing more than 2 proportions Linear Regression Analysis Simple linear regression Multiple linear regression Nonlinear regression Regression with dummy variables Logistic Regression Introduction to logistic regression 1.3.2 Part 2: Advanced Modeling (Prof. Sophie Dabo-Niang) Factor Analysis Cluster Analysis Discrimination &amp; Classification Binomial &amp; Multinomial Logistic Regression Kernel Methods General Additive Models Other Supervised Models 1.4 Exam 📝 The exam is going to be open and closed questions on theory and practice (coding part). You will be asked to provide results and sometimes code leading to these results. You can also be asked to directly provide code to solve for that exercise. The exam is going to take place in labs classroom, this means you are not going to have your laptop during the exam. We generally don’t provide assignment neither group works. Indeed we provide intermediate exams for those who want to try them. we are going to have 2 intermediate sessions exams on half of the whole content of the course. This means: - first intermediate: will happen typically on November and will be on Part 1 content (Dr. Salvini’s part) - second intermediate: will happen in January/February on Part 2 content (Prof. Dabo-Niang’s part) you can take first intermediate and take the second on each exam date within the winter session, meaning you take part 1 in Nov and part 2 in either Jan and Feb. You can not reject intermediates, that means if you take first part, try the second and did not perform well, you need to take the full. Grades may undergo to a review process before being official if they are particularly low. This has happened quite often, but it does not happen every time. 1.5 📚 Suggested reading list I am going to split resources by the expected level of their audience: 1.5.1 Minimal or 0 knowledge of R Everitt, B., Hothorn, T. (2011) An Introduction to Applied Multivariate Analysis with R, Springer-Verlag James, G, Witten, D, Hastie, T and Tibshirani, R, (2015) An Introduction to Statistical Learning, with Applications in R T. Timbers, T. Campbell, M. Lee Data Science: A First Introduction, Jul 2022 online version Wickham, H., Grolemund G. (2018) R for Data Science, O’Reilly. Freely available on-line at https://r4ds.had.co.nz/index.html R for non-programmers, Daniel Dauber 2022, free book 1.5.2 Advanced knowledge of R to become a top G Reproducible Medical Research with R,Peter D.R. Higgins, MD, PhD, MSc, 2022, free book Fundamentals of Wrangling Healthcare Data with R, J. Kyle Armstrong 2022, free book Advanced Wickham H. (2015). Advanced r. CRC Press free book 1.6 📜 Honor Code Permissive but strict. If unsure, please ask the course staff! NOT OKAY Pleeease, I am using ChatGPT and its derivatives on a daily basis. I understand, it’s awesome and we are not enforcing any rule against it a home. Don’t do that during the exam. OK to search, ask in public about the systems we’re studying. Cite all the resources you reference. E.g. if you read it in a paper, cite it. If you ask on Quora, include the link. NOT OKAY to ask someone to do assignments/projects for you, we are monitoring freelancing websites, we have a plethora of bots doing this job daily. OK to discuss questions with classmates. Disclose your discussion partners. NOT OKAY to blindly copy solutions from classmates. OK to use existing solutions as part of your projects/assignments. Clarify your contributions. NOT OKAY to pretend that someone’s solution is yours. OK to publish your final project after the course is over (we encourage that and if you need it I would love to help you!) NOT OKAY to post your assignment solutions online. 1.7 QR code time! 1.8 Colophon This book was authored using bookdown inside RStudio with bs4 theme The website is hosted with Netlify, and automatically updated after Netlify CI. The complete source is available from GitHub. This version of the book was built with: library(devtools) #&gt; Loading required package: usethis library(roxygen2) library(testthat) #&gt; #&gt; Attaching package: &#39;testthat&#39; #&gt; The following object is masked from &#39;package:devtools&#39;: #&gt; #&gt; test_file #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; matches devtools::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 4.2.0 (2022-04-22) #&gt; os macOS 15.6.1 #&gt; system aarch64, darwin20 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz Europe/Rome #&gt; date 2025-10-21 #&gt; pandoc 3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────────── #&gt; package * version date (UTC) lib source #&gt; bookdown 0.29 2022-09-12 [1] CRAN (R 4.2.0) #&gt; brio 1.1.3 2021-11-30 [1] CRAN (R 4.2.0) #&gt; bslib 0.5.1 2023-08-11 [1] CRAN (R 4.2.0) #&gt; cachem 1.0.8 2023-05-01 [1] CRAN (R 4.2.0) #&gt; callr 3.7.3 2022-11-02 [1] CRAN (R 4.2.0) #&gt; cli 3.6.2 2023-12-11 [1] CRAN (R 4.2.3) #&gt; crayon 1.5.2 2022-09-29 [1] CRAN (R 4.2.0) #&gt; devtools * 2.4.5 2022-10-11 [1] CRAN (R 4.2.0) #&gt; dichromat 2.0-0.1 2022-05-02 [1] CRAN (R 4.2.0) #&gt; digest 0.6.33 2023-07-07 [1] CRAN (R 4.2.0) #&gt; dplyr * 1.1.4 2023-11-17 [1] CRAN (R 4.2.3) #&gt; ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.2.0) #&gt; evaluate 1.0.3 2025-01-10 [1] CRAN (R 4.2.0) #&gt; fansi 1.0.4 2023-01-22 [1] CRAN (R 4.2.0) #&gt; farver 2.1.1 2022-07-06 [1] CRAN (R 4.2.0) #&gt; fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.2.0) #&gt; fs 1.6.3 2023-07-20 [1] CRAN (R 4.2.0) #&gt; generics 0.1.3 2022-07-05 [1] CRAN (R 4.2.0) #&gt; glue * 1.6.2 2022-02-24 [1] CRAN (R 4.2.0) #&gt; htmltools 0.5.6.1 2023-10-06 [1] CRAN (R 4.2.0) #&gt; htmlwidgets 1.6.2 2023-03-17 [1] CRAN (R 4.2.0) #&gt; httpuv 1.6.6 2022-09-08 [1] CRAN (R 4.2.0) #&gt; httr 1.4.6 2023-05-08 [1] CRAN (R 4.2.0) #&gt; jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.2.0) #&gt; jsonlite 1.8.7 2023-06-29 [1] CRAN (R 4.2.0) #&gt; kableExtra * 1.3.4.9000 2023-06-01 [1] Github (kupietz/kableExtra@3bf9b21) #&gt; knitr * 1.44 2023-09-11 [1] CRAN (R 4.2.0) #&gt; later 1.3.0 2021-08-18 [1] CRAN (R 4.2.0) #&gt; lifecycle 1.0.3 2022-10-07 [1] CRAN (R 4.2.0) #&gt; lubridate * 1.9.2 2023-02-10 [1] CRAN (R 4.2.0) #&gt; magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.2.0) #&gt; memoise 2.0.1 2021-11-26 [1] CRAN (R 4.2.0) #&gt; mime 0.12 2021-09-28 [1] CRAN (R 4.2.0) #&gt; miniUI 0.1.1.1 2018-05-18 [1] CRAN (R 4.2.0) #&gt; pillar 1.9.0 2023-03-22 [1] CRAN (R 4.2.0) #&gt; pkgbuild 1.4.2 2023-06-26 [1] CRAN (R 4.2.0) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.2.0) #&gt; pkgload 1.4.0 2024-06-28 [1] CRAN (R 4.2.0) #&gt; prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.2.0) #&gt; processx 3.8.4 2024-03-16 [1] CRAN (R 4.2.3) #&gt; profvis 0.3.8 2023-05-02 [1] CRAN (R 4.2.0) #&gt; promises 1.2.0.1 2021-02-11 [1] CRAN (R 4.2.0) #&gt; ps 1.7.5 2023-04-18 [1] CRAN (R 4.2.0) #&gt; purrr 1.0.2 2023-08-10 [1] CRAN (R 4.2.0) #&gt; R6 2.5.1 2021-08-19 [1] CRAN (R 4.2.0) #&gt; RColorBrewer 1.1-3 2022-04-03 [1] CRAN (R 4.2.0) #&gt; Rcpp 1.0.12 2024-01-09 [1] CRAN (R 4.2.3) #&gt; remotes 2.4.2 2021-11-30 [1] CRAN (R 4.2.0) #&gt; rlang 1.1.3 2024-01-10 [1] CRAN (R 4.2.3) #&gt; rmarkdown 2.25 2023-09-18 [1] CRAN (R 4.2.0) #&gt; roxygen2 * 7.3.1 2024-01-22 [1] CRAN (R 4.2.3) #&gt; rstudioapi 0.14 2022-08-22 [1] CRAN (R 4.2.0) #&gt; rvest 1.0.3 2022-08-19 [1] CRAN (R 4.2.0) #&gt; sass 0.4.6 2023-05-03 [1] CRAN (R 4.2.0) #&gt; scales 1.4.0 2025-04-24 [1] CRAN (R 4.2.0) #&gt; sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.2.0) #&gt; shiny 1.7.2 2022-07-19 [1] CRAN (R 4.2.0) #&gt; stringi 1.7.12 2023-01-11 [1] CRAN (R 4.2.0) #&gt; stringr 1.5.0 2022-12-02 [1] CRAN (R 4.2.0) #&gt; svglite 2.1.1 2023-01-10 [1] CRAN (R 4.2.0) #&gt; systemfonts 1.0.4 2022-02-11 [1] CRAN (R 4.2.0) #&gt; testthat * 3.2.1.1 2024-04-14 [1] CRAN (R 4.2.3) #&gt; tibble 3.2.1 2023-03-20 [1] CRAN (R 4.2.0) #&gt; tidyselect 1.2.0 2022-10-10 [1] CRAN (R 4.2.0) #&gt; timechange 0.2.0 2023-01-11 [1] CRAN (R 4.2.0) #&gt; urlchecker 1.0.1 2021-11-30 [1] CRAN (R 4.2.0) #&gt; usethis * 2.1.6 2022-05-25 [1] CRAN (R 4.2.0) #&gt; utf8 1.2.3 2023-01-31 [1] CRAN (R 4.2.0) #&gt; vctrs 0.6.5 2023-12-01 [1] CRAN (R 4.2.3) #&gt; viridisLite 0.4.2 2023-05-02 [1] CRAN (R 4.2.0) #&gt; webexercises * 1.0.0 2021-09-15 [1] CRAN (R 4.2.0) #&gt; webshot 0.5.4 2022-09-26 [1] CRAN (R 4.2.0) #&gt; xfun 0.40 2023-08-09 [1] CRAN (R 4.2.0) #&gt; xml2 1.3.4 2023-04-27 [1] CRAN (R 4.2.0) #&gt; xtable 1.8-4 2019-04-21 [1] CRAN (R 4.2.0) #&gt; yaml 2.3.7 2023-01-23 [1] CRAN (R 4.2.0) #&gt; #&gt; [1] /Users/niccolo/Library/R/arm64/4.2/library #&gt; [2] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library #&gt; #&gt; ────────────────────────────────────────────────────────────────────────────────── UCSC?↩︎ "],["prereq.html", "Chapter 2 ✨Prerequisites 2.1 Setting up R and RStudio 2.2 How to Download and Install R 2.3 Using R 2.4 Using RStudio 2.5 When you first start RStudio 2.6 Updating R and RStudio: Living at the pulse of innovation", " Chapter 2 ✨Prerequisites 2.1 Setting up R and RStudio To get started with R, you need to acquire your own copy. This appendix will show you how to download R as well as RStudio, a software application that makes R easier to use. You’ll go from downloading R to opening your first R session. Use the menu on the right hand side of the page to select your OS and follow the correct installation. Both R and RStudio are free and easy to download. If you feel more comfortable interacting with videos instead of reading please visit this interactive tutorials that will guide you through the full R set up! 2.2 How to Download and Install R R is maintained by an international team of developers who make the language available through the web page of The Comprehensive R Archive Network i.e. CRAN. The top of the web page provides three links for downloading R. Follow the link that describes your operating system: Windows, Mac, or Linux. 2.2.1 R in Windows To install R on Windows, click the “Download R for Windows” link. Then click the “base” link. Next, click the first link at the top of the new page. This link should say something like “Download R 3.0.3 for Windows,” except the 3.0.3 will be replaced by the most current version of R. The link downloads an installer program, which installs the most up-to-date version of R for Windows. Run this program and step through the installation wizard that appears. The wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine. (more detailed steps) 2.2.2 R in Mac This is also my setup, feel free to reach me at my mail address if something is messed up. Go to www.r-project.org Figure 2.1: The R mirrors website Click on CRAN where it says Download. Choose a server in your country (all of them work, but downloads will perform quicker if you choose your country or one that is close to where you are). Figure 2.2: CRAN mirrors Select the operating system for your computer, for example Download R for macOS. Figure 2.3: OS choices available Select the version you want to install (I recommend the latest version) Figure 2.4: R versions available Open the downloaded file and follow the installation instructions. I recommend leaving the suggested settings as they are. Binaries Versus Source R can be installed from precompiled binaries or built from source on any operating system. For Windows and Mac machines, installing R from binaries is extremely easy. The binary comes preloaded in its own installer. Although you can build R from source on these platforms, the process is much more complicated and won’t provide much benefit for most users. For Linux systems, the opposite is true. Precompiled binaries can be found for some systems, but it is much more common to build R from source files when installing on Linux. The download pages on CRAN’s website provide information about building R from source for the Windows, Mac, and Linux platforms. 2.2.3 R in Linux R comes preinstalled on many Linux systems, but you’ll want the newest version of R if yours is out of date. The CRAN website provides files to build R from source on Debian, Redhat, SUSE, and Ubuntu systems under the link “Download R for Linux.” Click the link and then follow the directory trail to the version of Linux you wish to install on. The exact installation procedure will vary depending on the Linux system you use. CRAN guides the process by grouping each set of source files with documentation or README files that explain how to install on your system. 32-bit Versus 64-bit R comes in both 32-bit and 64-bit versions. Which should you use? In most cases, it won’t matter. Both versions use 32-bit integers, which means they compute numbers to the same numerical precision. The difference occurs in the way each version manages memory. 64-bit R uses 64-bit memory pointers, and 32-bit R uses 32-bit memory pointers. This means 64-bit R has a larger memory space to use (and search through). As a rule of thumb, 32-bit builds of R are faster than 64-bit builds, though not always. On the other hand, 64-bit builds can handle larger files and data sets with fewer memory management problems. In either version, the maximum allowable vector size tops out at around 2 billion elements. If your operating system doesn’t support 64-bit programs, or your RAM is less than 4 GB, 32-bit R is for you. The Windows and Mac installers will automatically install both versions if your system supports 64-bit R. 2.3 Using R R isn’t a program that you can open and start using, like Microsoft Word or Internet Explorer. Instead, R is a computer language, like C, C++, or UNIX. You use R by writing commands in the R language and asking your computer to interpret them. In the old days, people ran R code in a UNIX terminal window—as if they were hackers in a movie from the 1980s. Now almost everyone uses R with an application called RStudio, and I recommend that you do, too. R and UNIX You can still run R in a UNIX or BASH window (prompt or Powershell) by typing the command: R which opens an R interpreter. You can then do your work and close the interpreter by running q() when you are finished. 2.4 Using RStudio R by itself is just the ‘beating heart’ of R programming, but it has no particular user interface. You may have heard me saying stuff like: “R is the engine of the car, indeed RStudio is the car body, that’s true, you just don’t need an engine if you don’t have a car body. That is to say: if you want buttons to click and actually ‘see’ what you are doing, there is no better way than RStudio. RStudio is an integrated development environment (IDE) and will be our primary tool to interact with R. It is the only software you need to do all the fun parts and, of course, to follow along with the examples of this book. You may ask yourself what is Posit, fair question. Back in the days Posit, the company behind RStudio, was actually named RStudio (as their product). Then in 2023 they rebranded themselves as Posit to also include other languages like Python, Howeveeeer to install RStudio perform the following steps: Go to https://posit.co/ Figure 2.5: The Posit.co main page Go to DOWNLOAD RSTUDIO in the upper right corner (download R if you still haven’t). Select DOWNLOAD RSTUDIO, just on the left of DOWNLOAD RSTUDIO SERVER. Figure 2.6: Choose RStudio version On this page, scroll down and select the Download (in the download column) corresponding to your OS (mind that different versions of the same OS, say macOS 11.2 or macOS 8.3 need different RStudio download installations). Figure 2.7: Choose RStudio version Open the downloaded file and follow the installation instructions. Again, keep it to the default settings as much as possible. Congratulations, you are all set up to learn R. From now on you only need to start RStudio and not R. Of course, if you are the curious type, nothing shall stop you to try R without RStudio. 2.5 When you first start RStudio Before you start programming away, you might want to make some tweaks to your settings right away to have a better experience (in my humble opinion). To open the Rstudio settings you have to click on RStudio &gt; Preferences or press ⌘ + , if you are on a Mac. RStudio &gt; Tools &gt; Global Options or press Ctrl + , if you work on a Windows computer. I recommend to at least make the following changes to set yourself up for success right from the beginning: Already on the first tab, i.e. General &gt; Basic, we should make one of the most significant changes. Deactivate every option that starts with Restore. This will ensure that every time you start RStudio, you begin with a clean slate. At first sight, it might sound counter-intuitive not to restart everything where you left off, but it is essential to make all your projects easily reproducible. Furthermore, if you work together with others, not restoring your personal settings also ensures that your programming works across different computers. Therefore, I recommend having the following unticked: Restore most recently opened project at startup, Restore previsouly open source documents at startup, Restore .Rdata into workspace at startup Figure 2.8: get your RStudio preferences In the same tab under Workspace, select Never for the setting Save workspace to .RData on exit. One might think it is wise to keep intermediary results stored from one R session to another. However, I often found myself fixing issues due to this lazy method, and my code became less reliable and, therefore, reproducible. With experience, you will find that this avoids many headaches. In the Code &gt; Editing tab, make sure to have at least the first five options ticked, especially the Auto-indent code after paste. This setting will save time when trying to format your coding appropriately, making it easier to read. Indentation is the primary way of making your code look more readable and less like a series of characters that appear almost random. Figure 2.9: Pimp your RStudio IDE In the Display tab, you might want to have the first three options selected. In particular, Highlight selected line is helpful because, in more complicated code, it is helpful to see where your cursor is. Figure 2.10: Edit your RStudio display preferences Of course, if you wish to customise your workspace further, you can do so. The visually most impactful way to alter the default appearance of RStudio is to select Appearance and pick a completely different colour theme. Feel free to browse through various options and see what you prefer. There is no right or wrong here. Just make it your own. Figure 2.11: This will get you instantly nerd 2.6 Updating R and RStudio: Living at the pulse of innovation While not strictly something that helps you become a better programmer, this advice might come in handy to avoid turning into a frustrated programmer. When you update your software, you need to update R and RStudio separately from each other. While both R and RStudio work closely with each other, they still constitute separate pieces of software. Thus, it is essential to keep in mind that updating RStudio will not automatically update R. This can become problematic if specific tools you installed via RStudio (like a fancy learning algorithm) might not be compatible with earlier versions of R. Also, additional R packages (see Chapter 3) developed by other developers are separate pieces which require updating too, independently from R and RStudio. I know what you are thinking: This already sounds complicated and cumbersome. However, rest assured, we take a look at how you can easily update all your packages with RStudio. Thus, all you need to remember is: R needs to be updated separately from everything else. "],["r-packages.html", "Chapter 3 📦 R Packages 3.1 Installing Packages 3.2 Loading Packages 3.3 Updating R and Its Packages", " Chapter 3 📦 R Packages Many of R’s most useful functions do not come preloaded when you start R, but reside in packages that can be installed on top of R. R packages are similar to libraries in C, C++, and Javascript, packages in Python, and gems in Ruby. An R package bundles together useful functions, help files, and data sets. You can use these functions within your own R code once you load the package they live in. Usually the contents of an R package are all related to a single type of task, which the package helps solve. R packages will let you take advantage of R’s most useful features: its large community of package writers (many of whom are active data scientists) and its prewritten routines for handling many common (and exotic) data-science tasks. Base R You may hear R users (or me) refer to “base R.” What is base R? It is just the collection of R functions that gets loaded every time you start R. These functions provide the basics of the language, and you don’t have to load a package before you can use them. 3.1 Installing Packages To use an R package, you must first install it on your computer and then load it in your current R session. The easiest way to install an R package is with the install.packages R function. Open R and type the following into the command line: install.packages(&quot;&lt;package name&gt;&quot;) This will search for the specified package in the collection of packages hosted on the CRAN site. When R finds the package, it will download it into a libraries folder on your computer. R can access the package here in future R sessions without reinstalling it. Anyone can write an R package and disseminate it as they like; however, almost all R packages are published through the CRAN website. CRAN tests each R package before publishing it. This doesn’t eliminate every bug inside a package, but it does mean that you can trust a package on CRAN to run in the current version of R on your OS. You can install multiple packages at once by linking their names with R’s concatenate function, c. For example, to install the ggplot2, reshape2, and dplyr packages, run: install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;carData&quot;, &quot;spdep&quot;)) If this is your first time installing a package, R will prompt you to choose an online mirror of to install from. Mirrors are listed by location. Your downloads should be quickest if you select a mirror that is close to you. If you want to download a new package, try the Austria mirror first. This is the main CRAN repository, and new packages can sometimes take a couple of days to make it around to all of the other mirrors. 3.2 Loading Packages Installing a package doesn’t immediately place its functions at your fingertips. It just places them on your computer. To use an R package, you next have to load it in your R session with the command: library(&quot;&lt;package name&gt;&quot;) Notice that the quotation marks have disappeared. You can use them if you like, but quotation marks are optional for the library command. (This is not true for the install.packages command). library will make all of the package’s functions, data sets, and help files available to you until you close your current R session. The next time you begin an R session, you’ll have to reload the package with library if you want to use it, but you won’t have to reinstall it. You only have to install each package once. After that, a copy of the package will live in your R library. To see which packages you currently have in your R library, run: library() library() also shows the path to your actual R library, which is the folder that contains your R packages. You may notice many packages that you don’t remember installing. This is because R automatically downloads a set of useful packages when you first install R. Install packages from (almost) anywhere The devtools R package makes it easy to install packages from locations other than the CRAN website. devtools provides functions like install_github, install_gitorious, install_bitbucket, and install_url. These work similar to install.packages, but they search new locations for R packages. install_github is especially useful because many R developers provide development versions of their packages on GitHub. The development version of a package will contain a sneak peek of new functions and patches but may not be as stable or as bug free as the CRAN version. Why does R make you bother with installing and loading packages? You can imagine an R where every package came preloaded, but this would be a very large and slow program. As of May 6, 2014, the CRAN website hosts 5,511 packages. It is simpler to only install and load the packages that you want to use when you want to use them. This keeps your copy of R fast because it has fewer functions and help pages to search through at any one time. The arrangement has other benefits as well. For example, it is possible to update your copy of an R package without updating your entire copy of R. What’s the best way to learn about R packages? It is difficult to use an R package if you don’t know that it exists. You could go to the CRAN website and click the Packages link to see a list of available packages, but you’ll have to wade through thousands of them. Moreover, many R packages do the same things. How do you know which package does them best? The R-packages mailing list is a place to start. It sends out announcements of new packages and maintains an archive of old announcements. Blogs that aggregate posts about R can also provide valuable leads. I recommend R-bloggers. RStudio maintains a list of some of the most useful R packages in the Getting Started section of http://support.rstudio.com. Finally, CRAN groups together some of the most useful—and most respected—packages by subject area. This is an excellent place to learn about the packages designed for your area of work. 3.3 Updating R and Its Packages The R Core Development Team continuously hones the R language by catching bugs, improving performance, and updating R to work with new technologies. As a result, new versions of R are released several times a year. The easiest way to stay current with R is to periodically check the CRAN website. The website is updated for each new release and makes the release available for download. You’ll have to install the new release. The process is the same as when you first installed R. Don’t worry if you’re not interested in staying up-to-date on R Core’s doings. R changes only slightly between releases, and you’re not likely to notice the differences. However, updating to the current version of R is a good place to start if you ever encounter a bug that you can’t explain. RStudio also constantly improves its product. You can acquire the newest updates just by downloading them from RStudio. 3.3.1 R Packages Package authors occasionally release new versions of their packages to add functions, fix bugs, or improve performance. The update.packages command checks whether you have the most current version of a package and installs the most current version if you do not. The syntax for update.packages follows that of install.packages. If you already have ggplot2, reshape2, and dplyr on your computer, it’d be a good idea to check for updates before you use them: update.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;carData&quot;, &quot;spdep&quot;)) You should start a new R session after updating packages. If you have a package loaded when you update it, you’ll have to close your R session and open a new one to begin using the updated version of the package. "],["nice-warm-up.html", "Chapter 4 🔥 Nice warm-up 4.1 Starting your fresh new R project 4.2 Creating an R Project file 4.3 Working Directory with here 4.4 Creating an R Script 4.5 Using R Markdown", " Chapter 4 🔥 Nice warm-up Now we are going to cover some very basic operations and computer science concepts with R. Hopefully this will get you with a really cool starter pack of function that you might reuse throughout you R journey. 4.1 Starting your fresh new R project Every fresh attempt is likely to pique your interest and pique your emotions. And it should. You will uncover the answers to your research questions, and you should become more knowledgeable as a consequence. However, you are likely to dislike certain aspects of data analysis. Two examples spring to mind: A Keeping track of all the files generated by my project B Data manipulation While we will go into deeper detail on data manipulation in a later chapter, I’d like to share some ideas from my work that helped me stay organized and, as a result, less frustrated. The following is applicable to both small and large research projects, making it extremely useful regardless of the circumstance or size of the project. 4.2 Creating an R Project file When working on a project, you likely create many different files for various purposes, especially R Scripts (File &gt; New File &gt; R Script). If you are not careful, this file is stored in your system’s default location, which might not be where you want them to be. RStudio allows you to manage your entire project intuitively and conveniently through R Project files. Using R Project files comes with a couple of perks, for example: All of the files you create are saved in the same location. Your data, coding, exported charts, reports, and so on are all in one location, so you don’t have to maintain the files manually. This is because RStudio sets the root directory to the folder where your project is stored. If you wish to share your project, you may do so by sharing the entire folder, and others can rapidly replicate your study or assist in issue resolution. This is due to the fact that all file paths are relative rather than absolute. You may utilize GitHub more readily for backups and so-called ‘version control’ tools, which allows you to trace changes to your code over time. (btw this is really crucial in work envirnoments, if you would like to know more about that I dedicated a tutorial website of git+GitHub+RStudio workflow and a set of slides to explain these concepts). This is not a requirement for the course and you can skip that. However let me clarify that: it is a nice-to-have skill whenever you are collaborating with someone. It could happen on the job, or while writing your thesis, you name it. For the time being, the most significant reason to make R Project files is the ease of file organization and the ability to readily share them with co-investigators, your supervisor, or your students. To create an R Project, you need to perform the following steps: Select File &gt; New Project… from the menu bar. Figure 4.1: Get the R project Select New Directory from the popup window. Figure 4.2: New Project Wizard pop up menu Next, select New Project. Figure 4.3: The full set of project you can initialize through the RStudio IDE Pick a meaningful name for your project folder, i.e. the Directory Name. Ensure this project folder is created in the right place. You can change the subdirectory by clicking on Browse…. Ideally the subdirectory is a place where you usually store your research projects. Figure 4.4: The RProject specifications You have the option to Create a git repository. This is only relevant if you already have a GitHub account and wish to use version control. For now, you can happily ignore it if you do not use GitHub. Lastly, tick Open in new session. This will open your R Project in a new RStudio window. Figure 4.5: Choose a directory name for your new project Once you are happy with your choices, you can click Create Project. This will open a new R Session, and you can start working on your project. Figure 4.6: A new RStudio Session will pop up just like magic! If you look carefully, you can see that your RStudio is now ‘branded’ with your project name. At the top of the window, you see the project name, the files pane shows the root directory where all your files will be, and even the console shows on top the file path of your project. You could set all this up manually, but I would not recommend it, not the least because it is easy and swift to work with R Projects 4.3 Working Directory with here When you bootstrap your RProject in that way, RStudio is going to take care of many headaches that any fresher and sophmore developer have in the beginning. As a matter of fact each time you double click on the RStudio project file (the one that finishes with .RProj) RStudio will link itself to the directory on your computer you specified during the creation of the project, in the previous case “tidy_tuesday_2021_08_03”. This is called the Working Directory. What it is interesting it that this place is where R will look for files when you attempt to load them, and it is where R will save files when you save them. The location of your working directory will vary on different computers. There is a base (rather vintage) way to look for the working directory. To understrand which directory R is using as your working directory, run: getwd() ## &quot;/Users/niccolo/Desktop/r_projects/sbd_22-23&quot; However since we live in 2022 we are going to use a very convenient package i.e. here that does exactly the same thing but prettier and more intuitively. install.packages(&quot;here&quot;) library(here) here() ## &quot;/Users/niccolo/Desktop/r_projects/sbd_22-23&quot; here() is going to look for the .RProj file and will the Working Directory exactly where it is placed. 4.4 Creating an R Script Code may easily grow lengthy and complicated. As a result, writing it on the console is inconvenient. As an alternative, we may write code into a R Script. An R Script is a document that is recognized by RStudio as R programming code. Non-R Script files, such as .txt,.rtf, or .md, can also be opened in RStudio, but any code typed in them will not be immediately recognized. When you open or create a new R script, it will appear in the Source pane. This window is sometimes referred to as the ‘script editor’. An R script begins with an empty file. Good coding etiquette requires us to put a comment # on the first line to describe what this file does. Here’s a ‘TidyTuesday’ R Project sample. Figure 4.7: Open an R Script and write some on it All of the examples in this tutorial are made to be copied and pasted into your own R script. However, you will need to install the R packages for certain code. Let’s give it a shot with the following code. The plot produced by this code displays which car company provides the most fuel-efficient vehicles. This code should be copied and pasted into your R script. Below there’s a simple script that generates a plot, copy and paste into your file, then execute it. library(tidyverse) mpg %&gt;% ggplot(aes(x = reorder(manufacturer, desc(hwy), FUN = median), y = hwy, fill = manufacturer)) + geom_boxplot() + coord_flip() + theme_minimal() + xlab(&quot;Manufacturer&quot;) + ylab(&quot;Highway miles per gallon&quot;) You’re probably wondering what happened to your plot. Copying the code will not execute it in your R script. However, this is required in order to develop the plot. If you pressed Return ↵, you would just add a new line. Instead, choose the code you wish to run and hit Ctrl+Return ↵ (PC) or Cmd+Return ↵ (Mac). You may also use the Run command at the top of your source window, but the keyboard shortcut is far more convenient. Furthermore, you will rapidly remember this shortcut because we will need to utilize it frequently. If everything is in order, you should see the following: As you can see, Honda automobiles appear to travel the furthest with the same quantity of fuel (a gallon) as other vehicles. As a result, if you’re seeking for cheap automobiles, you now know where to look at. It’s worth noting that the R script editor includes some handy features for developing code. You’ve undoubtedly noticed that part of the code we’ve pasted is blue and others is green. Because they have a distinct significance, these colors aid in making your code more understandable. In the default settings, green represents any value in ““, which often represents characters. Syntax highlighting refers to the automatic coloring of our programming code. 4.5 Using R Markdown There is too lot to say about R Markdown, so I’ll just mention that it exists and highlight one feature that could persuade you to use it instead of plain R scripts: They appear to be Word documents (almost). R Markdown files, as the name implies, are a mix of R scripts and ‘Markdown.’ ‘Markdown’ is a method of composing and formatting text documents without the use of software such as Microsoft Word. You instead write everything in plain text. Such plain text may be translated into a variety of document forms, including HTML webpages, PDF files, and Word documents. I recommend checking out the R Markdown Cheatsheet to learn how it works. Click File &gt; New File &gt; R Markdown to create a R Markdown file. An R Markdown file is the inverse of a R script. By default, a R script treats everything as code, and we can only use language to describe what the code does by commenting #. This is what you’ve seen in all of the previous code examples. An R Markdown file, on the other hand, treats everything as text and requires us to declare what is code. We may accomplish this by injecting ‘code chunks.’ As a result, using comments # in R Markdown files is less necessary because you may write about it. Another advantage of R Markdown files is that the results of your analysis are shown immediately underneath the code chunk rather than in the terminal. They are also sometimes called notebooks since they can display both code and text together, the Python equivalent for those that have been someway exposed to Python scripting in Jupyter "],["syllabus.html", "Chapter 5 🗒️ Syllabus 5.1 Course Materials 5.2 Course Structure", " Chapter 5 🗒️ Syllabus The lecture slides, notes, tutorials, and assignments will be posted on this drive , feel free to jump on it. please do not anticipate questions that we will address in class, instead drop me a mail if you are not sure about something. If for any reasons you have trouble accessing the G Drive, still please contact your teaching assistant. One common issue students complain is that you may need to be authorization to access, because I may have forgotten to switch on the open to share option. If you see me, knock me on the shoulder! This schedule is subject to change according to the pace of the class and we may schedule a further lab if you feel you are not really ready for the intermediate exam. This is the updated edition of this course with Dr. Niccolò Salvini (Part 1) and Prof. Sophie Dabo-Niang (Part 2). 5.1 Course Materials All course materials are available in the slides/ folder: 5.1.1 Part 1: The Foundations (Dr. Niccolò Salvini) Slides by Dr. Niccolò Salvini: - Hypothesis Testing with Null Hypothesis - Hypothesis Testing with Alternative Hypothesis - How to Calculate P-values - Hypothesis Testing on Averages - Exercises on Hypothesis Testing - Multiple Linear Regression - Nonlinear Regression - Introduction to Linear Regression - Introduction to Logistic Regression Slides by Prof. Vincenzo Nardelli: - Linear Regression 5.1.2 Part 2: Advanced Modeling (Prof. Sophie Dabo-Niang) Materials will be provided during the intensive session (week of November 17th) 5.2 Course Structure Part 1: The Foundations (Dr. Niccolò Salvini) - 5 hours of integrated labs during regular lectures - Mondays: 14:00 - 17:00 CET - Tuesdays: 10:00 - 13:00 CET Part 2: Advanced Modeling (Prof. Sophie Dabo-Niang) - 5 hours intensive session during the week of November 17th to be ready: Date Description Materials Events Sept 22-23 Introduction to RStudio + Hypothesis Testing Fundamentals Null Hypothesis Testing Laboratory + Lecture Sept 22-23 Alternative Hypothesis Testing + P-values Calculation Alternative Hypothesis Laboratory + Lecture Sept 22-23 How to Calculate P-values P-values Calculation Laboratory + Lecture [TBD] Hypothesis Testing on Averages Testing on Averages and Exercises Laboratory [TBD] Chi-Square Tests (more than 2 proportions) Null Hypothesis Testing Laboratory Sept 29-30, Oct 6-7 Simple Linear Regression Introduction to Linear Regression, Linear Regression Laboratory + Theory Sept 29-30, Oct 6-7 Multiple Linear Regression Multiple Linear Regression Laboratory + Theory Sept 29-30, Oct 6-7 Nonlinear Regression + Dummy Variables Nonlinear Regression Laboratory + Theory Oct 7 Logistic Regression Introduction to Logistic Regression Laboratory + Theory [TBD] First Intermediate Exam (Part 1) Exam Week of Nov 17th Intensive Session - Advanced Modeling Materials provided during session Laboratory [TBD] Second Intermediate Exam (Part 2) Exam "],["hypothesis-testing-fundamentals.html", "Chapter 6 🧪 Hypothesis Testing Fundamentals 6.1 Learning Objectives 6.2 Introduction to Hypothesis Testing 6.3 Null and Alternative Hypotheses 6.4 P-values and Statistical Significance 6.5 Hypothesis Testing on Averages 6.6 Practical Example 6.7 Type I and Type II Errors 6.8 Best Practices 6.9 Summary 6.10 Further-on", " Chapter 6 🧪 Hypothesis Testing Fundamentals This chapter introduces the fundamental concepts of hypothesis testing, covering alternative hypothesis testing, p-value calculation, and hypothesis testing with null hypothesis. 6.1 Learning Objectives By the end of this chapter, you will be able to: Understand the concept of hypothesis testing Formulate null and alternative hypotheses Calculate and interpret p-values Perform hypothesis tests on averages Make statistical decisions based on test results 6.2 Introduction to Hypothesis Testing Hypothesis testing is a statistical method used to make decisions about population parameters based on sample data. It involves: Formulating hypotheses: Stating a null hypothesis (H₀) and an alternative hypothesis (H₁) Collecting data: Gathering sample data relevant to the hypothesis Calculating test statistics: Computing appropriate test statistics Making decisions: Comparing test statistics to critical values or p-values 6.3 Null and Alternative Hypotheses 6.3.1 Null Hypothesis (H₀) The null hypothesis represents the status quo or the claim we want to test. It typically states that there is no effect, no difference, or no relationship. Examples: - H₀: μ = 50 (population mean equals 50) - H₀: μ₁ = μ₂ (two population means are equal) - H₀: ρ = 0 (no correlation between variables) 6.3.2 Alternative Hypothesis (H₁) The alternative hypothesis represents what we want to prove or the claim we’re testing for. It can be: One-tailed: H₁: μ &gt; 50 or H₁: μ &lt; 50 Two-tailed: H₁: μ ≠ 50 6.4 P-values and Statistical Significance 6.4.1 What is a P-value? The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the sample data, assuming the null hypothesis is true. 6.4.2 Interpreting P-values p &lt; 0.05: Strong evidence against H₀ (reject H₀) p &lt; 0.01: Very strong evidence against H₀ (reject H₀) p &gt; 0.05: Weak evidence against H₀ (fail to reject H₀) 6.4.3 Common Misconceptions P-value is NOT the probability that H₀ is true P-value is NOT the probability that H₁ is true P-value is NOT the probability of making a Type I error 6.5 Hypothesis Testing on Averages 6.5.1 One-Sample t-test Used to test whether a population mean differs from a specified value. Assumptions: - Data is normally distributed (or large sample size) - Observations are independent - Random sampling Test Statistic: t = (x̄ - μ₀) / (s/√n) Where: - x̄ = sample mean - μ₀ = hypothesized population mean - s = sample standard deviation - n = sample size 6.5.2 Two-Sample t-test Used to compare means between two groups. Types: - Independent samples: Two separate groups - Paired samples: Same subjects measured twice 6.6 Practical Example Let’s work through a practical example using R: # Load required packages library(tidyverse) # Example: Testing if a new teaching method improves test scores # H₀: μ_new = μ_old (no difference in means) # H₁: μ_new &gt; μ_old (new method is better) # Sample data old_method &lt;- c(65, 70, 68, 72, 69, 71, 67, 73, 70, 68) new_method &lt;- c(72, 75, 78, 74, 76, 79, 73, 77, 75, 74) # Perform two-sample t-test t_test_result &lt;- t.test(new_method, old_method, alternative = &quot;greater&quot;) print(t_test_result) # Extract p-value p_value &lt;- t_test_result$p.value cat(&quot;P-value:&quot;, p_value, &quot;\\n&quot;) # Make decision if (p_value &lt; 0.05) { cat(&quot;Reject H₀: New method significantly improves scores\\n&quot;) } else { cat(&quot;Fail to reject H₀: No significant improvement\\n&quot;) } 6.7 Type I and Type II Errors 6.7.1 Type I Error (α) Definition: Rejecting H₀ when it’s actually true Probability: α (significance level, typically 0.05) Consequence: False positive 6.7.2 Type II Error (β) Definition: Failing to reject H₀ when it’s actually false Probability: β Consequence: False negative 6.7.3 Power (1 - β) Definition: Probability of correctly rejecting H₀ when it’s false Goal: Maximize power while controlling Type I error 6.8 Best Practices State hypotheses clearly before collecting data Choose appropriate significance level (usually α = 0.05) Check assumptions before performing tests Report effect sizes along with p-values Avoid p-hacking (don’t change hypotheses after seeing results) Consider multiple comparisons when testing many hypotheses 6.9 Summary Hypothesis testing is a powerful statistical tool for making data-driven decisions. Key points to remember: Always formulate clear null and alternative hypotheses Understand what p-values represent and don’t represent Consider both statistical and practical significance Be aware of Type I and Type II errors Follow best practices to ensure valid results 6.10 Further-on Slides: 02_hypt_testing_null_hypo.pdf, 03_hypt_testing_alternative_hypo.pdf, 04_how_to_calculate_pvalues.pdf, hypt_testing_on_avg.pdf, 05_hypt_testing_exeR.pdf Additional resources available in the course drive "],["linear-regression-analysis.html", "Chapter 7 📈 Linear Regression Analysis 7.1 Learning Objectives 7.2 Introduction to Linear Regression 7.3 Assumptions of Linear Regression 7.4 Model Evaluation Metrics 7.5 Practical Example: Simple Linear Regression 7.6 Multiple Linear Regression 7.7 Nonlinear Regression 7.8 Model Diagnostics 7.9 Best Practices 7.10 Common Pitfalls 7.11 Summary 7.12 Further-on", " Chapter 7 📈 Linear Regression Analysis This chapter covers linear regression analysis, including simple linear regression, multiple linear regression, and nonlinear regression techniques. 7.1 Learning Objectives By the end of this chapter, you will be able to: Understand the principles of linear regression Perform simple and multiple linear regression Interpret regression coefficients and statistics Assess model fit and assumptions Handle nonlinear relationships Use R for regression analysis 7.2 Introduction to Linear Regression Linear regression is a statistical method used to model the relationship between a dependent variable (Y) and one or more independent variables (X). It assumes a linear relationship between the variables. 7.2.1 Simple Linear Regression Simple linear regression models the relationship between two variables: Model: Y = β₀ + β₁X + ε Where: - Y = dependent variable (response) - X = independent variable (predictor) - β₀ = intercept - β₁ = slope - ε = error term 7.2.2 Multiple Linear Regression Multiple linear regression extends simple regression to include multiple predictors: Model: Y = β₀ + β₁X₁ + β₂X₂ + … + βₖXₖ + ε 7.3 Assumptions of Linear Regression Linearity: The relationship between X and Y is linear Independence: Observations are independent Homoscedasticity: Constant variance of errors Normality: Errors are normally distributed No multicollinearity: Independent variables are not highly correlated 7.4 Model Evaluation Metrics 7.4.1 R-squared (R²) Definition: Proportion of variance in Y explained by X Range: 0 to 1 Interpretation: Higher values indicate better fit 7.4.2 Adjusted R-squared Definition: R² adjusted for the number of predictors Use: Compare models with different numbers of predictors Formula: 1 - (1-R²)(n-1)/(n-k-1) 7.4.3 Root Mean Square Error (RMSE) Definition: Standard deviation of residuals Interpretation: Lower values indicate better fit Units: Same as dependent variable 7.5 Practical Example: Simple Linear Regression # Load required packages library(tidyverse) library(broom) # Create sample data set.seed(123) n &lt;- 100 x &lt;- rnorm(n, mean = 50, sd = 10) y &lt;- 2 + 0.5 * x + rnorm(n, mean = 0, sd = 5) # Create data frame data &lt;- data.frame(x = x, y = y) # Fit simple linear regression model &lt;- lm(y ~ x, data = data) # View model summary summary(model) # Extract key statistics model_summary &lt;- summary(model) r_squared &lt;- model_summary$r.squared adj_r_squared &lt;- model_summary$adj.r.squared p_value &lt;- model_summary$coefficients[2, 4] cat(&quot;R-squared:&quot;, round(r_squared, 3), &quot;\\n&quot;) cat(&quot;Adjusted R-squared:&quot;, round(adj_r_squared, 3), &quot;\\n&quot;) cat(&quot;P-value:&quot;, round(p_value, 4), &quot;\\n&quot;) # Create visualization ggplot(data, aes(x = x, y = y)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, se = TRUE) + labs( title = &quot;Simple Linear Regression&quot;, x = &quot;Independent Variable (X)&quot;, y = &quot;Dependent Variable (Y)&quot; ) + theme_minimal() 7.6 Multiple Linear Regression 7.6.1 Example with Multiple Predictors # Load required packages library(tidyverse) library(car) # for VIF calculation # Create sample data with multiple predictors set.seed(123) n &lt;- 100 x1 &lt;- rnorm(n, mean = 50, sd = 10) x2 &lt;- rnorm(n, mean = 30, sd = 8) x3 &lt;- rnorm(n, mean = 20, sd = 5) y &lt;- 10 + 0.3 * x1 + 0.2 * x2 - 0.1 * x3 + rnorm(n, mean = 0, sd = 3) # Create data frame data &lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y) # Fit multiple linear regression model &lt;- lm(y ~ x1 + x2 + x3, data = data) # View model summary summary(model) # Check for multicollinearity using VIF vif_values &lt;- vif(model) print(&quot;Variance Inflation Factors:&quot;) print(vif_values) # Interpretation guidelines: # VIF &lt; 5: No multicollinearity concern # VIF 5-10: Moderate multicollinearity # VIF &gt; 10: High multicollinearity 7.7 Nonlinear Regression When the relationship between variables is not linear, we can use nonlinear regression techniques. 7.7.1 Polynomial Regression # Create nonlinear data set.seed(123) x &lt;- seq(0, 10, length.out = 50) y &lt;- 2 + 0.5 * x + 0.1 * x^2 + rnorm(50, mean = 0, sd = 1) # Create data frame data &lt;- data.frame(x = x, y = y) # Fit polynomial regression (quadratic) model_poly &lt;- lm(y ~ x + I(x^2), data = data) # View model summary summary(model_poly) # Create visualization ggplot(data, aes(x = x, y = y)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), se = TRUE) + labs( title = &quot;Polynomial Regression (Quadratic)&quot;, x = &quot;Independent Variable (X)&quot;, y = &quot;Dependent Variable (Y)&quot; ) + theme_minimal() 7.7.2 Logarithmic Transformation # Create exponential data set.seed(123) x &lt;- seq(1, 10, length.out = 50) y &lt;- exp(0.5 + 0.3 * x + rnorm(50, mean = 0, sd = 0.1)) # Create data frame data &lt;- data.frame(x = x, y = y) # Fit log-transformed model model_log &lt;- lm(log(y) ~ x, data = data) # View model summary summary(model_log) # Create visualization ggplot(data, aes(x = x, y = y)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, se = TRUE) + scale_y_log10() + labs( title = &quot;Log-transformed Regression&quot;, x = &quot;Independent Variable (X)&quot;, y = &quot;Dependent Variable (Y) - Log Scale&quot; ) + theme_minimal() 7.8 Model Diagnostics 7.8.1 Residual Analysis # Load required packages library(tidyverse) library(broom) # Fit model model &lt;- lm(y ~ x1 + x2 + x3, data = data) # Get residuals and fitted values model_data &lt;- augment(model) # Residual plots # 1. Residuals vs Fitted Values ggplot(model_data, aes(x = .fitted, y = .resid)) + geom_point(alpha = 0.6) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs( title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot; ) + theme_minimal() # 2. Q-Q Plot for normality ggplot(model_data, aes(sample = .resid)) + stat_qq() + stat_qq_line() + labs( title = &quot;Q-Q Plot of Residuals&quot;, x = &quot;Theoretical Quantiles&quot;, y = &quot;Sample Quantiles&quot; ) + theme_minimal() # 3. Scale-Location Plot ggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) + geom_point(alpha = 0.6) + geom_smooth(se = FALSE) + labs( title = &quot;Scale-Location Plot&quot;, x = &quot;Fitted Values&quot;, y = &quot;√|Standardized Residuals|&quot; ) + theme_minimal() 7.9 Best Practices Check assumptions before interpreting results Use appropriate transformations for nonlinear relationships Avoid overfitting by not including too many predictors Consider interaction terms when theoretically justified Report confidence intervals for coefficients Validate models using cross-validation when possible 7.10 Common Pitfalls Correlation vs Causation: Regression doesn’t imply causation Extrapolation: Be cautious when predicting outside the data range Outliers: Check for influential observations Missing data: Handle missing values appropriately Model selection: Use appropriate criteria for model comparison 7.11 Summary Linear regression is a fundamental statistical technique for modeling relationships between variables. Key points: Understand the assumptions and check them Use appropriate metrics to evaluate model fit Consider nonlinear relationships when necessary Perform thorough diagnostics Interpret results carefully and avoid common pitfalls 7.12 Further-on Slides: linear_regression.pdf, mlt_lin_reg.pdf, nonlinear_regression.pdf Additional resources available in the course drive "],["advanced-statistical-methods.html", "Chapter 8 🔬 Advanced Statistical Methods 8.1 Learning Objectives 8.2 Regression with Dummy Variables 8.3 Logistic Regression 8.4 Factor Analysis 8.5 Cluster Analysis 8.6 Model Selection and Validation 8.7 Best Practices 8.8 Common Pitfalls 8.9 Summary 8.10 References", " Chapter 8 🔬 Advanced Statistical Methods This chapter covers advanced statistical methods including regression with dummy variables, logistic regression, factor analysis, and cluster analysis. 8.1 Learning Objectives By the end of this chapter, you will be able to: Understand and implement regression with dummy variables Perform logistic regression for binary outcomes Conduct factor analysis to reduce dimensionality Apply cluster analysis techniques Interpret results from advanced statistical methods Use R for advanced statistical analysis 8.2 Regression with Dummy Variables Dummy variables (also called indicator variables) are binary variables (0/1) used to represent categorical data in regression models. 8.2.1 Creating Dummy Variables # Load required packages library(tidyverse) library(fastDummies) # Create sample data with categorical variable set.seed(123) n &lt;- 100 education &lt;- sample(c(&quot;High School&quot;, &quot;Bachelor&quot;, &quot;Master&quot;, &quot;PhD&quot;), n, replace = TRUE) experience &lt;- rnorm(n, mean = 5, sd = 2) salary &lt;- 30000 + 5000 * (education == &quot;Bachelor&quot;) + 8000 * (education == &quot;Master&quot;) + 12000 * (education == &quot;PhD&quot;) + 2000 * experience + rnorm(n, mean = 0, sd = 3000) # Create data frame data &lt;- data.frame(education = education, experience = experience, salary = salary) # Create dummy variables data_dummy &lt;- dummy_cols(data, select_columns = &quot;education&quot;, remove_first_dummy = TRUE) # View the data head(data_dummy) # Fit regression with dummy variables model &lt;- lm(salary ~ experience + education_Bachelor + education_Master + education_PhD, data = data_dummy) # View model summary summary(model) # Alternative: R automatically creates dummy variables model_auto &lt;- lm(salary ~ experience + education, data = data) summary(model_auto) 8.2.2 Interpreting Dummy Variable Coefficients Reference category: The category not included in the model (usually the first alphabetically) Coefficients: Represent the difference from the reference category Example: If “High School” is the reference, the coefficient for “Bachelor” represents the additional salary for Bachelor’s degree holders 8.3 Logistic Regression Logistic regression is used when the dependent variable is binary (0/1, Yes/No, Success/Failure). 8.3.1 Binary Logistic Regression # Load required packages library(tidyverse) library(broom) # Create sample data for logistic regression set.seed(123) n &lt;- 200 age &lt;- rnorm(n, mean = 35, sd = 10) income &lt;- rnorm(n, mean = 50000, sd = 15000) education_years &lt;- rnorm(n, mean = 16, sd = 3) # Create binary outcome (loan approval) log_odds &lt;- -2 + 0.05 * age + 0.0001 * income + 0.2 * education_years prob &lt;- exp(log_odds) / (1 + exp(log_odds)) loan_approved &lt;- rbinom(n, 1, prob) # Create data frame data &lt;- data.frame(age = age, income = income, education_years = education_years, loan_approved = loan_approved) # Fit logistic regression model &lt;- glm(loan_approved ~ age + income + education_years, data = data, family = binomial()) # View model summary summary(model) # Extract coefficients and odds ratios coef_summary &lt;- tidy(model, exponentiate = TRUE) print(coef_summary) # Predict probabilities data$predicted_prob &lt;- predict(model, type = &quot;response&quot;) # Create visualization ggplot(data, aes(x = income, y = loan_approved)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = TRUE) + labs( title = &quot;Logistic Regression: Loan Approval vs Income&quot;, x = &quot;Income&quot;, y = &quot;Loan Approved (0/1)&quot; ) + theme_minimal() 8.3.2 Interpreting Logistic Regression Results Coefficients: Represent the change in log-odds Odds Ratios: e^(coefficient) represents the multiplicative change in odds Probabilities: Use the logistic function to convert log-odds to probabilities 8.4 Factor Analysis Factor analysis is used to identify underlying latent factors that explain the correlations among observed variables. 8.4.1 Exploratory Factor Analysis (EFA) # Load required packages library(tidyverse) library(psych) library(GPArotation) # Create sample data with underlying factors set.seed(123) n &lt;- 300 # Factor 1: Math ability math1 &lt;- rnorm(n, mean = 0, sd = 1) math2 &lt;- 0.8 * math1 + rnorm(n, mean = 0, sd = 0.6) math3 &lt;- 0.7 * math1 + rnorm(n, mean = 0, sd = 0.7) # Factor 2: Verbal ability verbal1 &lt;- rnorm(n, mean = 0, sd = 1) verbal2 &lt;- 0.9 * verbal1 + rnorm(n, mean = 0, sd = 0.4) verbal3 &lt;- 0.8 * verbal1 + rnorm(n, mean = 0, sd = 0.6) # Create data frame data &lt;- data.frame( math_test1 = math1, math_test2 = math2, math_test3 = math3, verbal_test1 = verbal1, verbal_test2 = verbal2, verbal_test3 = verbal3 ) # Perform factor analysis # First, check if data is suitable for factor analysis cortest.bartlett(data) # Determine number of factors fa.parallel(data, fa = &quot;fa&quot;, n.iter = 100) # Perform factor analysis fa_result &lt;- fa(data, nfactors = 2, rotate = &quot;varimax&quot;) print(fa_result) # Plot factor loadings fa.diagram(fa_result) # Extract factor scores factor_scores &lt;- factor.scores(data, fa_result) data$factor1 &lt;- factor_scores$scores[, 1] data$factor2 &lt;- factor_scores$scores[, 2] # Visualize factor scores ggplot(data, aes(x = factor1, y = factor2)) + geom_point(alpha = 0.6) + labs( title = &quot;Factor Scores&quot;, x = &quot;Factor 1 (Math Ability)&quot;, y = &quot;Factor 2 (Verbal Ability)&quot; ) + theme_minimal() 8.5 Cluster Analysis Cluster analysis groups similar observations together based on their characteristics. 8.5.1 K-Means Clustering # Load required packages library(tidyverse) library(cluster) library(factoextra) # Create sample data with clusters set.seed(123) n &lt;- 200 # Generate three clusters cluster1 &lt;- data.frame( x = rnorm(n/3, mean = 2, sd = 0.5), y = rnorm(n/3, mean = 2, sd = 0.5), cluster = 1 ) cluster2 &lt;- data.frame( x = rnorm(n/3, mean = 6, sd = 0.5), y = rnorm(n/3, mean = 2, sd = 0.5), cluster = 2 ) cluster3 &lt;- data.frame( x = rnorm(n/3, mean = 4, sd = 0.5), y = rnorm(n/3, mean = 6, sd = 0.5), cluster = 3 ) # Combine clusters data &lt;- rbind(cluster1, cluster2, cluster3) data$cluster &lt;- as.factor(data$cluster) # Perform K-means clustering kmeans_result &lt;- kmeans(data[, 1:2], centers = 3, nstart = 25) data$kmeans_cluster &lt;- as.factor(kmeans_result$cluster) # Visualize clusters ggplot(data, aes(x = x, y = y, color = cluster)) + geom_point(size = 2) + labs( title = &quot;True Clusters&quot;, x = &quot;X&quot;, y = &quot;Y&quot; ) + theme_minimal() ggplot(data, aes(x = x, y = y, color = kmeans_cluster)) + geom_point(size = 2) + labs( title = &quot;K-means Clusters&quot;, x = &quot;X&quot;, y = &quot;Y&quot; ) + theme_minimal() # Determine optimal number of clusters fviz_nbclust(data[, 1:2], kmeans, method = &quot;wss&quot;) fviz_nbclust(data[, 1:2], kmeans, method = &quot;silhouette&quot;) 8.5.2 Hierarchical Clustering # Perform hierarchical clustering dist_matrix &lt;- dist(data[, 1:2]) hclust_result &lt;- hclust(dist_matrix, method = &quot;ward.D2&quot;) # Plot dendrogram plot(hclust_result, main = &quot;Hierarchical Clustering Dendrogram&quot;) # Cut tree to get clusters hclust_clusters &lt;- cutree(hclust_result, k = 3) data$hclust_cluster &lt;- as.factor(hclust_clusters) # Visualize hierarchical clusters ggplot(data, aes(x = x, y = y, color = hclust_cluster)) + geom_point(size = 2) + labs( title = &quot;Hierarchical Clusters&quot;, x = &quot;X&quot;, y = &quot;Y&quot; ) + theme_minimal() 8.6 Model Selection and Validation 8.6.1 Cross-Validation for Logistic Regression # Load required packages library(tidyverse) library(caret) # Create sample data set.seed(123) n &lt;- 300 x1 &lt;- rnorm(n, mean = 0, sd = 1) x2 &lt;- rnorm(n, mean = 0, sd = 1) x3 &lt;- rnorm(n, mean = 0, sd = 1) # Create binary outcome log_odds &lt;- -1 + 0.5 * x1 + 0.3 * x2 - 0.2 * x3 prob &lt;- exp(log_odds) / (1 + exp(log_odds)) y &lt;- rbinom(n, 1, prob) # Create data frame data &lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y) # Set up cross-validation ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE) # Train logistic regression model model &lt;- train(as.factor(y) ~ x1 + x2 + x3, data = data, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = ctrl) # View results print(model) print(model$results) 8.7 Best Practices Check assumptions for each method Use appropriate sample sizes for reliable results Validate models using cross-validation Interpret results in context of the research question Consider multiple methods when appropriate Document decisions and rationale 8.8 Common Pitfalls Overfitting: Including too many variables Multicollinearity: Highly correlated predictors Sample size: Insufficient data for reliable results Assumptions: Not checking method-specific assumptions Interpretation: Misunderstanding coefficients and results 8.9 Summary Advanced statistical methods provide powerful tools for analyzing complex data. Key points: Choose appropriate methods based on research questions Check assumptions and validate models Interpret results carefully and in context Use multiple methods when appropriate Document all decisions and rationale 8.10 References Slides: Available in the course drive Additional resources and examples provided in class R documentation for specific packages used "],["advanced-modeling-techniques.html", "Chapter 9 🚀 Advanced Modeling Techniques 9.1 Learning Objectives 9.2 Course Structure 9.3 Factor Analysis 9.4 Cluster Analysis 9.5 Discrimination &amp; Classification 9.6 Logistic Regression 9.7 Kernel Methods 9.8 General Additive Models (GAMs) 9.9 Other Supervised Models 9.10 Practical Implementation 9.11 Assessment and Evaluation 9.12 Intensive Session Schedule 9.13 Prerequisites 9.14 Resources 9.15 Summary 9.16 References", " Chapter 9 🚀 Advanced Modeling Techniques This chapter covers advanced modeling techniques taught by Prof. Sophie Dabo-Niang during the intensive session. These methods extend beyond basic statistical analysis to include sophisticated machine learning and modeling approaches. 9.1 Learning Objectives By the end of this chapter, you will be able to: Understand and apply factor analysis techniques Perform cluster analysis for data segmentation Implement discrimination and classification methods Use binomial and multinomial logistic regression Apply kernel methods for non-linear relationships Work with general additive models Explore other supervised learning models 9.2 Course Structure This part of the course consists of 5 hours of intensive sessions held during the week of November 17th. The sessions are designed to provide hands-on experience with advanced modeling techniques that build upon the foundations covered in Part 1. 9.3 Factor Analysis Factor analysis is a statistical method used to identify underlying latent factors that explain the correlations among observed variables. 9.3.1 Key Concepts Exploratory Factor Analysis (EFA): Discovering the underlying structure Confirmatory Factor Analysis (CFA): Testing hypothesized structures Factor Loadings: Relationships between variables and factors Eigenvalues: Amount of variance explained by each factor 9.3.2 Applications Psychometric testing Market research Social science research Data reduction 9.4 Cluster Analysis Cluster analysis groups similar observations together based on their characteristics, without prior knowledge of group membership. 9.4.1 Methods Covered K-means clustering: Partitioning data into k clusters Hierarchical clustering: Building clusters in a tree-like structure Density-based clustering: Finding clusters of arbitrary shape Model-based clustering: Using statistical models 9.4.2 Applications Customer segmentation Market research Image segmentation Gene expression analysis 9.5 Discrimination &amp; Classification These methods aim to classify observations into predefined categories based on their characteristics. 9.5.1 Techniques Linear Discriminant Analysis (LDA): Linear boundaries between classes Quadratic Discriminant Analysis (QDA): Quadratic boundaries Naive Bayes: Probabilistic classification Support Vector Machines (SVM): Finding optimal separating hyperplanes 9.6 Logistic Regression Logistic regression models the probability of categorical outcomes. 9.6.1 Types Covered Binomial Logistic Regression: Binary outcomes (0/1, Yes/No) Multinomial Logistic Regression: Multiple categories Ordinal Logistic Regression: Ordered categories 9.6.2 Key Concepts Odds and Odds Ratios: Interpreting coefficients Maximum Likelihood Estimation: Parameter estimation Model Diagnostics: Assessing model fit Model Selection: Choosing appropriate predictors 9.7 Kernel Methods Kernel methods extend linear algorithms to handle non-linear relationships by mapping data to higher-dimensional spaces. 9.7.1 Applications Kernel SVM: Non-linear classification Kernel PCA: Non-linear dimensionality reduction Kernel Ridge Regression: Non-linear regression 9.8 General Additive Models (GAMs) GAMs extend linear models by allowing non-linear relationships between predictors and the response variable. 9.8.1 Features Smooth functions: Flexible non-linear relationships Additive structure: Sum of smooth functions Interpretability: Maintains model interpretability Flexibility: Handles various data types 9.9 Other Supervised Models Additional supervised learning techniques for classification and regression. 9.9.1 Methods Covered Random Forest: Ensemble of decision trees Gradient Boosting: Sequential ensemble method Neural Networks: Multi-layer perceptrons Ensemble Methods: Combining multiple models 9.10 Practical Implementation All methods will be implemented using R with appropriate packages: # Load required packages for advanced modeling library(factoextra) # Factor analysis library(cluster) # Cluster analysis library(MASS) # LDA, QDA library(e1071) # SVM library(mgcv) # GAMs library(randomForest) # Random Forest library(gbm) # Gradient Boosting library(nnet) # Neural Networks library(caret) # Model training and validation 9.11 Assessment and Evaluation 9.11.1 Model Evaluation Metrics Classification: Accuracy, Precision, Recall, F1-score Regression: RMSE, MAE, R-squared Clustering: Silhouette score, Within-cluster sum of squares Cross-validation: Ensuring model generalizability 9.11.2 Best Practices Data Preprocessing: Handle missing values and outliers Feature Selection: Choose relevant predictors Model Validation: Use cross-validation techniques Hyperparameter Tuning: Optimize model parameters Model Comparison: Compare different approaches Interpretation: Understand and communicate results 9.12 Intensive Session Schedule The intensive session will cover: Day 1: Factor Analysis and Cluster Analysis - Morning: Theory and concepts - Afternoon: Hands-on implementation Day 2: Classification and Logistic Regression - Morning: Discrimination methods - Afternoon: Logistic regression applications Day 3: Advanced Methods - Morning: Kernel methods and GAMs - Afternoon: Ensemble methods and model comparison 9.13 Prerequisites Students should be familiar with: - Basic statistical concepts from Part 1 - R programming fundamentals - Linear regression concepts - Hypothesis testing 9.14 Resources Course slides and materials will be provided during the intensive session Additional resources available in the course drive R documentation for specific packages Practice datasets for hands-on exercises 9.15 Summary This intensive session provides students with advanced modeling techniques essential for modern data analysis. The focus is on practical implementation and interpretation of results, building upon the statistical foundations established in Part 1 of the course. 9.16 References Slides and materials provided by Prof. Sophie Dabo-Niang Additional resources available in the course drive R documentation for advanced modeling packages "],["int-prep.html", "Chapter 10 💻 Intermediate preparation 10.1 🪨 core concepts of T test 10.2 t.test() function 10.3 Introduction 10.4 Basic Usage 10.5 Arguments 10.6 VIF (Variance Inflation Factor) 10.7 ANOVA (Analysis of Variance) 10.8 Logistic Regression with glm() 10.9 Poisson Regression with glm() 10.10 class exercises, do it in groups 👯 10.11 solutions 10.12 🍬 tips and tricks 10.13 further exercies 🏋️", " Chapter 10 💻 Intermediate preparation Intermediate test topics should include: - hypothesis tests on means - anova - simple + multiple regression (adj-R2 + collinearity + dummy vars) - logistic regression Below you find a review of core concepts and a bunch of exercises which in some way may resemble intermediate ones. 10.1 🪨 core concepts of T test recall from Arbia’s slide 69 that we are focusing on test on mean and that steps for calculating statistical tests generally are: Develop the null Hypothesis Specify the level of Significance \\(\\alpha\\) Collect the sample data and compute the test statistic. then pvalues: 4. Use the value of the test statistic to compute the p-value 5. Reject \\(H_0\\) if p-value $ &lt; $ Notice that Z-test are not really used since we almost always do not know the variance \\(\\sigma^2\\) of the distribution. So instead of Standard Normal distribution we use Student’s T distribution, doing t-tests. That’s the t test stat: \\[ t = \\frac{(m - \\mu_0)}{S/\\sqrt{n}} \\] Below all the possible cases you should be dealing with: Figure 10.1: The t.test vademecum 10.2 t.test() function The t.test() function may run one and two sample t-tests on data vectors. The function has several parameters and is invoked as follows: t.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) In this case, x is a numeric vector of data values, and y is optional. If y is not included, the function does a one-sample t-test on the data in x; if it is included, the function runs a two-sample t-test on both x and y. The mu (i.e. \\(\\mu\\)) argument returns a number showing the real value of the mean (or difference in means if a two sample test is used) under the null hypothesis. The test conducts a two-sided t-test by default; however, you may perform an alternative hypothesis by changing the alternative argument to \"greater\" or \"less\", depending on whether the alternative hypothesis is that the mean is larger than or smaller than mu. Consider the following: t.test(x, alternative = &quot;less&quot;, mu = 25) …performs a one-sample t-test on the data contained in x where the null hypothesis is that $ = 25$ and the alternative is that $ &lt; 25 $ The paired argument will indicate whether or not you want a paired t-test. The default is set to FALSE but can be set to TRUE if you desire to perform a paired t-test. We do paired test when we are considering pre-post treatment test or when we are considering couples of individuals. When doing a two-sample t-test, the var.equal option specifies whether or not to assume equal variances. The default assumption is unequal variance and the Welsh approximation to degrees of freedom; however, you may change this to TRUE to pool the variance. Finally, the conf.level parameter specifies the degree of confidence in the reported confidence interval for \\(\\mu\\) in the one-sample and for \\(\\mu_1 - \\mu_2\\) two-sample cases. Below a very simple example: To use the lm() function in R for linear modeling, you can create a Markdown document to explain its usage and its various arguments. Below is an example of how to create a conceptual guide in RMarkdown format: ## Linear Modeling with lm() in R 10.3 Introduction The lm() function in R is used for performing linear regression modeling. Linear regression is a statistical method that models the relationship between a dependent variable and one or more independent variables. In this document, we will explore how to use the lm() function and its various arguments for linear modeling. 10.4 Basic Usage To perform linear modeling with lm(), you need to specify the formula of the linear regression model. The basic syntax is as follows: model &lt;- lm(dependent_variable ~ independent_variable, data = your_data) dependent_variable: The variable you want to predict. independent_variable: The variable(s) used to make the prediction. data: The data frame containing your variables. 10.5 Arguments 10.5.1 Formula The formula is at the core of the lm() function and specifies the relationship between the dependent and independent variables. It follows the Y ~ X pattern, where Y is the dependent variable, and X is the independent variable. You can include multiple independent variables and interactions by using + and *. For example: model &lt;- lm(y ~ x1 + x2 + x1*x2, data = your_data) 10.5.2 Data The data argument should point to the data frame where your variables are located. This argument helps R locate the variables specified in the formula. 10.5.3 Subset You can use the subset argument to specify a subset of your data for modeling. This is useful when you want to focus on a specific portion of your dataset. model &lt;- lm(y ~ x, data = your_data, subset = condition) 10.5.4 Weight The weight argument allows you to assign different weights to each data point. This can be useful when you want to give more importance to certain observations. model &lt;- lm(y ~ x, data = your_data, weights = weight_variable) 10.5.5 Na.action The na.action argument controls how missing values are treated. By default, na.action = na.fail, which means the model will fail if there are missing values in your data. You can also use na.omit to automatically remove rows with missing values. model &lt;- lm(y ~ x, data = your_data, na.action = na.omit) 10.5.6 Other Arguments There are additional arguments for controlling aspects of the modeling process, such as offset, method, and control. You can refer to the R documentation for a complete list of available arguments and their descriptions. 10.6 VIF (Variance Inflation Factor) The Variance Inflation Factor (VIF) is a measure used to detect multicollinearity in a linear regression model. Multicollinearity occurs when independent variables in the model are highly correlated, making it challenging to determine the individual effect of each variable on the dependent variable. The VIF function from the regclass package in R can help us identify multicollinearity in our linear regression model. 10.6.1 Calculating VIF To calculate the VIF for the independent variables in your model, you can use the VIF() function. First, make sure you have the regclass package installed and loaded: install.packages(&quot;regclass&quot;) library(regclass) Next, you can calculate VIF for your linear regression model as follows: # Fit a linear regression model model &lt;- lm(formula = y ~ x1 + x2 + x3, data = your_data) # Calculate VIF vif_results &lt;- VIF(model) The VIF() function takes the linear regression model as its argument and returns a data frame with VIF values for each independent variable. Higher VIF values indicate stronger multicollinearity, typically with a threshold of 5 or 10 as a rule of thumb. 10.6.2 Interpreting VIF If VIF is close to 1, it suggests that the variable is not highly correlated with other independent variables, indicating no significant multicollinearity. If VIF is greater than 1 but less than a chosen threshold (e.g., 5 or 10), it suggests some correlation but not necessarily problematic multicollinearity. If VIF is significantly greater than the chosen threshold (e.g., 10), it indicates a high degree of multicollinearity, and you may need to consider removing or combining variables to address this issue. 10.6.3 Addressing Multicollinearity If you detect problematic multicollinearity with high VIF values, you can take several steps to address the issue: Remove one of the correlated variables: If two or more variables are highly correlated, removing one of them can often resolve the multicollinearity issue. Combine correlated variables: You can create new variables that are combinations of highly correlated variables, reducing multicollinearity. Collect more data: Sometimes, multicollinearity can be alleviated by collecting more data, especially if the sample size is small. Regularization techniques: Consider using regularization techniques like Ridge or Lasso regression, which can handle multicollinearity by adding penalties to the coefficients of correlated variables. VIF analysis is a crucial step in assessing the quality of your linear regression model and ensuring the independence of your independent variables. 10.7 ANOVA (Analysis of Variance) ANOVA, or Analysis of Variance, is a statistical technique used to analyze the differences among group means in a dataset. It is particularly useful when you want to compare the means of more than two groups. The aov() function in R is commonly used to perform ANOVA. 10.7.1 Performing ANOVA with aov() To perform ANOVA using the aov() function, you need to specify a formula that describes the relationship between the dependent variable and the grouping factor (categorical variable). The basic syntax is as follows: anova_model &lt;- aov(dependent_variable ~ grouping_factor, data = your_data) dependent_variable: The continuous variable you want to analyze. grouping_factor: The categorical variable that defines the groups. data: The data frame containing your variables. 10.7.2 ANOVA Tables Once you’ve created the ANOVA model, you can obtain an ANOVA table using the summary() function applied to the aov object: summary(anova_model) This table provides various statistics, including the sum of squares, degrees of freedom, F-statistic, and p-value, which allow you to assess the significance of differences among group means. 10.7.3 Interpreting ANOVA The F-statistic in the ANOVA table tests whether there are significant differences among the group means. A small p-value (&lt; 0.05) suggests that there are significant differences. If the ANOVA is not statistically significant, it indicates that there are no significant differences among the group means. 10.7.4 Assumptions of ANOVA ANOVA assumes that the variances of the groups are equal (homogeneity of variances) and that the data are normally distributed. Violations of these assumptions may lead to inaccurate results. You can check the homogeneity of variances using tests like Levene’s test or Bartlett’s test and assess the normality of data using normal probability plots or statistical tests like the Shapiro-Wilk test. 10.8 Logistic Regression with glm() Logistic regression is a statistical technique used for modeling the relationship between a binary dependent variable (0/1, Yes/No, True/False) and one or more independent variables. The glm() function in R is commonly used to perform logistic regression. 10.8.1 Performing Logistic Regression with glm() To perform logistic regression using the glm() function, you need to specify a formula that describes the relationship between the binary dependent variable and the independent variables. The basic syntax is as follows: logistic_model &lt;- glm(formula = dependent_variable ~ independent_variable1 + independent_variable2, family = &quot;binomial&quot;, data = your_data) dependent_variable: The binary dependent variable you want to model. independent_variable1, independent_variable2, etc.: The independent variables that influence the probability of the binary outcome. family: Specify the family argument as binomial to indicate logistic regression. data: The data frame containing your variables. 10.8.2 Model Summary After creating the logistic regression model, you can obtain a summary of the model’s coefficients, standard errors, z-values, and p-values using the summary() function applied to the glm object: summary(logistic_model) This summary provides valuable information about the influence of the independent variables on the log-odds of the binary outcome. 10.8.3 Interpreting Logistic Regression The coefficients in the summary indicate the direction and strength of the relationship between the independent variables and the log-odds of the binary outcome. Positive coefficients suggest an increase in the log-odds, while negative coefficients suggest a decrease. The odds ratio (exp(coef)) can be used to interpret the change in the odds of the binary outcome for a one-unit change in the independent variable. A significant p-value (&lt; 0.05) for a coefficient suggests that the independent variable has a significant effect on the binary outcome. The null hypothesis in logistic regression is that there is no relationship between the independent variable and the binary outcome. 10.8.4 Model Evaluation To evaluate the performance of your logistic regression model, you can assess its accuracy, sensitivity, specificity, and other metrics using techniques like cross-validation and ROC analysis. You can also plot the ROC curve and calculate the AUC (Area Under the Curve) to assess the model’s predictive power. 10.8.5 Assumptions of Logistic Regression Logistic regression assumes that the log-odds of the binary outcome are a linear combination of the independent variables. It is important to check for violations of this assumption, which can be done through residual analysis. 10.9 Poisson Regression with glm() Poisson regression is a statistical technique used to model the relationship between a count-dependent variable (typically non-negative integers) and one or more independent variables. The glm() function in R is commonly used to perform Poisson regression. 10.9.1 Performing Poisson Regression with glm() To perform Poisson regression using the glm() function, you need to specify a formula that describes the relationship between the count-dependent variable and the independent variables. The basic syntax is as follows: poisson_model &lt;- glm(formula = count_dependent_variable ~ independent_variable1 + independent_variable2, family = &quot;poisson&quot;, data = your_data) count_dependent_variable: The count-dependent variable you want to model. independent_variable1, independent_variable2, etc.: The independent variables that influence the count-dependent variable. family: Specify the family argument as poisson to indicate Poisson regression. data: The data frame containing your variables. 10.9.2 Model Summary After creating the Poisson regression model, you can obtain a summary of the model’s coefficients, standard errors, z-values, and p-values using the summary() function applied to the glm object: summary(poisson_model) This summary provides information about the influence of the independent variables on the expected count of the dependent variable. 10.9.3 Interpreting Poisson Regression The coefficients in the summary indicate the direction and strength of the relationship between the independent variables and the expected count of the dependent variable. Positive coefficients suggest an increase in the expected count, while negative coefficients suggest a decrease. The exponential of the coefficients (exp(coef)) can be used to interpret the multiplicative effect of a one-unit change in the independent variable on the expected count. A significant p-value (&lt; 0.05) for a coefficient suggests that the independent variable has a significant effect on the expected count. The null hypothesis in Poisson regression is that there is no relationship between the independent variable and the expected count. 10.10 class exercises, do it in groups 👯 first guided, Exercise 10.1 A state Highway Patrol periodically samples vehicle at various location on a particular roadway. The sample of vehicle speed is used to test the hypothesis H0 for which the mean is less than equal to 65 The locations where \\(H_0\\) is rejected are deemed the best locations for radar traps. At location F, a sample of 64 vehicles shows a mean speed of 66.2 mph with a std dev of 4.2 mph. Use a \\(\\alpha = 0.05\\) to test the hypothesis. Exercise 10.2 Let’s assume to have dataset midwest in ggplot2 package: this contains demographic information of midwest counties from 2000 US census. Besides all the other variables we are interested in percollege which describes the Percent college educated in midwest. test if the midwest average is less than the national average (i.e. *35%) with a p-value &lt; .02. Exercise 10.3 Download the datafile ‘prawnGR.CSV’ from the Data link and save it to the data directory (Remember R projects and working directory). Import these data into R and assign to a variable with an appropriate name. These data were collected from an experiment to investigate the difference in growth rate of the giant tiger prawn (Penaeus monodon) fed either an artificial or natural diet. Have a quick look at the structure of this dataset. plot the growth rate versus the diet using an appropriate plot. How many observations are there in each diet treatment? You want to compare the difference in growth rate between the two diets using a two sample t-test. Conduct a two sample t-test using the t.test() using the argument var.equal = TRUE to perform the t-test assuming equal variances. What is the null hypothesis you want to test? Do you reject or fail to reject the null hypothesis? What is the value of the t statistic, degrees of freedom and p value? How would you summarise these summary statistics in a report? Exercise 10.4 A new coach has been hired at an athletics school, and the effectiveness of the new type of training will be evaluated by comparing the average times of 10 centimeters. The times in seconds before and after each athlete’s competition are repeated.ù before_training: = c(12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3) after_training = c( 12.7, 13.6, 12.0, 15.2, 16.8, 20.0, 12.0, 15.9, 16.0, 11.1) We are up against two groups of trained competitors, as measurements were taken on the same athletes before and after the competition. To determine if there has been an improvement, a deterioration, or if the time averages have remained essentially constant (i.e., H0). Conduct a test t of student for paired changes if the difference significant to a 95% confidence level? Exercise 10.5 Following exercise 4 Assume that the club management, based on the statistics, fires this coach who has not improved and hires another more promising coach. Following the second training session, we record the athletes’ times: before training: 12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3 after training: 12.0, 12.2, 11.2, 13.0, 15.0, 15.8, 12.2, 13.4, 12.9, 11.0 Exercise 10.6 Let’s assume genderweight in datarium package, containing the weight of 40 individuals (20 women and 20 men). which are the mean weights for male and females? test is they are statistically significant with 95% confidence level Exercise 10.7 Let’s assume to have a sample with this data and respective belonging group: x&lt;-c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16) z&lt;-c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3) perform anova test with aov() function testing if there is significant differences between group 1, 2 and 3. You specify the formula, where x is the continous variable and z is the group variable. This is how you solve it. Exercise 10.8 Assume you have a dataset named PlantGrowth with variables weight (dependent variable) and group (categorical independent variable). Perform an ANOVA analysis to compare the means of weight among different group levels. Check the p-value and determine whether there are significant differences among the group means. Exercise 10.9 We recruit 90 people to participate in an experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month. #make this example reproducible set.seed(0) #create data frame data &lt;- data.frame(program = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 30), weight_loss = c(runif(30, 0, 3), runif(30, 0, 5), runif(30, 1, 7))) plot boxplot of weight_loss ~ program Hint: use boxplot() function specifying the formula. fit 1 way anova to test difference in weight loss for each program. Exercise 10.10 Consider the maximum size of 4 fish each from 3 populations (n=12). We want to use a model that will help us examine the question of whether the mean maximum fish size differs among populations. size &lt;- c(3,4,5,6,4,5,6,7,7,8,9,10) pop &lt;- c(&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;C&quot;,&quot;C&quot;,&quot;C&quot;,&quot;C&quot;) visualize it through boxplot Using ANOVA model test whether any group means differ from another. Exercise 10.11 Let’s consider 6 different insect sprays in InsectSprays contained in R. Let’s assume we are interested in testing if there was a difference in the number of insects found in the field after each spraying, use varibales count and spray. Exercise 10.12 Let’s consider the diet dataset in this link here The data set contains information on 76 people who undertook one of three diets (referred to as diet A, B and C). There is background information such as age, gender, and height. The aim of the study was to see which diet was best for losing weight. to read data first dowload it from the link, then move data inside your R project. then run these commands: diet = read.csv(&quot;&lt; the dataset name&gt;.csv&quot;) We will be using variable Diet, pre.weight and weight6weeks read data from kaggle compute mean weights for each group calculate anova on Diet against the weight cut Exercise 10.13 Use the built-in mtcars dataset with variables mpg (miles per gallon) and vs (engine type: 0 = V-shaped, 1 = straight). Fit a logistic regression model to predict vs (engine type) based on mpg. Interpret the coefficients of the logistic regression model. Exercise 10.14 Use the built-in mtcars dataset with variables mpg (miles per gallon) and gear (number of forward gears). Fit a Poisson regression model to predict gear based on mpg. Interpret the coefficients of the Poisson regression model. 10.11 solutions Answer to Exercise 10.1: Sample random data from a Normal distribution with a given mean and sd. Then define H0 and H1. In the end run hte test. x &lt;- rnorm(n = 64, mean = 66.2, sd = 4.2) test&lt;-t.test(x, mu = 65, alternative = &quot;less&quot;) t = 1.469, df = 63, p-value = 0.9266 alternative hypothesis: true mean is less than 65 95 percent confidence interval: -Inf 66.80805 sample estimates: mean of x 65.84629 The One Sample t-test testing the difference between x (mean = 65.85) and mu = 65 suggests that the effect is positive, statistically not significant, and very small (difference = 0.85, 95% CI [-Inf, 66.81], t(63) = 1.47, p = 0.927 Answer to Exercise 10.7: x &lt;- c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16) z &lt;- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3) z &lt;- as.factor(z) anova_test &lt;- aov(x ~ z) summary(anova_test) Df Sum Sq Mean Sq F value Pr(&gt;F) z 1 1.6 1.60 0.047 **0.832** Residuals 13 446.1 34.32 The ANOVA (formula: x ~ z) suggests that the main effect of z is statistically not significant and very small (F(1, 13) = 0.05, p = 0.832; Eta2 = 3.57e-03, 95% CI [0.00, 1.00]). That means that group means are not that different one from the other Answer to Exercise 10.12: Assuming you have the ‘plant_growth’ dataset # 1. Perform ANOVA anova_model &lt;- aov(weight ~ group, data = plant_growth) # 2. Check p-value summary(anova_model) Answer to Exercise 10.13: data(mtcars) Convert vs to a factor variable mtcars$vs &lt;- as.factor(mtcars$vs) Fit a logistic regression model logistic_model &lt;- glm(vs ~ mpg, family = binomial, data = mtcars) Interpret coefficients summary(logistic_model) Answer to Exercise 10.14: data(mtcars) Fit a Poisson regression model poisson_model &lt;- glm(gear ~ mpg, family = poisson, data = mtcars) Interpret coefficients summary(poisson_model) 10.12 🍬 tips and tricks Yout might be interested in standardizing/fornalize how you say things with a statistical jargon, report does that for you. you simply pass the test, wether it is ANOVA or t.test object inside report report(). Do you rememer the function summary() we have been using for linear regression? This is exactly that, but for both ANOVA and t tests. # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;easystats/report&quot;) library(report) x &lt;- rnorm(n = 64, mean = 66.2, sd = 4.2) test&lt;-t.test(x, mu = 65, alternative = &quot;less&quot;) report(test) #&gt; Effect sizes were labelled following Cohen&#39;s (1988) #&gt; recommendations. #&gt; #&gt; The One Sample t-test testing the difference between x #&gt; (mean = 66.58) and mu = 65 suggests that the effect is #&gt; positive, statistically not significant, and small #&gt; (difference = 1.58, 95% CI [-Inf, 67.41], t(63) = 3.18, p = #&gt; 0.999; Cohen&#39;s d = 0.40, 95% CI [-Inf, 0.61]) when you have data in longer fromat there a different in syntax when you specify t test and it pretty much follows the one for linear models i.e. lm(). let’s look at it. we may have something like: library(tibble) longer = tribble( ~group, ~var, &quot;a&quot;, 10, &quot;b&quot;, 24, &quot;a&quot;, 31, &quot;a&quot;, 75, &quot;b&quot;, 26, &quot;a&quot;, 8, &quot;b&quot;, 98, &quot;b&quot;, 62, ) wider = tribble( ~group_a, ~group_b, 10, 24, 31, 26, 75, 98, 8, 62 ) Those are exaclty the same dataset but arranged in a different format. We are used to the wider format but iut might happen that we bump into the longer one. What do we do? There’sa trick for that, let’s say you want to test if the mean are statistically different with a 95% confidence level, the instead of supplying x and y to t.test() you would follow pretty much the syntax for linear models lm(): test_for_wider_format = t.test(var~group, data = longer) test_for_wider_format #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: var by group #&gt; t = -0.91809, df = 5.9192, p-value = 0.3944 #&gt; alternative hypothesis: true difference in means between group a and group b is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -78.99271 35.99271 #&gt; sample estimates: #&gt; mean in group a mean in group b #&gt; 31.0 52.5 what we conclude? we conclude that: Effect sizes were labelled following Cohen’s (1988) recommendations. The Welch Two Sample t-test testing the difference of var by group (mean in group a = 31.00, mean in group b = 52.50) suggests that the effect is negative, statistically not significant, and medium (difference = -21.50, 95% CI [-78.99, 35.99], t(5.92) = -0.92, p = 0.394; Cohen’s d = -0.75, 95% CI [-2.39, 0.94]) 10.13 further exercies 🏋️ Exercise 10.15 We have the test scores of students before and after an intervention. How can we assess if the intervention had a statistically significant effect on the scores? Specify the type of test to use and the assumptions involved. Exercise 10.16 Using the dataset mtcars, write the R command to calculate the mean and standard deviation of the disp variable grouped by cyl. (#exr:mean_diff1) Explain how you would assess if the mean difference between two groups is statistically significant, without running any R code. Exercise 10.17 Use the Boston dataset in MASS to create a histogram for the variable crim. Report the R code you used. Exercise 10.18 Describe the meaning of the p-value in the context of hypothesis testing. (#exr:temp_diff1) Given a dataset with daily temperatures recorded for two cities over one year, write the R code to perform a hypothesis test to determine if there is a significant difference in mean temperature between the cities. Specify which test should be used. Exercise 10.19 Given X = c(10, 12, 15, 20) and Y = c(11, 14, 16, 18), calculate the Pearson correlation coefficient between X and Y using R. Exercise 10.20 Explain the concept of Type I and Type II errors in hypothesis testing. Exercise 10.21 Using the PlantGrowth dataset in R, calculate the mean of weight for each level of group and plot a boxplot of weight grouped by group. Exercise 10.1 Run a one-sample t-test to test if the mean of hp in mtcars is different from 120. Write the R code. Exercise 10.22 Using the dataset iris, write the R code to test for a significant difference between the average Sepal.Length of setosa and versicolor species. Exercise 10.23 Write an R function to simulate 500 observations from a Poisson distribution with a lambda of 3 and plot its histogram. Exercise 10.24 Describe how to check for multicollinearity in a multiple regression model in R. Exercise 10.25 Using the airquality dataset, calculate the correlation matrix for Ozone, Solar.R, Wind, and Temp. Exercise 10.26 Perform a paired t-test using the before and after variables where before = c(5, 7, 8, 6, 10) and after = c(6, 8, 9, 7, 12). Report the p-value. Exercise 10.13 Using the dataset mtcars, perform a linear regression with mpg as the dependent variable and hp and wt as independent variables. Report the adjusted R-squared. Exercise 10.27 Write the R code to create a density plot of the variable Sepal.Length for each species in the iris dataset. Exercise 10.7 Using iris, perform a one-way ANOVA to test if the mean Sepal.Length differs across the three species. Exercise 10.28 Define the term “confidence interval” in the context of statistical estimation. Exercise 10.29 Using the mtcars dataset, refit a multiple linear regression model with mpg as the dependent variable and hp, wt, and drat as independent variables. Use stepwise regression to iteratively remove insignificant predictors. Report the final model with the significant coefficients. Exercise 10.8 Using the dataset ToothGrowth, perform a one-way ANOVA with the function aov() to test if the mean tooth length differs across the supplement types and doses. Exercise 10.9 We recruit 90 people to participate in an experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month. #make this example reproducible set.seed(0) #create data frame data &lt;- data.frame(program = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 30), weight_loss = c(runif(30, 0, 3), runif(30, 0, 5), runif(30, 1, 7))) plot boxplot of weight_loss ~ program Hint: use boxplot() function specifying the formula. fit 1 way anova to test difference in weight loss for each program. Answer to Exercise 10.15: The appropriate test to use is the paired t-test. This test is used when we have two related samples, such as before and after measurements for the same individuals, and we want to determine if there is a statistically significant difference between the means. The assumptions include that the differences are normally distributed and the data is paired. Answer to Exercise 10.16: The dplyr package is used to calculate the mean and standard deviation of the disp variable grouped by cyl: library(dplyr) mtcars %&gt;% group_by(cyl) %&gt;% summarise(mean_disp = mean(disp), sd_disp = sd(disp)) Answer to Exercise @ref(exr:mean_diff1): To assess if the mean difference between two groups is statistically significant, we can use a hypothesis test such as the independent t-test. This involves setting up null and alternative hypotheses, calculating the test statistic, and comparing it to a critical value or using the p-value to determine significance, typically using a significance level (e.g., 0.05). Answer to Exercise 10.17: To create a histogram of the crim variable from the Boston dataset, use the following code: library(MASS) data(Boston) hist(Boston$crim, main = &quot;Histogram of crim&quot;, xlab = &quot;Crime rate per capita&quot;) Answer to Exercise 10.18: The p-value is the probability of obtaining test results at least as extreme as the observed results, under the assumption that the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis, and if the p-value is below a chosen significance level (e.g., 0.05), we reject the null hypothesis. Answer to Exercise @ref(exr:temp_diff1): The appropriate test to use is the two-sample t-test, as we are comparing the means of two independent groups. The R code is as follows: t.test(temp_city1, temp_city2, var.equal = TRUE) Answer to Exercise 10.19: To calculate the Pearson correlation coefficient between vectors X and Y: X &lt;- c(10, 12, 15, 20) Y &lt;- c(11, 14, 16, 18) cor(X, Y) Answer to Exercise 10.20: A Type I error occurs when we reject a true null hypothesis (false positive), while a Type II error occurs when we fail to reject a false null hypothesis (false negative). Answer to Exercise 10.21: To calculate the mean of weight for each level of group and plot a boxplot of weight grouped by group: data(PlantGrowth) aggregate(weight ~ group, data = PlantGrowth, mean) boxplot(weight ~ group, data = PlantGrowth, main = &quot;Boxplot of Weight by Group&quot;) Answer to Exercise 10.1: To run a one-sample t-test to test if the mean of hp in mtcars is different from 120: t.test(mtcars$hp, mu = 120) Answer to Exercise 10.22: To test for a significant difference between the average Sepal.Length of setosa and versicolor species: t.test(Sepal.Length ~ Species, data = subset(iris, Species %in% c(&quot;setosa&quot;, &quot;versicolor&quot;))) Answer to Exercise 10.23: To simulate 500 observations from a Poisson distribution with a lambda of 3 and plot its histogram: set.seed(0) poisson_data &lt;- rpois(500, lambda = 3) hist(poisson_data, main = &quot;Histogram of Poisson Distribution&quot;, xlab = &quot;Values&quot;) Answer to Exercise 10.24: To check for multicollinearity, we can use the Variance Inflation Factor (VIF). A VIF value greater than 10 indicates high multicollinearity: library(car) vif(model) Answer to Exercise 10.25: To calculate the correlation matrix for Ozone, Solar.R, Wind, and Temp in the airquality dataset: airquality_subset &lt;- airquality[, c(&quot;Ozone&quot;, &quot;Solar.R&quot;, &quot;Wind&quot;, &quot;Temp&quot;)] cor(airquality_subset, use = &quot;complete.obs&quot;) Answer to Exercise 10.26: To perform a paired t-test using the before and after variables: before &lt;- c(5, 7, 8, 6, 10) after &lt;- c(6, 8, 9, 7, 12) t.test(before, after, paired = TRUE) Answer to Exercise 10.13: To perform a linear regression with mpg as the dependent variable and hp and wt as independent variables, and report the adjusted R-squared: model &lt;- lm(mpg ~ hp + wt, data = mtcars) summary(model)$adj.r.squared Answer to Exercise 10.27: To create a density plot of the variable Sepal.Length for each species in the iris dataset: library(ggplot2) ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_density(alpha = 0.5) + labs(title = &quot;Density Plot of Sepal.Length by Species&quot;) Answer to Exercise 10.7: To perform a one-way ANOVA to test if the mean Sepal.Length differs across the three species: aov_model &lt;- aov(Sepal.Length ~ Species, data = iris) summary(aov_model) Answer to Exercise 10.28: A confidence interval is a range of values, derived from sample statistics, that is likely to contain the population parameter with a specified level of confidence (e.g., 95%). It provides an estimated range that is expected to include the true parameter value. Answer to Exercise 10.29: To refit a multiple linear regression model with mpg as the dependent variable and hp, wt, and drat as independent variables, using stepwise regression: library(MASS) initial_model &lt;- lm(mpg ~ hp + wt + drat, data = mtcars) stepwise_model &lt;- stepAIC(initial_model, direction = &quot;both&quot;) summary(stepwise_model) Answer to Exercise 10.8: To perform a one-way ANOVA using the ToothGrowth dataset to test if the mean tooth length differs across the supplement types and doses: data(ToothGrowth) aov_model &lt;- aov(len ~ supp * dose, data = ToothGrowth) summary(aov_model) Answer to Exercise 10.9: To plot a boxplot of weight_loss by program and fit a one-way ANOVA to test the difference in weight loss for each program: # Plot boxplot boxplot(weight_loss ~ program, data = data, main = &quot;Boxplot of Weight Loss by Program&quot;, xlab = &quot;Program&quot;, ylab = &quot;Weight Loss&quot;) # Fit one-way ANOVA aov_model &lt;- aov(weight_loss ~ program, data = data) summary(aov_model) "],["int-samp-q-second.html", "Chapter 11 📚 Additional Practice Exercises for Intermediate Exam 11.1 🏥 Part 1: Data Wrangling and Descriptive Statistics 11.2 🔬 Part 2: Hypothesis Testing - Single Population 11.3 👥 Part 3: Hypothesis Testing - Two Populations 11.4 📊 Part 4: ANOVA - Comparing Multiple Groups 11.5 📈 Part 5: Simple and Multiple Linear Regression 11.6 🔄 Part 6: Nonlinear Regression and Transformations 11.7 🎯 Part 7: Logistic Regression 11.8 📊 Part 8: Poisson Regression 11.9 🏷️ Part 9: Regression with Dummy Variables 11.10 🧮 Part 10: Model Diagnostics and Interpretation 11.11 ✅ SOLUTIONS", " Chapter 11 📚 Additional Practice Exercises for Intermediate Exam This is a comprehensive collection of practice exercises designed to prepare you for the intermediate exam. These exercises cover all major topics from hypothesis testing to advanced regression techniques, with a focus on healthcare and biomedical data analysis. Instructions: Try to solve all exercises on your own first. Solutions are provided at the end of this chapter. 11.1 🏥 Part 1: Data Wrangling and Descriptive Statistics Exercise 11.1 Load the built-in dataset airquality and remove all rows with missing values. How many complete observations remain? Exercise 11.2 Using the chickwts dataset, create a summary table that shows the number of chicks, the mean weight, and the standard deviation of weight for each feed type. Exercise 11.3 You are given a blood_pressure dataframe. It contains patient IDs and two separate columns for systolic and diastolic blood pressure readings (systolic, diastolic). Reshape the data into a “long” format with three columns: patient_id, bp_type (containing “systolic” or “diastolic”), and value. set.seed(123) blood_pressure &lt;- data.frame( patient_id = 1:5, systolic = c(120, 135, 122, 140, 130), diastolic = c(80, 88, 82, 90, 85) ) Exercise 11.4 Using the mtcars dataset, create a new variable called efficiency that categorizes cars as “High” if mpg &gt; 20 and “Low” otherwise. How many cars are in the “High” efficiency category? Exercise 11.5 Write the R command to calculate the mean, median, and standard deviation of a variable called blood_pressure in a dataset named health_data. Exercise 11.6 Given a healthcare dataset, health_data, with variables patient_id, age, treatment (A or B), and recovery_days, write the R command to create a summary table showing the mean recovery days for each treatment group. For reproducibility, first create the health_data dataframe with the following code: set.seed(42) health_data &lt;- data.frame( patient_id = 1:100, age = rnorm(100, 50, 10), treatment = sample(c(&quot;A&quot;, &quot;B&quot;), 100, replace = TRUE), recovery_days = rpois(100, 15) ) Exercise 11.7 From a patients dataset with columns patient_id, age, sex, diagnosis_code, and hospital_stay_days, write a dplyr pipeline to find the average hospital stay for male patients over 65 for each diagnosis code, showing only diagnoses with more than 10 such patients. 11.2 🔬 Part 2: Hypothesis Testing - Single Population Exercise 11.8 A hospital claims that the average waiting time in their emergency room is 45 minutes. A random sample of 50 patients showed a mean waiting time of 52 minutes with a standard deviation of 12 minutes. Test at α = 0.05 if the actual waiting time is significantly different from the claimed 45 minutes. Write the R command to perform this test. Exercise 11.9 A pharmaceutical company claims that 75% of patients experience relief from headaches within 30 minutes of taking their new medication. In a sample of 200 patients, 140 reported relief within 30 minutes. Test if the actual proportion is significantly different from 75%. Write the appropriate R command. Exercise 11.10 Without using formulas, explain the difference between Type I and Type II errors in hypothesis testing. Exercise 11.11 A medical study tests whether the average cholesterol level in a population is greater than 200 mg/dL. The p-value obtained is 0.032. At a significance level of 0.05, what is your decision and conclusion? Exercise 11.12 A pharmaceutical company has developed a new drug to increase T-cell count in patients. A clinically significant improvement is defined as a mean increase of at least 150 cells/µL. In a sample of 35 patients, the mean increase was 162 cells/µL with a standard deviation of 40. Write the R code to test if the drug produces a clinically significant improvement. Explain your choice of a one-sided test. Exercise 11.13 A new drug is tested to reduce systolic blood pressure. The mean reduction in a sample of 40 patients is 8.5 mmHg with a standard deviation of 10 mmHg. A reduction of at least 10 mmHg is considered clinically significant. Perform a one-sample t-test to check if the mean reduction is less than 10 mmHg. Write the R code and interpret the p-value in the context of clinical significance. 11.3 👥 Part 3: Hypothesis Testing - Two Populations Exercise 11.14 A clinical trial compares two different treatments for hypertension. Group A (n=30) has a mean blood pressure reduction of 15 mmHg (sd=5), while Group B (n=35) has a mean reduction of 12 mmHg (sd=6). Assume equal variances. Write the R command to test if there is a significant difference between the two treatments. Exercise 11.15 A hospital is testing a new training program for nurses. The same 20 nurses took a competency test before and after the training. The data is as follows: nurse_data &lt;- data.frame( nurse_id = 1:20, before = c(72, 68, 75, 70, 73, 69, 71, 74, 68, 72, 70, 73, 69, 71, 74, 72, 70, 68, 73, 71), after = c(78, 74, 80, 76, 79, 75, 77, 80, 74, 78, 76, 79, 75, 77, 80, 78, 76, 74, 79, 77) ) Test if the training program significantly improved the test scores. Write the appropriate R command and state whether this should be a paired or unpaired test. Exercise 11.16 Two hospitals are comparing their patient satisfaction rates. Hospital A reports that 180 out of 250 patients (72%) were satisfied, while Hospital B reports that 210 out of 280 patients (75%) were satisfied. Write the R command to test if there is a significant difference in satisfaction rates between the two hospitals. Exercise 11.17 A researcher wants to compare the effectiveness of three different diets (A, B, and C) on weight loss. Given the following data, can you determine if this requires a t-test or ANOVA? Explain why. diet_study &lt;- data.frame( diet = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 15), weight_loss = c(...) # 45 observations total ) Exercise 11.18 A study compares the cholesterol levels of patients in two different clinics. Before running a t-test, you perform a Levene’s test for equality of variances and get a p-value of 0.02. Which type of two-sample t-test should you use and why? Write the R command for the appropriate t-test, assuming you have a cholesterol_data dataframe with level and clinic columns. 11.4 📊 Part 4: ANOVA - Comparing Multiple Groups Exercise 11.19 Using the built-in PlantGrowth dataset in R, test if there is a significant difference in plant weight across the three treatment groups. Write the R command and report the p-value. Exercise 11.20 A medical study examines the effect of four different medications on reducing fever. The temperature reduction (in degrees Celsius) for patients in each group is recorded: fever_data &lt;- data.frame( medication = rep(c(&quot;Med_A&quot;, &quot;Med_B&quot;, &quot;Med_C&quot;, &quot;Med_D&quot;), each = 10), temp_reduction = c( 1.2, 1.5, 1.3, 1.4, 1.6, 1.3, 1.5, 1.4, 1.2, 1.5, # Med_A 1.8, 2.0, 1.9, 2.1, 1.8, 2.0, 1.9, 1.8, 2.0, 1.9, # Med_B 1.0, 1.2, 1.1, 1.3, 1.0, 1.2, 1.1, 1.0, 1.2, 1.1, # Med_C 2.3, 2.5, 2.4, 2.6, 2.3, 2.5, 2.4, 2.3, 2.5, 2.4 # Med_D ) ) Perform an ANOVA test to determine if there are significant differences among the medications. What is the F-statistic? Exercise 11.21 After conducting an ANOVA test that shows significant differences among groups, what additional analysis would you perform to identify which specific groups differ from each other? Exercise 11.22 After running an ANOVA on the effect of three different physical therapy routines (RoutineA, RoutineB, RoutineC) on recovery time, you get a significant p-value. The subsequent Tukey’s HSD test gives the following output. Which routines are significantly different from each other, and what does the diff column represent? Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = recovery_time ~ routine) $routine diff lwr upr p adj RoutineB-RoutineA -5.231 -8.992618 -1.469382 0.0045121 RoutineC-RoutineA 1.543 -2.218618 5.304618 0.5694884 RoutineC-RoutineB 6.774 3.012382 10.535618 0.0002196 11.5 📈 Part 5: Simple and Multiple Linear Regression Exercise 11.23 Using the mtcars dataset, estimate a simple linear regression model where mpg (miles per gallon) is predicted by wt (weight). What is the interpretation of the slope coefficient? Exercise 11.24 A researcher collects data on hospital readmission rates. They want to predict the readmission_rate (percentage) based on bed_count, nurse_ratio (nurses per patient), and avg_stay (average length of stay). Write the R command to estimate this multiple regression model, assuming the data is in a dataframe called hospital_data. Exercise 11.25 In a multiple linear regression model predicting patient recovery time based on age, BMI, and exercise hours, you obtain the following VIF values: - age: VIF = 2.3 - BMI: VIF = 12.5 - exercise: VIF = 2.1 Which variable(s) might be problematic due to multicollinearity, and what would you recommend? Exercise 11.26 Using the mtcars dataset, estimate a multiple regression model: mpg ~ wt + hp + cyl. After checking for significance and multicollinearity, you find that cyl has a p-value of 0.85 and high VIF. What should you do next? Exercise 11.27 Explain the difference between R-squared and Adjusted R-squared. Why is Adjusted R-squared particularly important in multiple regression? Exercise 11.28 Given the following regression output for predicting systolic blood pressure: Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 90.234 5.123 17.61 &lt; 2e-16 *** age 0.652 0.085 7.67 1.2e-11 *** weight 0.234 0.045 5.20 8.3e-07 *** exercise -1.234 0.312 -3.95 0.00015 *** Residual standard error: 8.5 on 146 degrees of freedom Multiple R-squared: 0.5234, Adjusted R-squared: 0.5136 F-statistic: 53.45 on 3 and 146 DF, p-value: &lt; 2.2e-16 What is the predicted systolic blood pressure for a 50-year-old person weighing 75 kg who exercises 5 hours per week? Exercise 11.29 In the regression output from the previous exercise, which variable has the strongest effect on blood pressure? How did you determine this? Exercise 11.30 You are modeling the length of a hospital stay (los) based on age and infection_status (1=yes, 0=no). You suspect that the effect of age on the length of stay is different for patients with and without an infection. How would you specify this in an R model? Write the lm() command and explain how to interpret the interaction term’s coefficient if it is significant. Assume you have a hospital_data dataframe with los, age, and infection_status. 11.6 🔄 Part 6: Nonlinear Regression and Transformations Exercise 11.31 You want to model the relationship between BMI and the risk of diabetes, but the relationship appears to be quadratic. Write the R command to estimate a polynomial regression model with BMI and BMI² as predictors, assuming your data is in a model_data dataframe. Exercise 11.32 Given a variable income that is highly right-skewed, what transformation would you typically apply to make it more normally distributed for regression analysis? Write the R command to create this transformed variable. Exercise 11.33 Using the mtcars dataset, create a model predicting mpg using both wt and wt² (weight squared). Write the complete R commands needed. Exercise 11.34 In a model predicting blood_pressure from log(body_weight), the coefficient for log(body_weight) is 15.3. How do you interpret this coefficient? 11.7 🎯 Part 7: Logistic Regression Exercise 11.35 A study examines factors affecting whether a patient develops a certain disease (yes/no). The predictors are age, BMI, and smoking status (0=non-smoker, 1=smoker). Write the R command to estimate a logistic regression model with disease as the outcome variable, assuming the data is in a dataframe called health_data. Exercise 11.36 In logistic regression, what does an odds ratio of 2.5 for the variable “smoking” mean in practical terms? Exercise 11.37 You have a dataset with a binary outcome survived (1=yes, 0=no) and predictors age, treatment_type, and severity_score. After fitting a logistic regression model, you get a coefficient of -0.05 for age. What does this negative coefficient indicate? Exercise 11.38 What is the key difference between linear regression and logistic regression in terms of what they predict? Exercise 11.39 Using the built-in mtcars dataset, create a binary variable high_mpg (1 if mpg &gt; 20, 0 otherwise) and estimate a logistic regression model predicting high_mpg based on wt and hp. Write the R commands. Exercise 11.40 Using the logistic regression model from Exercise 11.39 (high_mpg ~ wt + hp), write the R code to predict the probability of having high MPG for a car with wt = 2.8 and hp = 150. 11.8 📊 Part 8: Poisson Regression Exercise 11.41 A hospital administrator wants to model the number of emergency room visits per day based on day of the week and weather conditions. What type of regression model is most appropriate for count data like this? Assume the data is in hospital_data. Exercise 11.42 You have data on the number of hospital-acquired infections (infection_count) and want to predict it based on bed_occupancy_rate and nurse_staff_ratio. Write the R command to fit a Poisson regression model, assuming the data is in hospital_data. Exercise 11.43 What is the primary assumption of Poisson regression regarding the relationship between the mean and variance of the outcome variable? Exercise 11.44 You fit a Poisson model to predict the number of asthma-related ER visits. The residual deviance is 150 on 80 degrees of freedom. What does this suggest about your model, and what alternative model should you consider? Write the R command for fitting this alternative model, assuming the data is in hospital_data. 11.9 🏷️ Part 9: Regression with Dummy Variables Exercise 11.45 A dataset contains a variable blood_type with categories: A, B, AB, and O. You want to include this in a regression model predicting cholesterol levels. How many dummy variables do you need to create, and why? Exercise 11.46 You have a dataset with the following variables: - recovery_time (continuous outcome) - age (continuous) - treatment (categorical: “Standard”, “Experimental”, “Control”) Write the R command to create a regression model that includes the categorical treatment variable. Exercise 11.47 In a regression model with a categorical variable “hospital_ward” (ICU, Surgery, General), you create dummy variables. The regression output shows: Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.5 1.2 10.42 &lt; 2e-16 *** ward_Surgery -2.3 0.8 -2.88 0.0045 ** ward_ICU 4.7 0.9 5.22 &lt; 0.0001 *** If “General” is the reference category, what is the predicted value for a patient in the Surgery ward (assuming no other predictors)? Exercise 11.48 Using the mtcars dataset, the variable am is binary (0=automatic, 1=manual). Create a regression model predicting mpg based on wt, hp, and am. Write the R command. Exercise 11.49 In a model with a dummy variable for gender (male=1, female=0) predicting salary, the coefficient for gender is 5000. What does this mean? Exercise 11.50 In a model predicting patient_satisfaction (scale 1-100), you include a dummy variable has_private_room (1=yes, 0=no). The intercept of the model is 75.2. How do you interpret this intercept value? 11.10 🧮 Part 10: Model Diagnostics and Interpretation Exercise 11.51 What are the four main assumptions that should be checked for linear regression models? Exercise 11.52 You fit a regression model and obtain an R² of 0.85 and an Adjusted R² of 0.83. You have 100 observations. Approximately how many predictors are in your model? (Hint: use the relationship between R² and Adjusted R²) Exercise 11.53 A researcher finds that in their regression model, the residuals show a clear funnel shape when plotted against fitted values. What problem does this indicate, and what might be a solution? Exercise 11.54 Using the mtcars dataset, fit a model mpg ~ wt + hp and create diagnostic plots using the plot() function. Write the R commands. Exercise 11.55 You run diagnostic plots for a linear model and notice that observation #45 has a Cook’s distance value much larger than 1. What does this indicate, and what are two potential next steps to address this issue? 11.11 ✅ SOLUTIONS Answer to Exercise 11.1: data(airquality) complete_data &lt;- na.omit(airquality) nrow(complete_data) Result: 111 complete observations remain. Answer to Exercise 11.2: library(dplyr) chickwts %&gt;% group_by(feed) %&gt;% summarise( count = n(), mean_weight = mean(weight, na.rm = TRUE), sd_weight = sd(weight, na.rm = TRUE) ) Answer to Exercise 11.3: library(tidyr) set.seed(123) blood_pressure &lt;- data.frame( patient_id = 1:5, systolic = c(120, 135, 122, 140, 130), diastolic = c(80, 88, 82, 90, 85) ) blood_pressure_long &lt;- blood_pressure %&gt;% pivot_longer( cols = c(systolic, diastolic), names_to = &quot;bp_type&quot;, values_to = &quot;value&quot; ) print(blood_pressure_long) Answer to Exercise 11.4: data(mtcars) mtcars$efficiency &lt;- ifelse(mtcars$mpg &gt; 20, &quot;High&quot;, &quot;Low&quot;) table(mtcars$efficiency) Result: 14 cars are in the “High” efficiency category. Answer to Exercise 11.5: # First, let&#39;s create a sample health_data dataframe for the example set.seed(42) health_data &lt;- data.frame( patient_id = 1:100, blood_pressure = rnorm(100, 120, 15) ) mean(health_data$blood_pressure, na.rm = TRUE) median(health_data$blood_pressure, na.rm = TRUE) sd(health_data$blood_pressure, na.rm = TRUE) # Or all at once: summary(health_data$blood_pressure) Answer to Exercise 11.6: library(dplyr) # Create the dataframe as specified in the exercise set.seed(42) health_data &lt;- data.frame( patient_id = 1:100, age = rnorm(100, 50, 10), treatment = sample(c(&quot;A&quot;, &quot;B&quot;), 100, replace = TRUE), recovery_days = rpois(100, 15) ) health_data %&gt;% group_by(treatment) %&gt;% summarise(mean_recovery = mean(recovery_days, na.rm = TRUE)) # Or base R: aggregate(recovery_days ~ treatment, data = health_data, FUN = mean) Answer to Exercise 11.8: # Create the sample data waiting_time &lt;- rnorm(50, mean = 52, sd = 12) # Simulated data # Perform one-sample t-test t.test(waiting_time, mu = 45, alternative = &quot;two.sided&quot;) # Or if you have the actual data vector: # t.test(sample_waiting_times, mu = 45, alternative = &quot;two.sided&quot;) This is a one-sample t-test because we’re comparing the sample mean to a known population value (45 minutes). Answer to Exercise 11.9: # Proportion test prop.test(x = 140, n = 200, p = 0.75, alternative = &quot;two.sided&quot;) The null hypothesis is H0: p = 0.75. The sample proportion is 140/200 = 0.70 (70%). Answer to Exercise 11.10: Type I Error (α): Rejecting the null hypothesis when it is actually true. This is a “false positive” - concluding there is an effect when there isn’t one. The probability of Type I error is the significance level (usually 0.05). Type II Error (β): Failing to reject the null hypothesis when it is actually false. This is a “false negative” - missing a real effect. The probability of Type II error is related to the power of the test (Power = 1 - β). Example in healthcare: Type I error would be concluding a drug works when it doesn’t; Type II error would be concluding a drug doesn’t work when it actually does. Answer to Exercise 11.11: Decision: Reject the null hypothesis. Conclusion: Since p-value (0.032) &lt; α (0.05), we reject H0. There is sufficient evidence to conclude that the average cholesterol level in the population is significantly greater than 200 mg/dL. Answer to Exercise 11.12: # H0: mu &lt;= 150 (The mean increase is not clinically significant) # Ha: mu &gt; 150 (The mean increase is clinically significant) # We simulate data for the example set.seed(123) t_cell_increase &lt;- rnorm(35, mean = 162, sd = 40) t.test(t_cell_increase, mu = 150, alternative = &quot;greater&quot;) Explanation: A one-sided test (alternative = \"greater\") is used because the research question is directional. We are not just interested in whether the T-cell count is different from 150, but specifically whether it is greater than 150. If the p-value is less than our significance level (e.g., 0.05), we would reject the null hypothesis and conclude that there is statistically significant evidence that the drug’s effect meets the clinically significant threshold. Answer to Exercise 11.14: # Create sample data (in practice, you&#39;d have actual data) group_A &lt;- rnorm(30, mean = 15, sd = 5) group_B &lt;- rnorm(35, mean = 12, sd = 6) # Two-sample t-test assuming equal variances t.test(group_A, group_B, var.equal = TRUE, alternative = &quot;two.sided&quot;) This is an unpaired (independent samples) t-test with equal variances. Answer to Exercise 11.15: nurse_data &lt;- data.frame( nurse_id = 1:20, before = c(72, 68, 75, 70, 73, 69, 71, 74, 68, 72, 70, 73, 69, 71, 74, 72, 70, 68, 73, 71), after = c(78, 74, 80, 76, 79, 75, 77, 80, 74, 78, 76, 79, 75, 77, 80, 78, 76, 74, 79, 77) ) # Paired t-test t.test(nurse_data$after, nurse_data$before, paired = TRUE, alternative = &quot;greater&quot;) This should be a PAIRED test because the same nurses are measured before and after the training (repeated measures on the same subjects). Answer to Exercise 11.16: # Two-sample proportion test prop.test(x = c(180, 210), n = c(250, 280), alternative = &quot;two.sided&quot;) This tests if the proportions (0.72 vs 0.75) are significantly different between the two hospitals. Answer to Exercise 11.17: This requires ANOVA (Analysis of Variance), not a t-test. Reason: We are comparing means across three groups (diets A, B, and C). A t-test can only compare two groups at a time. ANOVA is specifically designed to test if there are significant differences among three or more group means simultaneously. # The appropriate test would be: anova_result &lt;- aov(weight_loss ~ diet, data = diet_study) summary(anova_result) Answer to Exercise 11.18: You should use Welch’s two-sample t-test (the default in R). Reason: Levene’s test checks if the variances of the two groups are equal. A p-value of 0.02 (&lt; 0.05) indicates that the variances are significantly different. The standard two-sample t-test assumes equal variances, so using it would lead to incorrect results. Welch’s t-test does not assume equal variances and is therefore the appropriate choice. # Assuming &#39;cholesterol_data&#39; with &#39;level&#39; and &#39;clinic&#39; columns # The default t.test in R is Welch&#39;s t-test t.test(level ~ clinic, data = cholesterol_data) # You would NOT do this, as var.equal=TRUE is for equal variances: # t.test(level ~ clinic, data = cholesterol_data, var.equal = TRUE) Answer to Exercise 11.19: data(PlantGrowth) plant_model &lt;- aov(weight ~ group, data = PlantGrowth) summary(plant_model) The p-value is approximately 0.0159, indicating significant differences among the groups at α = 0.05. Answer to Exercise 11.20: fever_data &lt;- data.frame( medication = rep(c(&quot;Med_A&quot;, &quot;Med_B&quot;, &quot;Med_C&quot;, &quot;Med_D&quot;), each = 10), temp_reduction = c( 1.2, 1.5, 1.3, 1.4, 1.6, 1.3, 1.5, 1.4, 1.2, 1.5, 1.8, 2.0, 1.9, 2.1, 1.8, 2.0, 1.9, 1.8, 2.0, 1.9, 1.0, 1.2, 1.1, 1.3, 1.0, 1.2, 1.1, 1.0, 1.2, 1.1, 2.3, 2.5, 2.4, 2.6, 2.3, 2.5, 2.4, 2.3, 2.5, 2.4 ) ) fever_anova &lt;- aov(temp_reduction ~ medication, data = fever_data) summary(fever_anova) The F-statistic is approximately 167.4, with a very small p-value (&lt; 2e-16), indicating highly significant differences among medications. Answer to Exercise 11.21: After a significant ANOVA, you should perform post-hoc tests (pairwise comparisons) such as: Tukey’s HSD (Honestly Significant Difference) - most common Bonferroni correction Scheffe’s test # Example using Tukey&#39;s HSD: TukeyHSD(anova_model) # Or using the multcomp package: library(multcomp) post_hoc &lt;- glht(anova_model, linfct = mcp(group_variable = &quot;Tukey&quot;)) summary(post_hoc) These tests identify which specific pairs of groups differ significantly from each other. Answer to Exercise 11.22: Interpretation: * The diff column shows the difference in mean recovery time between the two routines being compared. For example, RoutineB-RoutineA has a diff of -5.231, meaning Routine B’s average recovery time is 5.231 days shorter than Routine A’s. * The p adj column shows the p-value adjusted for multiple comparisons. * Significant differences (p &lt; 0.05): * RoutineB is significantly different from RoutineA (p = 0.0045). * RoutineC is significantly different from RoutineB (p = 0.0002). * No significant difference: * RoutineC is not significantly different from RoutineA (p = 0.569). Conclusion: Routine B leads to the fastest recovery, significantly faster than both A and C. There is no statistical difference between routines A and C. Answer to Exercise 11.23: data(mtcars) mpg_model &lt;- lm(mpg ~ wt, data = mtcars) summary(mpg_model) Interpretation of slope: The coefficient for wt is approximately -5.34. This means that for every 1,000 lb increase in car weight, the fuel efficiency (mpg) decreases by about 5.34 miles per gallon, on average. The negative sign indicates an inverse relationship: heavier cars have lower fuel efficiency. Answer to Exercise 11.24: # Create sample hospital_data for the example set.seed(42) hospital_data &lt;- data.frame( readmission_rate = runif(50, 5, 20), bed_count = rpois(50, 150), nurse_ratio = rnorm(50, 0.8, 0.2), avg_stay = rnorm(50, 7, 2) ) readmission_model &lt;- lm(readmission_rate ~ bed_count + nurse_ratio + avg_stay, data = hospital_data) summary(readmission_model) Answer to Exercise 11.25: Problem: The variable BMI has a VIF of 12.5, which exceeds the common threshold of 10, indicating problematic multicollinearity. Age and exercise have acceptable VIF values (&lt; 10). Recommendation: 1. Consider removing BMI from the model, OR 2. Investigate which other variable(s) BMI is highly correlated with 3. Consider combining correlated variables or using principal component analysis (PCA) 4. Check if the model performance and interpretability are actually affected The other variables (age and exercise) are fine and should be retained. Answer to Exercise 11.26: You should remove the cyl variable from the model. Reasoning: 1. The p-value of 0.85 is much larger than any reasonable significance level (e.g., 0.05), indicating cyl is not significantly associated with mpg in this model 2. High VIF suggests multicollinearity with other predictors 3. Removing non-significant variables with high VIF improves model parsimony and interpretability Next step: # Refit the model without cyl improved_model &lt;- lm(mpg ~ wt + hp, data = mtcars) summary(improved_model) # Check VIF again library(car) vif(improved_model) Answer to Exercise 11.27: R-squared (R²): Represents the proportion of variance in the dependent variable that is explained by the independent variable(s). It ranges from 0 to 1. However, R² always increases (or stays the same) when you add more predictors, even if they’re not truly useful. Adjusted R-squared (R²adj): Adjusts R² for the number of predictors in the model and the sample size. It penalizes the addition of unhelpful variables. Formula: R²adj = 1 - [(1 - R²)(n - 1)/(n - k - 1)] where n = sample size, k = number of predictors Why important in multiple regression: - Prevents overfitting - Allows fair comparison between models with different numbers of predictors - Can decrease if you add predictors that don’t improve the model enough to justify their inclusion - More realistic measure of model quality Use R²adj when comparing models with different numbers of predictors; use R² when evaluating a single model’s goodness of fit. Answer to Exercise 11.28: The regression equation is: Systolic BP = 90.234 + 0.652(age) + 0.234(weight) - 1.234(exercise) For a 50-year-old, 75 kg person who exercises 5 hours/week: Systolic BP = 90.234 + 0.652(50) + 0.234(75) - 1.234(5) Systolic BP = 90.234 + 32.6 + 17.55 - 6.17 Systolic BP = 134.214 mmHg Approximately 134.2 mmHg Answer to Exercise 11.29: The variable with the strongest effect appears to be age, with a t-value of 7.67 (the highest absolute t-value). How to determine: Compare t-values (most direct method): Larger absolute t-values indicate stronger effects age: t = 7.67 weight: t = 5.20 exercise: t = -3.95 Compare p-values: All are highly significant, but age has the smallest p-value (1.2e-11) Standardized coefficients would be the most precise method (not shown here), as they account for different scales of measurement Note: We cannot simply compare the raw coefficients (0.652, 0.234, -1.234) because the variables are measured in different units (years, kg, hours). Answer to Exercise 11.30: You would specify the model with an interaction term using *. # Fit model with interaction term interaction_model &lt;- lm(los ~ age * infection_status, data = hospital_data) summary(interaction_model) # This is equivalent to: # lm(los ~ age + infection_status + age:infection_status, data = hospital_data) Interpretation of the interaction coefficient (age:infection_status): If the interaction term is significant, it means the effect of age on the length of stay is different depending on infection status. The coefficient for age:infection_status represents the additional change in the slope of age for patients with an infection compared to those without. For example, if the coefficient is 1.5, it means that for each one-year increase in age, the length of stay increases by an additional 1.5 days for infected patients, on top of the baseline age effect for non-infected patients. Answer to Exercise 11.31: # Create sample model_data for the example set.seed(42) model_data &lt;- data.frame( BMI = rnorm(200, 28, 5), diabetes_risk = rnorm(200, 0.3, 0.1) ) # Create the polynomial term model_data$BMI_squared &lt;- model_data$BMI^2 # Fit polynomial regression poly_model &lt;- lm(diabetes_risk ~ BMI + BMI_squared, data = model_data) summary(poly_model) # Alternative using poly() function (creates orthogonal polynomials): poly_model2 &lt;- lm(diabetes_risk ~ poly(BMI, 2), data = model_data) summary(poly_model2) The first approach gives more interpretable coefficients; the second is better for avoiding multicollinearity between BMI and BMI². Answer to Exercise 11.32: For a right-skewed variable like income, apply a logarithmic transformation: # Natural log transformation data$log_income &lt;- log(data$income) # Or log base 10 data$log10_income &lt;- log10(data$income) # Make sure there are no zero or negative values first: data$log_income &lt;- log(data$income + 1) # Adding 1 if zeros exist Why log transformation: - Reduces right skewness - Makes the distribution more normal - Reduces the influence of extreme values - Often makes relationships more linear Other options for right-skewed data: square root transformation sqrt(income) or inverse transformation 1/income. Answer to Exercise 11.33: data(mtcars) # Method 1: Create the squared term explicitly mtcars$wt_squared &lt;- mtcars$wt^2 poly_model &lt;- lm(mpg ~ wt + wt_squared, data = mtcars) summary(poly_model) # Method 2: Using I() function (cleaner) poly_model2 &lt;- lm(mpg ~ wt + I(wt^2), data = mtcars) summary(poly_model2) # Method 3: Using poly() for orthogonal polynomials poly_model3 &lt;- lm(mpg ~ poly(wt, 2), data = mtcars) summary(poly_model3) Method 2 is recommended for most applications as it’s concise and easy to interpret. Answer to Exercise 11.34: Interpretation: Since the predictor is log-transformed, the interpretation is: a 1% increase in body weight is associated with a (15.3 / 100) = 0.153 mmHg increase in blood pressure, on average. This type of model (log-lin) is used when the effect of the predictor is not linear but multiplicative. Answer to Exercise 11.35: # Create sample health_data for the example set.seed(42) health_data &lt;- data.frame( disease = sample(0:1, 150, replace = TRUE), age = rnorm(150, 55, 12), BMI = rnorm(150, 27, 4), smoking = sample(0:1, 150, replace = TRUE) ) logistic_model &lt;- glm(disease ~ age + BMI + smoking, data = health_data, family = binomial(link = &quot;logit&quot;)) summary(logistic_model) Key points: - Use glm() instead of lm() - Specify family = binomial for logistic regression - The outcome variable disease should be binary (0/1, or factor with 2 levels) - Results give log-odds; exponentiate coefficients to get odds ratios: exp(coef(logistic_model)) Answer to Exercise 11.36: An odds ratio of 2.5 for smoking means: The odds of developing the disease are 2.5 times higher for smokers compared to non-smokers, holding all other variables constant. More specifically: - If OR = 2.5, smokers have 150% higher odds of disease than non-smokers - OR &gt; 1: positive association (increased risk) - OR = 1: no association - OR &lt; 1: negative association (protective effect) Example interpretation: If non-smokers have a 10% probability of disease, smokers might have approximately 22% probability (though exact conversion from odds to probability requires more calculation). Note: Odds ratio is NOT the same as relative risk, but for rare diseases (&lt; 10% prevalence), they are approximately equal. Answer to Exercise 11.37: The negative coefficient of -0.05 for age indicates: Direction: As age increases, the log-odds of survival decrease Practical meaning: Older patients have lower probability of survival, holding other factors constant Odds ratio: exp(-0.05) = 0.951, meaning for each one-year increase in age, the odds of survival multiply by 0.951 (decrease by about 5%) For a 10-year age difference: exp(-0.05 × 10) = exp(-0.5) = 0.606 A patient 10 years older has about 60.6% the odds of survival compared to a younger patient. Answer to Exercise 11.38: Linear Regression: - Predicts the actual value of a continuous outcome variable - Output: A number on a continuous scale - Example: Predicting exact blood pressure (135 mmHg), salary ($45,000), temperature (98.6°F) - Uses normal distribution Logistic Regression: - Predicts the probability of belonging to a category (usually binary: yes/no, 0/1) - Output: A probability between 0 and 1, which can be converted to a category - Example: Predicting probability of disease (0.75 = 75% chance), probability of survival, probability of clicking an ad - Uses binomial distribution with logit link function Technical difference: - Linear regression: E(Y) = β₀ + β₁X₁ + β₂X₂ + … - Logistic regression: log(P/(1-P)) = β₀ + β₁X₁ + β₂X₂ + … (models log-odds) Answer to Exercise 11.39: data(mtcars) # Create binary outcome variable mtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, 1, 0) # Fit logistic regression model logit_model &lt;- glm(high_mpg ~ wt + hp, data = mtcars, family = binomial(link = &quot;logit&quot;)) # View results summary(logit_model) # Get odds ratios exp(coef(logit_model)) # Predict probabilities for original data mtcars$predicted_prob &lt;- predict(logit_model, type = &quot;response&quot;) Answer to Exercise 11.40: # First, refit the model from Exercise #add30 data(mtcars) mtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, 1, 0) logit_model &lt;- glm(high_mpg ~ wt + hp, data = mtcars, family = binomial) # Create a new data frame for the car we want to predict new_car &lt;- data.frame(wt = 2.8, hp = 150) # Predict the probability predicted_prob &lt;- predict(logit_model, newdata = new_car, type = &quot;response&quot;) print(predicted_prob) The output predicted_prob will be a value between 0 and 1, representing the model’s estimated probability that a car with these specifications has an MPG over 20. Answer to Exercise 11.41: Poisson regression is most appropriate for count data. Reasoning: - The outcome (number of ER visits per day) is a count variable (0, 1, 2, 3, …) - Count data is discrete and non-negative - Poisson distribution is designed for modeling count outcomes - It models the rate of events occurring # Example Poisson regression poisson_model &lt;- glm(er_visits ~ day_of_week + weather, data = hospital_data, family = poisson(link = &quot;log&quot;)) Note: If the variance is much larger than the mean (overdispersion), consider using negative binomial regression instead. Answer to Exercise 11.42: # Create sample hospital_data for the example set.seed(42) hospital_data &lt;- data.frame( infection_count = rpois(100, 5), bed_occupancy_rate = runif(100, 0.7, 0.95), nurse_staff_ratio = rnorm(100, 0.7, 0.1) ) poisson_model &lt;- glm(infection_count ~ bed_occupancy_rate + nurse_staff_ratio, data = hospital_data, family = poisson(link = &quot;log&quot;)) summary(poisson_model) # Check for overdispersion # library(AER) # dispersiontest(poisson_model) # If overdispersion detected, use negative binomial: # library(MASS) # nb_model &lt;- glm.nb(infection_count ~ bed_occupancy_rate + nurse_staff_ratio, # data = hospital_data) Answer to Exercise 11.43: The primary assumption of Poisson regression is that the mean equals the variance (equidispersion). Specifically: - E(Y) = Var(Y) = λ (the Poisson parameter) In practice: - If Var(Y) &gt; E(Y): Overdispersion - common in real data, violates assumption - Solution: Use negative binomial regression or quasi-Poisson - If Var(Y) &lt; E(Y): Underdispersion - less common - May still use Poisson or consider other models Other important assumptions: 1. Outcome variable is a count 2. Observations are independent 3. Counts cannot be negative 4. The log of the mean is a linear function of predictors Check overdispersion: # Deviance/df should be close to 1 model$deviance / model$df.residual If ratio &gt;&gt; 1, overdispersion is present. Answer to Exercise 11.44: Problem: The ratio of residual deviance to degrees of freedom (150 / 80 = 1.875) is much greater than 1. This suggests overdispersion, meaning the variance of the data is larger than what the Poisson model assumes (where mean equals variance). This violates a key assumption of Poisson regression. Alternative Model: A Negative Binomial regression is the appropriate alternative as it can handle overdispersed count data by including an extra parameter to model the variance. # Create sample hospital_data for the example set.seed(42) hospital_data &lt;- data.frame( er_visits = rnbinom(100, size = 1.5, mu = 10), predictor1 = rnorm(100), predictor2 = rnorm(100) ) library(MASS) # Fit a negative binomial model nb_model &lt;- glm.nb(er_visits ~ predictor1 + predictor2, data = hospital_data) summary(nb_model) Answer to Exercise 11.45: You need to create 3 dummy variables for a categorical variable with 4 categories. Rule: For a categorical variable with k categories, create (k-1) dummy variables. Why: - One category serves as the reference/baseline category - The other k-1 categories are represented by dummy variables - Including all k categories would cause perfect multicollinearity (dummy variable trap) Example for blood_type: # R does this automatically in regression, but manual creation: data$type_A &lt;- ifelse(data$blood_type == &quot;A&quot;, 1, 0) data$type_B &lt;- ifelse(data$blood_type == &quot;B&quot;, 1, 0) data$type_AB &lt;- ifelse(data$blood_type == &quot;AB&quot;, 1, 0) # Type O is the reference category (all three dummies = 0) # In regression, R handles this automatically: model &lt;- lm(cholesterol ~ blood_type, data = data) # R will create the dummy variables automatically If you include type O as well, the model matrix would be singular and inestimable. Answer to Exercise 11.46: # R automatically creates dummy variables for factors recovery_model &lt;- lm(recovery_time ~ age + treatment, data = patient_data) summary(recovery_model) # To explicitly set the reference category: patient_data$treatment &lt;- factor(patient_data$treatment, levels = c(&quot;Control&quot;, &quot;Standard&quot;, &quot;Experimental&quot;)) # Now &quot;Control&quot; is the reference # Alternatively, using relevel(): patient_data$treatment &lt;- relevel(factor(patient_data$treatment), ref = &quot;Control&quot;) recovery_model &lt;- lm(recovery_time ~ age + treatment, data = patient_data) summary(recovery_model) R will automatically create 2 dummy variables (treatmentStandard and treatmentExperimental) with “Control” as the reference category. Answer to Exercise 11.47: The predicted value for a patient in the Surgery ward is: Predicted value = 12.5 + (-2.3) = 10.2 Explanation: - The intercept (12.5) represents the predicted value for the reference category (General ward) - For Surgery ward: add the Surgery coefficient (-2.3) to the intercept - For ICU ward: you would add 4.7 to get 12.5 + 4.7 = 17.2 - For General ward (reference): just use the intercept = 12.5 Interpretation: Compared to the General ward (baseline), patients in Surgery have 2.3 units lower outcome (on average), and ICU patients have 4.7 units higher outcome. Answer to Exercise 11.48: data(mtcars) # The variable am is already coded as 0/1 mpg_model &lt;- lm(mpg ~ wt + hp + am, data = mtcars) summary(mpg_model) # To make it more explicit: mtcars$transmission &lt;- factor(mtcars$am, levels = c(0, 1), labels = c(&quot;Automatic&quot;, &quot;Manual&quot;)) mpg_model2 &lt;- lm(mpg ~ wt + hp + transmission, data = mtcars) summary(mpg_model2) The coefficient for am (or transmissionManual) tells you the difference in mpg between manual and automatic transmissions, holding weight and horsepower constant. Answer to Exercise 11.49: The coefficient of 5000 means: Males earn $5,000 more than females on average, holding all other variables in the model constant. More precisely: - When gender = 1 (male), add $5,000 to the predicted salary - When gender = 0 (female, reference), add $0 - This is the estimated gender pay gap in this model Example: If the model is: Salary = 40,000 + 5,000(gender) + other terms - Female (gender=0): Predicted base salary = $40,000 + other factors - Male (gender=1): Predicted base salary = $45,000 + other factors Important notes: - This is correlation, not necessarily causation - Other factors (education, experience, etc.) should be controlled for - The coefficient’s statistical significance (p-value) determines if this difference is reliable Answer to Exercise 11.50: Interpretation: The intercept of 75.2 represents the predicted average patient satisfaction score for the reference group. In this case, the reference group is patients for whom has_private_room is 0, i.e., patients who do not have a private room. Answer to Exercise 11.51: The four main assumptions for linear regression (LINE): Linearity: The relationship between predictors and outcome is linear Check: Residuals vs. Fitted plot should show random scatter Independence: Observations are independent of each other Check: Durbin-Watson test, knowledge of data collection Violated by: Time series, clustered data, repeated measures Normality: Residuals are normally distributed Check: Q-Q plot, Shapiro-Wilk test, histogram of residuals Most important for inference (confidence intervals, p-values) Equal variance (Homoscedasticity): Constant variance of residuals across all levels of predictors Check: Scale-Location plot, Breusch-Pagan test Residuals vs. Fitted should show constant spread Additional assumption often mentioned: 5. No multicollinearity (for multiple regression): Predictors are not highly correlated - Check: VIF (Variance Inflation Factor) # Check assumptions: plot(model) # Produces 4 diagnostic plots library(car) vif(model) # Check multicollinearity Answer to Exercise 11.52: Using the formula for Adjusted R²: R²adj = 1 - [(1 - R²)(n - 1)/(n - k - 1)] Given: - R² = 0.85 - R²adj = 0.83 - n = 100 Solving for k: 0.83 = 1 - [(1 - 0.85)(100 - 1)/(100 - k - 1)] 0.83 = 1 - [0.15 × 99/(99 - k)] 0.17 = 0.15 × 99/(99 - k) 0.17(99 - k) = 14.85 16.83 - 0.17k = 14.85 -0.17k = -1.98 k ≈ 11.6 So approximately 11 or 12 predictors are in the model. Quick approximation: When R² and R²adj are close, you typically have relatively few predictors. The difference of 0.02 with n=100 suggests around 10-12 predictors. Answer to Exercise 11.53: Problem identified: Heteroscedasticity (non-constant variance) A funnel shape means: - Variance of residuals increases (or decreases) with fitted values - Violates the assumption of homoscedasticity (equal variance) Potential solutions: Transform the outcome variable: # Log transformation model_log &lt;- lm(log(y) ~ x1 + x2, data = data) # Square root transformation model_sqrt &lt;- lm(sqrt(y) ~ x1 + x2, data = data) Use weighted least squares (WLS): # Weights inversely proportional to variance wls_model &lt;- lm(y ~ x1 + x2, data = data, weights = 1/residuals^2) Use robust standard errors: library(sandwich) library(lmtest) coeftest(model, vcov = vcovHC(model, type = &quot;HC1&quot;)) Add or transform predictors: May indicate missing variable or wrong functional form Use generalized linear models (GLM): If outcome is count or binary Check after solution: plot(fitted(new_model), residuals(new_model)) # Should show random scatter, not a pattern Answer to Exercise 11.54: data(mtcars) # Fit the model mpg_model &lt;- lm(mpg ~ wt + hp, data = mtcars) # Create diagnostic plots par(mfrow = c(2, 2)) # Set up 2x2 plot layout plot(mpg_model) par(mfrow = c(1, 1)) # Reset to default # Individual plots: # 1. Residuals vs Fitted (checks linearity and homoscedasticity) plot(mpg_model, which = 1) # 2. Q-Q plot (checks normality of residuals) plot(mpg_model, which = 2) # 3. Scale-Location (checks homoscedasticity) plot(mpg_model, which = 3) # 4. Residuals vs Leverage (identifies influential points) plot(mpg_model, which = 5) # Additional useful diagnostics: # Histogram of residuals hist(residuals(mpg_model), main = &quot;Histogram of Residuals&quot;) # Shapiro-Wilk test for normality shapiro.test(residuals(mpg_model)) Interpretation: - Plot 1: Should show random scatter around zero - Plot 2: Points should follow the diagonal line - Plot 3: Should show horizontal line with equal spread - Plot 4: Points outside Cook’s distance indicate influential observations Answer to Exercise 11.55: Indication: A Cook’s distance &gt; 1 indicates that observation #45 is a highly influential point. This means that its presence in the dataset has a substantial impact on the estimated regression coefficients. The model fitted with this point is likely very different from the model fitted without it. Next steps: 1. Investigate the data point: Check observation #45 for data entry errors or determine if it represents a truly unusual case (e.g., a patient with a rare condition). If it’s an error, correct it. 2. Perform sensitivity analysis: Rerun the regression model after removing observation #45. If the model’s coefficients or conclusions change dramatically, you must report this sensitivity and be cautious about interpreting the original model. You might report the results of both models. Simply deleting the point without justification is generally not recommended. "],["int-samp-q.html", "Chapter 12 💻 Intermediate Sample Questions 12.1 👨‍🎓 2020/2021 12.2 👨‍🎓 2021/2022 12.3 👨‍🎓 2022/2023 12.4 solutions", " Chapter 12 💻 Intermediate Sample Questions Hi guys, this is your favourite TA, I am just aggregating questions that have been asked in previous exam sessions the previous years i.e. 2020/2021 and 2021/2022. They are representative of the actual exam, but you know, take it like a grain of salt. I will also make sure to provide to you some other exercises if you are still anxious. 12.1 👨‍🎓 2020/2021 Exercise 12.1 Write the line of the R command that you use to produce a boxplot of the variable X Exercise 12.2 We want to test statistically the hypothesis that the performances of students at UCSC in Rome that graduated last year are better than those that graduated this year. Can we say that this is a paired sample test ? Exercise 12.3 Without using formulae, describe how you can calculate the test statistics in a hypothesis testing procedure on a single mean with known variance. Exercise 12.4 Using the dataset Boston downloaded from the library spdep, write the correlation matrix of the variables MEDV, NOX and CRIM. Exercise 12.5 How do you define the confidence of a statistical test? Exercise 12.6 Given the following 2 variables X = (1,5,3,3,5,5) and Y= (4,4,6,3,2,3), write the cross-tabulation between X and Y. Exercise 12.7 Write the line of the R command that you use to simulate 1000 random observation from normal distribution with 0 mean and variance = 0.5. Exercise 12.8 A law company is evaluating the performances of two departments measuring in terms of the time required for solving a conflict in the last year. The observed values are reported in the following table: … can we accept the hypothesis H0: (the mean of Dept 1 is equal to the mean of Dept 2) versus a bilateral alternative hypothesis? (F) Exercise 12.9 A company has recorded the number of costumers in 10 sample stores before (variable X) and after (Variable Y) a new advertising campaign was introduced. The observed values are reported in the following table … write the p-value of the test with H0: (the mean of X is equal to the mean of Y) versus a bilateral alternative hypothesis. ( 0,000341138) Exercise 12.10 The HR office of a cleaning company wants to test if there is a gender discrimination between its employees. Call X = the income of a set of 20 male workers and Y = the income of a set of 35 female workers. Write the line R command to run an appropriate test of hypothesis. Exercise 12.11 What is the power of statistical test? Exercise 12.12 Using the dataset boston.c downloaded from the library spdep, calculate the coefficient of skewness of the variable RM. Answer to Exercise 12.12: library(moments) skewness(boston.c$RM) 0,4024147 Exercise 12.13 How do you define the significance of a statistical test? 12.2 👨‍🎓 2021/2022 Exercise 12.14 Given the dataset “Duncan” in the library “carData” estimate the regression model where the variable prestige is regressed on the variables income Looking at the following information, Residuals: Min 1Q Median 3Q Max -29.538 -6.417 0.655 6.605 34.641 Do residuals display. Exercise 12.15 What are the consequences of collinearity among regressors? Estimators become biased Estimators become inefficient Estimators become inconsistent Estimators become unstable Exercise 12.16 What is the correct definition of the variance inflation factor i.e. VIF? \\(1-R2\\) \\(\\frac{1}{R2}\\) \\(\\frac{1}{1-R2}\\) \\(1-\\frac{1}{R2}\\) Answer to Exercise 12.16: A general guideline is that a VIF larger than 5 or 10 is large, indicating that the model has problems estimating the coefficient. However, this in general does not degrade the quality of predictions. If the VIF is larger than 1/(1-R2), where R2 is the Multiple R-squared of the regression, then that predictor is more related to the other predictors than it is to the response. install.packages(&quot;regclass&quot;) library(regclass) VIF(modello_regressione) alternatively you can use the library car and use vif() function install.packges(&quot;car&quot;) library(car) vif(modello_regressione) Exercise 12.17 Using only the following variables minority , crime , poverty , language highschool and housing of the Ericksen data in the library carData, run a factor analysis. What is the percentage explained by the first two factors? risposta: 90.130.001 Exercise 12.18 In a multiple linear regression model y= a+bx1+cx2, if Correlation(x1,x2)=0.9, do we have to discard one of the two variables for collinearity? risposta: F Exercise 12.19 Given the dataset Duncan in the library carData estimate the regression model where the variable prestige is regressed on the variables income and education. Which variable is the most significant? Education income Answer to Exercise 12.19: at first you load data from Duncan dataset library(carData) data(&quot;Duncan&quot;) Then you specify the model and produce sumamries: duncan_regression = lm(prestige~ income + education, data= Duncan) summary(duncan_regression) you look at pvalues and Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -6.06466 4.27194 -1.420 0.163 income 0.59873 0.11967 5.003 0.00001053 *** education 0.54583 0.09825 5.555 0.00000173 *** education is significant more than income since 0.00000173 &lt; 0.00001053 Exercise 12.20 In a multiple linear regression model y= a+bx1+cx2, what is the level of correlation between x1 and x2 beyond which we have to discard one of the two variables for collinearity? risposta: 0.948 Exercise 12.21 Given the dataset Duncan in the library carData estimate the regression model where the variable prestige is regressed on the variables income and education. What is the p-value of the coefficient of the variable education? Answer to Exercise 12.21: at first you load data from Duncan dataset library(carData) data(&quot;Duncan&quot;) Then you specify the model and produce sumamries: duncan_regression = lm(prestige~ income + education, data= Duncan) summary(duncan_regression) you look at pvalues and Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -6.06466 4.27194 -1.420 0.163 income 0.59873 0.11967 5.003 0.00001053 *** education 0.54583 0.09825 5.555 0.00000173 *** The pvalue for the coefficient is 0.00000173 you may want to directly access to it instead of just copying and pasting from console sumamry output Exercise 12.22 What is the reason for adjusting the R2 in a multiple regression To account for the number of degrees of freedom To account for the number of parameters To reduce the uncertainty To adjust for variance inflation factor rispoasta: To account for the number of degrees of freedom Exercise 12.23 Given the dataset Duncan in the library carData estimate the regression model where the variable prestige is regressed on the variables income. Using the VIF, do we have to exclude some variable due to collinearity? result: F Answer to Exercise 12.23: at first you load data from Duncan dataset library(carData) library(car) data(&quot;Duncan&quot;) Then you specify the model and produce sumamries: duncan_regression = lm(prestige~ income + education, data= Duncan) vif(duncan_regression) Then the output will look like something like. income education 2.1049 2.1049 Since they are below 10 which is the rule of thumb we gave to ourselves to assess multicollinearity then we conclude that neither income nor education are collinear. Exercise 12.24 Given the dataset Duncan in the library carData estimate the regression model where the variable prestige is regressed on the variables income. What is the value of the t value of the coefficient of the variable education? Answer to Exercise 12.24: at first you load data from Duncan dataset library(carData) data(&quot;Duncan&quot;) Then you specify the model and produce sumamries: duncan_regression = lm(prestige~ income + education, data= Duncan) summary(duncan_regression) Then the output will look like something like. Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -6.06466 4.27194 -1.420 0.163 income 0.59873 0.11967 5.003 0.00001053 *** education 0.54583 0.09825 5.555 0.00000173 *** By inspecting the summary wee obtain that the t value (t value column in the summary) dor variable education is 5.555 Exercise 12.24 Using only the following variables minority , crime , poverty , language, highschool and housing of the Ericksen data in the library carData, run a cluster analysis using the k-means method. If we divide the observations in 4 classes what is the frequency of the largest class ? result: 26 Exercise 12.25 Using only the following variables minority , crime , poverty , language, highschool and housing of the Ericksen data in the library carData, run a cluster analysis using the k-means method. What is the percentage explained by the first factor? risposta: 7.391.719 Exercise 12.26 Using only the following variables minority , crime , poverty , language, highschool and housing of the Ericksen data in the library carData, run a cluster analysis using the hierarchical method. If we divide the observations in 10 classes what is the frequency of the largest class ? risposta: 27 Exercise 12.27 Given the dataset Duncan in the library carData estimate the regression model where the variable prestige is regressed on the variables income and education and report the \\(R^2\\). Answer to Exercise 12.27: at first you load data from Duncan dataset library(carData) data(&quot;Duncan&quot;) Then you specify the model and produce sumamries: duncan_regression = lm(prestige~ income + education, data= Duncan) summary(duncan_regression) Then the output will look like something like. Residual standard error: 13.37 on 42 degrees of freedom Multiple R-squared: 0.8282, Adjusted R-squared: 0.82 F-statistic: 101.2 on 2 and 42 DF, p-value: &lt; 0.00000000000000022 By inspecting the lowe end of the summary we obtain that the R2 (multiple) for the model is 0.8282, which is high. 12.3 👨‍🎓 2022/2023 Exercise 12.28 Using the dataset Boston downloaded from the library spdep, calculate the coefficient of skewness of the variable RM. Exercise 12.29 How do you define the significance of a statistical test? Exercise 12.30 What is the power of statistical test? Exercise 12.31 How do you define the confidence of a statistical test? Exercise 12.32 A law company is evaluating the performances of two departments measuring in terms of the time required for solving a conflict in the last year. The observed values are reported in the following table: perf_table = data.frame( stringsAsFactors = FALSE, month = c(&quot;january&quot;,&quot;febraury&quot;,&quot;march&quot;, &quot;april&quot;,&quot;may&quot;,&quot;june&quot;,&quot;july&quot;,&quot;august&quot;,&quot;september&quot;, &quot;october&quot;,&quot;november&quot;,&quot;december&quot;), dept_1 = c(NA, NA, NA, 3L, 6L, 9L, 7L, 5L, 7L, 3L, 4L, 6L), dept_2 = c(4L, 3L, 9L, 5L, 7L, 2L, 6L, 3L, 6L, 7L, 4L, 1L) ) ) can we reject the hypothesis H0: (the mean of Dept 1 is equal to the mean of Dept 2) versus a bilateral alternative hypothesis? Exercise 12.33 A company has recorded the number of costumers in 10 sample stores before (variable X) and after (Variable Y) a new advertising campaign was introduced. The observed values are reported in the following table: stores = data.frame( n_store = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L), before = c(113L, 110L, 108L, 108L, 103L, 101L, 96L, 101L, 104L, 98L), after = c(125L, 113L, 115L, 117L, 105L, 112L, 100L, 103L, 116L, 104L) ) can we reject the hypothesis H0: (the mean of X, i.e. before is equal to the mean of Y, i.e. after) versus a bilateral alternative hypothesis? Exercise 12.34 Write the line of the R command that you use to simulate 2000 random observation from normal distribution with 0 mean and variance = 0.1 Many of you fall into this trap!. Tip: always use the “tab” for automatic suggestion but also check what are arguments. In this case exercise wants you to sample from a normal distribution with 2000 instances (data points), 0 mean and variance = 0.1. The argument in rnorm is sd not var, so you have to apply the square root! Answer to Question 12.34: rnorm(n = 2000, mean = 0, sd = 0.1^(1/2)) Exercise 12.35 Write the line of the R command that you use to produce a boxplot of the variable X Exercise 12.36 Given the following 2 variables X = (5,5,3,3,5,5) and Y= (4,4,3,3,3,3), test if the mean of X is significantly different from the mean of Y. Report the p-value of the appropriate test and your decision. Exercise 12.37 Using the dataset boston.c downloaded from the library spdep, write the elements of the correlation matrix of the variables MEDV, NOX and CRIM. Exercise 12.38 Without using formulae, describe how you can calculate the test statistics in a hypothesis testing procedure on a single mean with known variance. Exercise 12.39 The HR office of a cleaning company wants to test if there is significant difference in the salary between males and females. Call X = the salary of a set of 2000 male workers and Y = the salary of a set of 150 female workers. From previous survey we know that the variances of the two groups are equal. Write the line R command to run an appropriate test of hypothesis. Exercise 12.40 We want to test statistically the hypothesis that the students at UCSC in Rome have better performances in the second year than in first year year. Can we say that this is a paired sample test? Exercise 12.41 Using the dataset iris test if there is a significant difference between the mean of Petal.Length and the mean of Sepal.Width and report the outcome value of the t-test. Exercise 12.42 Using the dataset iris calculate the correlation between Sepal.Length and Sepal.Width. Exercise 12.43 Using the dataset iris report the highest correlation coefficient that you find between the four variables. Exercise 12.44 Using the dataset iris report the highest correlation coefficient that you find between the four variables. Exercise 12.45 Using the dataset iris report the variance of Sepal.Length Exercise 12.46 Using the dataset iris report the third quartile of Sepal.Length Exercise 12.47 What is the reason for adjusting the R2 in a multiple regression? Exercise 12.48 What is the correct definition of the variance inflation factor? Exercise 12.49 What are the consequences of collinearity among regressors? Exercise 12.50 Using the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. Which variables are retained in the model? (retained means tratteresti) Exercise 12.51 sing the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. What is the value of the adjusted R squared in the best model Exercise 12.52 sing the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. What is the estimated coefficient of the variable duration in the best model? Exercise 12.53 sing the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. What is the estimated value of the intercept in the best model? Exercise 12.54 sing the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. What is the p-value of the variable duration in the best model? Exercise 12.55 sing the dataset Wong from the R library carData, estimate a multiple linear regression where the variable piq is expressed as a function of age, days and duration. After the check of collinearity and of significance choose the best model. What is the value of the R square in the best model Exercise 12.56 Using the dataset iris, test if the average of the variable Sepal.Length changes significantly in the three Species considered. Report here the p-value of the appropiate test. We look at Species (we have already gone through that during lecture) by inspecting the dataset. What we see is that Species has three categories setosa, versicolor and virginica. If we would like to compare means across these 3 different categories we can’t use t.test() since they are 3. Instead we use ANOVA with the aov(). Sintax is similar to linear models. We saw this when we were trying to tackle “long” format data vs. “wide” format data. Answer to Question 12.56: test_species = aov(Sepal.Length~Species, data = iris) summary(test_species) resulting in 0.0000000000000002, very significant. We can conclude that: The ANOVA (formula: Sepal.Length ~ Species) suggests that the main effect of Species is statistically significant and large. Exercise 12.57 Using the dataset iris, test if the average of the variable Sepal.Length differs significantly in the three Species, Report here the value of the test statistic. 12.4 solutions Answer to Question 12.28: library(spdep) you are not required to load data in this case since the package already does it for you. SO you just need to type boston.c and you find it. Then you need to extract RM RM_var = boston.c$RM There are a nunber of packages that makes you able to compute skewness, there are some: e1071, moments, PerformanceAnalytics etc. I will suggest to use moments. So if you dont have it installed execute: install.packages(&quot;moments&quot;) library(moments) Then use teh function skewnes on RM_var skewness(RM_var) Answer to Question 12.29: The probability of type I error, The probability of rejecting H0 when H0 it is true Answer to Question 12.30: 1 minus the probability of type II error, The probability of accepting H0 when H0 it is true. Answer to Question 12.31: The probability of accepting H0 when H0 it is true. Answer to Question 12.32: H0: \\(\\mu_{dept1} = \\mu_{dept2}\\) H1: \\(\\mu_{dept1} \\neq \\mu_{dept2}\\) Remember you always test the alternative hypothesis H1. If the pvalue for the t test is not statistically significant then you reject H1 and conversely you accept H0, in this case means being the same (they are different but that’s because of randomness in data, i.e. sampling variation). t.test(x = perf_table$dept_1, y = perf_table$dept_2, paired = F, alternative = &quot;two.sided&quot;) Then we look at the p-value for this test and we see something like: 0.4076, so we can conclude that the Two Sample t-test testing the difference between dept_1 and dept_2 (mean of dept_1 = 5.56, mean of dept_2 = 4.75) suggests that the effect is positive, statistically not significant, and small. So we reject the alt. hypo H1 and accept H0. The question tells you if you can reject the Null Hypo, this is not the case since you just accepted it! Answer to Question 12.33: This is exactly the same reasoning as before except that this is a paired t test. “Are we talking about the same individuals? are we checking individuals pre and after a treatment?” YES t.test(x = stores$before, y = stores$after, paired = T, alternative = &quot;two.sided&quot;) Look at the p-value: p-value = 0.0004646.this is really small. SO we can conclude that the Paired t-test testing the difference between before and after (mean difference = -6.80) suggests that the effect is negative, statistically significant, and large. Answer to Question 12.34: rnorm(n = 2000, mean = 0, sd = 0.1^(1/2)) Answer to Question 12.35: boxplot(X) Answer to Question 12.36: you at first define vectors X and Y by executing: X = c(5,5,3,3,5,5) Y = c(4,4,3,3,3,3) t.test(X, Y, alternative = &quot;two.sided&quot;, paired = F) so the answer may look something like: The (Welch, remember we did not check variance so we rely on default R behavior applying a transformation to t.test) Sample t-test testing the difference between X and Y (mean of x = 4.33, mean of y = 3.33) suggests that the effect is positive, statistically not significant, and large given the pvalue being 0.0697. However this would also be significant if the alpha level of significance was 10%. Answer to Question 12.37: library(spdep) library(dplyr) new_boston = select(boston.c, MEDV, NOX, CRIM) cor(new_boston) Note that the principal diag for the matrix is all 1s. This is because you a variables has perfect correlation with itself. You are just interested in the upper triangle. You might also be interested in visualizing it with corrplot. Install it install.packages(\"corrplot\") then pass the matrix as the argument corrplot(cor(new_boston)) Answer to Question 12.38: the test statistic is calculated seeing, for example, how many times the absolute difference between the sample mean and the population mean (sm-mu) embodies the standard error = sqrt[(known variance)/n]. This value allow us to standardize the distribution and allocate the value in a Normal distribution (if the variance is known) or in a T di Student distribution (variance unknown) - looking at this value we can now calculate the probability that it is within the range of values established by the level of confidence of the statistical test. Answer to Question 12.39: t.test(X, Y, paired = F, alternative =&quot;less&quot;, var.equal = T) Answer to Question 12.40: FALSE Answer to Question 12.42: simple correlation aight?! cor(x = iris$Sepal.Length, y = iris$Sepal.Width) Answer to Question 12.43: Please note that correlation with cor() can be computed with only numeric values. Looking at iris you see the variable species which is a factor (aka grouping variable) we used that for ANOVA aov() when we are interested in comparing means across more than 2 groups. As a result you need to select all the variables but Species and do cor(). iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) cor(iris_filtr) there’s another way to do the filtering stuff, you just deselect Species such that: iris_filtr2 = select(iris, -Species) cor(iris_filtr2) You also might want to visualize the correlation as we did before (advanced trick): library(corrplot) iris_filtr2 = select(iris, -Species) corrplot::corrplot(cor(iris_filtr2)) Answer to Question 12.44: iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) cor(iris_filtr) there’s another way to do the filtering stuff, you just deselect Species such that: iris_filtr2 = select(iris, -Species) cor(iris_filtr2) Answer to Question 12.45: var(iris$Sepal.Length) Answer to Question 12.46: summary(iris$Sepal.Length) Answer to Question 12.47: Adjusted R2 is a corrected goodness-of-fit (model accuracy) measure for linear models. It identifies the percentage of variance in the target field that is explained by the input or inputs. R2 tends to optimistically estimate the fit of the linear regression. It always increases as the number of effects are included in the model. Adjusted R2 attempts to correct for this overestimation. Adjusted R2 might decrease if a specific effect does not improve the model. If you guessed the To account for the number of parameters this would also get you some points, But more precisely we are talking about the degrees of freedom. \\(R_{adj}^2 = 1- \\frac{(1-R^2)(n-1)}{n-k-1}\\) To account for the number of degrees of freedom! Answer to Question 12.48: As you may know Multicollinearity is problem that you can run into when you’re fitting a regression model, or other linear model. It refers to predictors that are correlated with other predictors in the model. Unfortunately, the effects of multicollinearity can feel murky and intangible, which makes it unclear whether it’s important to fix. Multicollinearity results in unstable parameter estimates which makes it very difficult to assess the effect of independent variables on dependent variables. Let’s see that from another pov: Consider the simplest case where \\(Y\\) is regressed against \\(X\\) and \\(Z\\) such that \\(Y = \\alpha + \\beta_1X +\\beta_2Z + \\epsilon\\) and where \\(Z\\) and \\(Z\\) are highly positively correlated. Then the effect of \\(X\\) on \\(Y\\) is hard to distinguish from the effect of \\(Z\\) on \\(Y\\) because any increase in \\(X\\) tends to be associated with an increase in \\(Z\\). Now let’s also consider th pathological case where \\(X = Z\\) highlights this further. \\(Y = \\alpha + \\beta_1X + \\beta_2Z + \\epsilon\\) -&gt; \\(Y = \\alpha + (\\beta_1 + \\beta_2)X + 0Z + \\epsilon\\) then both of the two variables would be indistinguishable. \\(\\frac{1}{1-R^2}\\) Answer to Question 12.49: Estimators become unstable Answer to Question 12.50: first attempt: library(carData) library(car) data(&quot;Wong&quot;) wong_regression = lm(piq ~ age + days + duration, data = Wong) summary(wong_regression) From here you see that age and days are not significant, indeed duration is. However days havign .13 as p values is much more significant than age which accounts for .38 Let’s also check collinearity for this uncorrectly specified model. They all look good since their values are all &lt;10. Then we mnay want to see how the model, behaves by cancelling out age and keeping days, so: wong_regression_2 = lm(piq ~ days + duration, data = Wong) In this iteration we verify that duration becomes even more important since now has ***. However days just got worst. We finally remove it too. We don’t check vif() we have already done that and we do not expect that a subset of non collinear varibales (as before) now become collinear. As a result:ù wong_regression_3 = lm(piq ~ duration, data = Wong) In the end we only retain duration Answer to Question 12.51: summary(wong_regression_3) resulting in 0.02618 Answer to Question 12.52: wong_regression_3$coefficients[2] resulting in -0.09918208 you can also directly look it through the summary summary(wong_regression_3) Answer to Question 12.53: wong_regression_3$coefficients[1] resulting in 88.97380549 you can also directly look it through the summary summary(wong_regression_3) Answer to Question 12.54: summary(wong_regression_3) resulting in duration p-value: 0.00183 Answer to Question 12.55: summary(wong_regression_3) result : 0.02913 Answer to Question 12.56: We look at Species (we have already gone through that during lecture) by inspecting the dataset. What we see is that Species has three categories setosa, versicolor and virginica. If we would like to compare means across these 3 different categories we can’t use t.test() since they are 3. Instead we use ANOVA with the aov(). Sintax is similar to linear models. We saw this when we were trying to tackle “long” format data vs. “wide” format data. test_species = aov(Sepal.Length~Species, data = iris) summary(test_species) resulting in 0.0000000000000002, very significant. We can conclude that: The ANOVA (formula: Sepal.Length ~ Species) suggests that the main effect of Species is statistically significant and large. Answer to Question 12.57: We take the exact same test as before and we look for the statistic summary(test_species) Here you look for the F statistics: F = 119.3 "],["course-performance.html", "Chapter 13 ⚡ Course Performance 13.1 🎓 24-25 Students Performance Analysis 13.2 🎓 20-21 Students 13.3 🎓 21-22 Students 13.4 🎓 22-23 Students 13.5 🎓 23-24 Students", " Chapter 13 ⚡ Course Performance In this place you can have a grasp on how your previous colleagues performed on past years in this course. Hopefully this will inspire you to do better and challenge your grade expectation. Data is extracted by Blackboard and have been carefully anonymized and securely stored to preserve privacy. 13.1 🎓 24-25 Students Performance Analysis Based on the data from the 2024-2025 academic year, here’s a comprehensive analysis of student performance: 13.1.1 Overall Statistics Table 13.1: Summary Statistics for 2024-2025 Academic Year Students Mean Grade Median Grade Std Dev Min Grade Max Grade Pass Rate (%) 49 23.2 26.75 7.63 1.5 32.25 77.6 13.1.2 Grade Distribution 13.1.3 Intermediate Exam Performance Table 13.2: Intermediate Exam Performance 1st Intermediate Mean 2nd Intermediate Mean 1st Intermediate Count 2nd Intermediate Count 23.26 27.07 41 30 13.1.4 Performance by Exam Type 13.1.5 Key Insights Overall Performance: The mean final grade for the 2024-2025 academic year was 23.2 out of 30. Pass Rate: 77.6% of students achieved a passing grade (≥18). Grade Range: Final grades ranged from 1.5 to 32.25. Intermediate Exams: Students performed similarly on both intermediate exams, with slight variations. Distribution: The grade distribution shows a normal distribution with most students scoring in the middle range. 13.2 🎓 20-21 Students 13.3 🎓 21-22 Students … soon to be analysed … 13.4 🎓 22-23 Students … soon to be analysed … 13.5 🎓 23-24 Students … soon to be analysed … "],["frequently-asked-questions.html", "Chapter 14 ❓ Frequently Asked Questions 14.1 Q n’A", " Chapter 14 ❓ Frequently Asked Questions So there are a number of frequent troubles that students get most frequently stucked on. Here a collection of them with relative answer. What I generally suggest is to look for online answer just coping and pasting the error message from console to Google. The most respected resource to look for when you are in trouble is Stackovverflow. 14.1 Q n’A I can’t really install via install.packages(\"&lt;package&gt;\"), it gives me some weird stuff I cant undestrand. have you check the syntax? Did you spell correctly the package name, capital letters matters you know it? still have the problem? try to install it from source, in most cases packages source code lies in GitHub. Look for the package author and name then do that: library(devtools) (if you still have not already installed the package devtools run install.packages(\"devtools) then execute library(devtools)). At this point run install_github(\"&lt;github package author username&gt;/&lt;package name&gt;\"). I did not mentioned it before because I give it fro granted: PLEASE BE SURE TO HAVE INTERNET CONNECTION ACCESS otherwise you are not going to dowload anything. So I have a file in local inside my R project and when I execute read.csv(\"&lt;path to file&gt;\") that says R cant find the files, so what’s up? most likely the problem is in the Working Directory ( check what Working Directory means 4.3). Load here with library(here) (if you still have not already installed the package here, then execute before install.packages(\"here\") then library(devtools)). At this point run here() and verify the file lies in the very same directory in the output. Are lectures recorded? &gt; Generally Yes, the lectures will be recorded and made available to enrolled students. It might happen sometimes that teacher forget to start registration, if you notice please raise your hand and say it! Will the videos be made available publicly? &gt; This we can’t do, we are sorry for that but it is internal policy Is attendance mandatory? &gt; We won’t be taking attendance but we expect to see you often in class. We love talking to students to understand how you are doing, make sure you get the most out of the class, and get your feedback to improve the materials. The class is relatively small so we will probably get to know each other well. If you have a time conflict and can’t attend the online lectures, please send us an email to let us know! What is the format of the class? &gt; It will be lectures, laboratories, and discussion. Occasionally depending on how fast we do lectures we might have some industry experts giving us tutorials or themed speeches. Do I need to know R for the course? &gt; Since R is the gold standard for statistics people, we expect most tutorials will be in R even though fluency isn’t required, but will make your life so much easier during the course. Are there any (group) assignments? &gt; We are still discussing about that, evemntually we will make it clear in one month max I have a question about the class. What is the best way to reach the course staff? &gt; please email your teaching assistant, Dr. Niccolo Salvini if it is related to R and laboratories, if that is not the case drop an email to Prof. Giuseppe Arbia I can’t install package OneTwoSamples, why? &gt; This is something that has to do with incopatible R version between built time and your current R version. If that cunfuses you don’t bother you are not going to use it. Instead you are going to use base R hyp testing with stats::t.test() or make use of infer, tidy statistical inferece. "],["StatisticsAndParameters.html", "Chapter 15 Symbols, formulas, statistics and parameters 15.1 Symbols and standard errors 15.2 Confidence intervals 15.3 Hypothesis testing 15.4 Other formulas 15.5 Other symbols used", " Chapter 15 Symbols, formulas, statistics and parameters 15.1 Symbols and standard errors Table 15.1: Some sample statistics used to estimate population parameters. Empty table cells means that these are not studied in this textbook. The dashes means that no formula is given in this textbook. Parameter Statistic Standard error S.E. formula reference Proportion \\(p\\) \\(\\hat{p}\\) \\(\\displaystyle\\text{s.e.}(\\hat{p}) = \\sqrt{\\frac{ \\hat{p} \\times (1 - \\hat{p})}{n}}\\) Def. ?? Mean \\(\\mu\\) \\(\\bar{x}\\) \\(\\displaystyle\\text{s.e.}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\) Def. ?? Standard deviation \\(\\sigma\\) \\(s\\) Mean difference \\(\\mu_d\\) \\(\\bar{d}\\) \\(\\displaystyle\\text{s.e.}(\\bar{d}) = \\frac{s_d}{\\sqrt{n}}\\) Def. ?? Diff. between means \\(\\mu_1 - \\mu_2\\) \\(\\bar{x}_1 - \\bar{x}_2\\) \\(\\displaystyle\\text{s.e.}(\\bar{x}_1 - \\bar{x}_2)\\) – Odds ratio Pop. OR Sample OR \\(\\displaystyle\\text{s.e.}(\\text{sample OR})\\) – Correlation \\(\\rho\\) \\(r\\) Slope of regression line \\(\\beta_1\\) \\(b_1\\) \\(\\text{s.e.}(b_1)\\) – Intercept of regression line \\(\\beta_0\\) \\(b_0\\) \\(\\text{s.e.}(b_0)\\) – R-squared \\(R^2\\) 15.2 Confidence intervals Almost all confidence intervals have the form \\[ \\text{statistic} \\pm ( \\text{multiplier} \\times \\text{s.e.}(\\text{statistic})). \\] Notes: The multiplier is approximately 2 for an approximate 95% CI (based on the 68–95–99.7 rule). \\(\\text{multiplier} \\times \\text{s.e.}(\\text{statistic})\\) is called the margin of error. Confidence intervals for odds ratios are slightly different, so this formula does not apply for odds ratios. For the same reason, a standard error for ORs is not given. 15.3 Hypothesis testing For many hypothesis tests, the test statistic is a \\(t\\)-score, which has the form: \\[ t = \\frac{\\text{statistic} - \\text{parameter}}{\\text{s.e.}(\\text{statistic})}. \\] Notes: Since \\(t\\)-scores are a little like \\(z\\)-scores, the 68–95–99.7 rule can be used to approximate \\(P\\)-values. Tests involving odds ratios do not use \\(t\\)-scores, so this formula does not apply for tests involving odds ratios. For tests involving odds ratios, the test statistic is a \\(\\chi^2\\) score and not \\(t\\)-score. For the same reason, a standard error for ORs is not given. The \\(\\chi^2\\) statistic is approximately like a \\(z\\)-score with a value of (where \\(\\text{df}\\) is the ‘degrees of freedom’ given in the software output): \\[ \\sqrt{\\frac{\\chi^2}{\\text{df}}}. \\] 15.4 Other formulas To estimate the sample size needed when estimating a proportion: \\(\\displaystyle n = \\frac{1}{(\\text{Margin of error})^2}\\). To estimate the sample size needed when estimating a mean: \\(\\displaystyle n = \\left( \\frac{2\\times s}{\\text{Margin of error}}\\right)^2\\). To calculate \\(z\\)-scores: \\(\\displaystyle z = \\frac{x - \\mu}{\\sigma}\\) or, more generally, \\(\\displaystyle z = \\frac{\\text{specific value of variable} - \\text{mean of variable}}{\\text{measure of variable&#39;s variation}}\\). The unstandardizing formula: \\(x = \\mu + (z\\times \\sigma)\\). Notes: In sample size calculations, always round up the sample size found from the above formulas. 15.5 Other symbols used Table 15.2: Some symbols used Symbol Meaning Reference \\(H_0\\) Null hypothesis Sect. ?? \\(H_1\\) Alternative hypothesis Sect. ?? df Degrees of freedom Sect. ?? CI Confidence interval Chap. ?? s.e. Standard error Def. ?? \\(n\\) Sample size \\(\\chi^2\\) The chi-squared test statistic Sect. ?? "],["appendixdatasets.html", "Chapter 16 Datasets 16.1 Built-in R Datasets 16.2 Course-Specific Datasets 16.3 How to Access Datasets 16.4 Dataset Usage in Course 16.5 Additional Resources", " Chapter 16 Datasets This section provides information about the datasets used throughout the course. These datasets come from various R packages and are commonly used in statistical analysis and data science education. 16.1 Built-in R Datasets 16.1.1 mtcars - Motor Trend Car Road Tests Package: datasets Description: Data extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). Variables: - mpg: Miles/(US) gallon - cyl: Number of cylinders - disp: Displacement (cu.in.) - hp: Gross horsepower - drat: Rear axle ratio - wt: Weight (1000 lbs) - qsec: 1/4 mile time - vs: Engine (0 = V-shaped, 1 = straight) - am: Transmission (0 = automatic, 1 = manual) - gear: Number of forward gears - carb: Number of carburetors 16.1.2 iris - Edgar Anderson’s Iris Data Package: datasets Description: Famous dataset giving the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. Variables: - Sepal.Length: Sepal length in cm - Sepal.Width: Sepal width in cm - Petal.Length: Petal length in cm - Petal.Width: Petal width in cm - Species: Species of iris (setosa, versicolor, virginica) 16.1.3 cars - Speed and Stopping Distances of Cars Package: datasets Description: The data give the speed of cars and the distances taken to stop. Variables: - speed: Speed (mph) - dist: Stopping distance (ft) 16.2 Course-Specific Datasets 16.2.1 bikeshare.csv - Bike Sharing Data File: data/bikeshare.csv Description: Dataset containing bike sharing information with various weather and temporal features. Variables: - datetime: Date and time - season: Season (1:spring, 2:summer, 3:fall, 4:winter) - holiday: Whether day is holiday or not - workingday: If day is neither weekend nor holiday - weather: Weather situation (1: Clear, 2: Mist, 3: Light Snow/Rain, 4: Heavy Rain) - temp: Temperature in Celsius - atemp: “Feels like” temperature in Celsius - humidity: Relative humidity - windspeed: Wind speed - casual: Count of casual users - registered: Count of registered users - count: Count of total rental bikes 16.2.2 stroke_data.csv - Stroke Prediction Data File: data/stroke_data.csv Description: Dataset for predicting stroke occurrence based on various health indicators. Variables: - id: Unique identifier - gender: Gender - age: Age - hypertension: Hypertension (0: No, 1: Yes) - heart_disease: Heart disease (0: No, 1: Yes) - ever_married: Ever married (No, Yes) - work_type: Type of work - Residence_type: Residence type (Rural, Urban) - avg_glucose_level: Average glucose level - bmi: Body mass index - smoking_status: Smoking status - stroke: Stroke (0: No, 1: Yes) 16.2.3 coronary.dta - Coronary Heart Disease Data File: data/coronary.dta Description: Dataset containing information about coronary heart disease patients. Variables: - Various medical and demographic variables related to coronary heart disease - Used for survival analysis and medical statistics 16.3 How to Access Datasets 16.3.1 Built-in R Datasets # Load built-in datasets data(mtcars) data(iris) data(cars) # View dataset structure str(mtcars) head(mtcars) 16.3.2 Course Datasets # Load course datasets bikeshare &lt;- read.csv(&quot;data/bikeshare.csv&quot;) stroke_data &lt;- read.csv(&quot;data/stroke_data.csv&quot;) # View dataset structure str(bikeshare) head(bikeshare) 16.4 Dataset Usage in Course mtcars: Used for linear regression examples and correlation analysis iris: Used for classification, clustering, and ANOVA examples cars: Used for simple linear regression demonstrations bikeshare: Used for time series analysis and multiple regression stroke_data: Used for logistic regression and classification examples coronary: Used for survival analysis examples 16.5 Additional Resources For more information about these datasets: - R documentation: ?mtcars, ?iris, ?cars - Course materials and exercises - Statistical analysis examples in course chapters "],["references-2.html", "References 16.6 Statistical Methods and R Programming 16.7 Advanced R Programming 16.8 Statistical Software and Tools 16.9 Course Materials 16.10 Additional Resources", " References 16.6 Statistical Methods and R Programming Everitt, B., Hothorn, T. (2011). An Introduction to Applied Multivariate Analysis with R. Springer-Verlag. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2015). An Introduction to Statistical Learning, with Applications in R. Springer. Timbers, T., Campbell, T., &amp; Lee, M. (2022). Data Science: A First Introduction. Online version Wickham, H., &amp; Grolemund, G. (2018). R for Data Science. O’Reilly. Freely available online Dauber, D. (2022). R for non-programmers. Free book 16.7 Advanced R Programming Higgins, P. D. R. (2022). Reproducible Medical Research with R. Free book Armstrong, J. K. (2022). Fundamentals of Wrangling Healthcare Data with R. Free book Wickham, H. (2015). Advanced R. CRC Press. Free book 16.8 Statistical Software and Tools R Core Team (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. RStudio Team (2024). RStudio: Integrated Development Environment for R. RStudio, PBC, Boston, MA. Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC. 16.9 Course Materials Salvini, N. (2025). Statistics &amp; Big Data 25-26 Labs. Course website and materials. Dabo-Niang, S. (2025). Advanced Modeling Techniques. Intensive session materials. 16.10 Additional Resources CRAN Task Views: https://cran.r-project.org/web/views/ R-bloggers: https://www.r-bloggers.com/ Stack Overflow R Tag: https://stackoverflow.com/questions/tagged/r R Documentation: https://www.rdocumentation.org/ "]]
