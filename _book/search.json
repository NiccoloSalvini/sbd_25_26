[{"path":"index.html","id":"about","chapter":"1 ğŸ«¶ About","heading":"1 ğŸ«¶ About","text":" official course website Statistics & Big data2 2024 - 2025 laboratories. website augments lecture topics provides exercises home class assignments. Additional theory wrt slides textbook part exam, indeed growth hopefully future quicksilver resource recover R proficiency lethargy.","code":""},{"path":"index.html","id":"logistics","chapter":"1 ğŸ«¶ About","heading":"1.1 ğŸ”§ Logistics","text":"Lectures: Mon 14:00 - 14:00 CET, room: 101 (?!) Class: 75% lectures, 25% tutorials.Location: Campus Gemelli /optionally remoteOffice hours:\nProf Giuseppe Arbia : Mon 12:00 - 13:00 CET\nProf Sophie Dabo-Niang: â€“ . â€“\nDr.Â NiccolÃ² Salvini: office times = lecture times (reach email)\nProf Giuseppe Arbia : Mon 12:00 - 13:00 CETProf Sophie Dabo-Niang: â€“ . â€“Dr.Â NiccolÃ² Salvini: office times = lecture times (reach email)â€™s shared drive (slides notebooks extra class)","code":""},{"path":"index.html","id":"team","chapter":"1 ğŸ«¶ About","heading":"1.2 ğŸ‘¥ Team","text":"","code":""},{"path":"index.html","id":"labs-content","chapter":"1 ğŸ«¶ About","heading":"1.3 ğŸ—’ labsâ€™ contents","text":"Introduction R ecosystem.\nInstall R\nInstall R Studio works\nR tricks might useful reasearch professional life\ndata wrangling R\nInstall RInstall R Studio worksSome R tricks might useful reasearch professional lifedata wrangling RBasic statistics (Descriptive statistics. Point interval estimation, test statistical hypotheses average percentage).Hypothesis testing 2 averages 2 percentages. - Hypothesis testing 2 averages (ANOVA) 2 percentages (CHI square).Multiple linear regression model.Nonlinear regression.Regression dummy variables.Binomial multinomial logistic regression. Factor analysis. Cluster analysis.","code":""},{"path":"index.html","id":"exam","chapter":"1 ğŸ«¶ About","heading":"1.4 Exam ğŸ“","text":"exam going open closed questions theory practice (coding part). asked provide results sometimes code leading results. can also asked directly provide code solve exercise. exam going take place labs classroom, means going laptop exam. generally donâ€™t provide assignment neither group works. Indeed provide intermediate exams want try .going 2 intermediate sessions exams half whole content course. means:\n- first intermediate: happen typically November Prof.Â Arbia contents (well labs)\n- second intermediate: happen January contents taught hosting teacher (well labs)can take first intermediate take second exam date within winter session, meaning take part 1 Nov part 2 either Jan Feb.Â can reject intermediates, means take first part, try second perform well, need take full. Grades may undergo review process official particularly low. happened quite often, happen every time.","code":""},{"path":"index.html","id":"suggested-reading","chapter":"1 ğŸ«¶ About","heading":"1.5 ğŸ“š Suggested reading list","text":"going split resources expected level audience:","code":""},{"path":"index.html","id":"minimal-or-0-knowledge-of-r","chapter":"1 ğŸ«¶ About","heading":"1.5.1 Minimal or 0 knowledge of R","text":"Everitt, B., Hothorn, T. (2011) Introduction Applied Multivariate Analysis R, Springer-VerlagJames, G, Witten, D, Hastie, T Tibshirani, R, (2015) Introduction Statistical Learning, Applications RT. Timbers, T. Campbell, M. Lee Data Science: First Introduction, Jul 2022 online versionWickham, H., Grolemund G. (2018) R Data Science, Oâ€™Reilly. Freely available -line https://r4ds..co.nz/index.htmlR non-programmers, Daniel Dauber 2022, free book","code":""},{"path":"index.html","id":"advanced-knowledge-of-r-to-become-a-top-g","chapter":"1 ğŸ«¶ About","heading":"1.5.2 Advanced knowledge of R to become a top G","text":"Reproducible Medical Research R,Peter D.R. Higgins, MD, PhD, MSc, 2022, free bookFundamentals Wrangling Healthcare Data R, J. Kyle Armstrong 2022, free book AdvancedWickham H. (2015). Advanced r. CRC Press free book","code":""},{"path":"index.html","id":"honorcode","chapter":"1 ğŸ«¶ About","heading":"1.6 ğŸ“œ Honor Code","text":"Permissive strict. unsure, please ask course staff!OKAY Pleeease, using ChatGPT derivatives daily basis. understand, â€™s awesome enforcing rule home. Donâ€™t exam.OK search, ask public systems â€™re studying. Cite resources reference.\nE.g. read paper, cite . ask Quora, include link.OKAY ask someone assignments/projects , monitoring freelancing websites, plethora bots job daily.OK discuss questions classmates. Disclose discussion partners.OKAY blindly copy solutions classmates.OK use existing solutions part projects/assignments. Clarify contributions.OKAY pretend someoneâ€™s solution .OK publish final project course (encourage need love help !)OKAY post assignment solutions online.","code":""},{"path":"index.html","id":"qr-code-time","chapter":"1 ğŸ«¶ About","heading":"1.7 QR code time!","text":"","code":""},{"path":"index.html","id":"intro-colophon","chapter":"1 ğŸ«¶ About","heading":"1.8 Colophon","text":"book authored using bookdown inside RStudio bs4 theme\nwebsite hosted Netlify, automatically updated Netlify CI.\ncomplete source available GitHub.version book built :","code":"\nlibrary(devtools)\n#> Loading required package: usethis\nlibrary(roxygen2)\nlibrary(testthat)\n#> \n#> Attaching package: 'testthat'\n#> The following object is masked from 'package:devtools':\n#> \n#>     test_file\n#> The following object is masked from 'package:dplyr':\n#> \n#>     matches\ndevtools::session_info()\n#> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS 14.5\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Rome\n#>  date     2024-10-26\n#>  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#>  package      * version    date (UTC) lib source\n#>  bookdown       0.29       2022-09-12 [1] CRAN (R 4.2.0)\n#>  brio           1.1.3      2021-11-30 [1] CRAN (R 4.2.0)\n#>  bslib          0.5.1      2023-08-11 [1] CRAN (R 4.2.0)\n#>  cachem         1.0.8      2023-05-01 [1] CRAN (R 4.2.0)\n#>  callr          3.7.3      2022-11-02 [1] CRAN (R 4.2.0)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.2.3)\n#>  colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.2.0)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.2.0)\n#>  desc           1.4.2      2022-09-08 [1] CRAN (R 4.2.0)\n#>  devtools     * 2.4.3      2021-11-30 [1] CRAN (R 4.2.0)\n#>  digest         0.6.33     2023-07-07 [1] CRAN (R 4.2.0)\n#>  downlit        0.4.2      2022-07-05 [1] CRAN (R 4.2.0)\n#>  dplyr        * 1.1.2      2023-04-20 [1] CRAN (R 4.2.0)\n#>  ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.2.0)\n#>  evaluate       0.22       2023-09-29 [1] CRAN (R 4.2.0)\n#>  fansi          1.0.4      2023-01-22 [1] CRAN (R 4.2.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.2.0)\n#>  fs             1.6.3      2023-07-20 [1] CRAN (R 4.2.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.2.0)\n#>  glue         * 1.6.2      2022-02-24 [1] CRAN (R 4.2.0)\n#>  htmltools      0.5.6.1    2023-10-06 [1] CRAN (R 4.2.0)\n#>  httr           1.4.6      2023-05-08 [1] CRAN (R 4.2.0)\n#>  jquerylib      0.1.4      2021-04-26 [1] CRAN (R 4.2.0)\n#>  jsonlite       1.8.7      2023-06-29 [1] CRAN (R 4.2.0)\n#>  kableExtra   * 1.3.4.9000 2023-06-01 [1] Github (kupietz/kableExtra@3bf9b21)\n#>  knitr        * 1.44       2023-09-11 [1] CRAN (R 4.2.0)\n#>  lifecycle      1.0.3      2022-10-07 [1] CRAN (R 4.2.0)\n#>  lubridate    * 1.9.2      2023-02-10 [1] CRAN (R 4.2.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.2.0)\n#>  memoise        2.0.1      2021-11-26 [1] CRAN (R 4.2.0)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.2.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.2.0)\n#>  pkgbuild       1.4.2      2023-06-26 [1] CRAN (R 4.2.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.2.0)\n#>  pkgload        1.2.4      2021-11-30 [1] CRAN (R 4.2.0)\n#>  prettyunits    1.1.1      2020-01-24 [1] CRAN (R 4.2.0)\n#>  processx       3.8.1      2023-04-18 [1] CRAN (R 4.2.0)\n#>  ps             1.7.5      2023-04-18 [1] CRAN (R 4.2.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.2.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.2.0)\n#>  remotes        2.4.2      2021-11-30 [1] CRAN (R 4.2.0)\n#>  rlang          1.1.2      2023-11-04 [1] CRAN (R 4.2.0)\n#>  rmarkdown      2.25       2023-09-18 [1] CRAN (R 4.2.0)\n#>  roxygen2     * 7.2.0      2022-05-13 [1] CRAN (R 4.2.0)\n#>  rprojroot      2.0.3      2022-04-02 [1] CRAN (R 4.2.0)\n#>  rstudioapi     0.14       2022-08-22 [1] CRAN (R 4.2.0)\n#>  rvest          1.0.3      2022-08-19 [1] CRAN (R 4.2.0)\n#>  sass           0.4.6      2023-05-03 [1] CRAN (R 4.2.0)\n#>  scales         1.2.1      2022-08-20 [1] CRAN (R 4.2.0)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.2.0)\n#>  stringi        1.7.12     2023-01-11 [1] CRAN (R 4.2.0)\n#>  stringr        1.5.0      2022-12-02 [1] CRAN (R 4.2.0)\n#>  svglite        2.1.1      2023-01-10 [1] CRAN (R 4.2.0)\n#>  systemfonts    1.0.4      2022-02-11 [1] CRAN (R 4.2.0)\n#>  testthat     * 3.1.4      2022-04-26 [1] CRAN (R 4.2.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.2.0)\n#>  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.2.0)\n#>  timechange     0.2.0      2023-01-11 [1] CRAN (R 4.2.0)\n#>  usethis      * 2.1.6      2022-05-25 [1] CRAN (R 4.2.0)\n#>  utf8           1.2.3      2023-01-31 [1] CRAN (R 4.2.0)\n#>  vctrs          0.6.3      2023-06-14 [1] CRAN (R 4.2.0)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.2.0)\n#>  webexercises * 1.0.0      2021-09-15 [1] CRAN (R 4.2.0)\n#>  webshot        0.5.4      2022-09-26 [1] CRAN (R 4.2.0)\n#>  withr          2.5.1      2023-09-26 [1] CRAN (R 4.2.0)\n#>  xfun           0.40       2023-08-09 [1] CRAN (R 4.2.0)\n#>  xml2           1.3.4      2023-04-27 [1] CRAN (R 4.2.0)\n#>  yaml           2.3.7      2023-01-23 [1] CRAN (R 4.2.0)\n#> \n#>  [1] /Users/niccolo/Library/R/arm64/4.2/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"prereq.html","id":"prereq","chapter":"2 âœ¨Prerequisites","heading":"2 âœ¨Prerequisites","text":"","code":""},{"path":"prereq.html","id":"setting-up-r-and-rstudio","chapter":"2 âœ¨Prerequisites","heading":"2.1 Setting up R and RStudio","text":"get started R, need acquire copy. appendix show download R well RStudio, software application makes R easier use. â€™ll go downloading R opening first R session. Use menu right hand side page select OS follow correct installation.R RStudio free easy download. feel comfortable interacting videos instead reading please visit  interactive tutorials  guide full R set !","code":""},{"path":"prereq.html","id":"how-to-download-and-install-r","chapter":"2 âœ¨Prerequisites","heading":"2.2 How to Download and Install R","text":"R maintained international team developers make language available web page Comprehensive R Archive Network .e.Â CRAN. top web page provides three links downloading R. Follow link describes operating system: Windows, Mac, Linux.","code":""},{"path":"prereq.html","id":"r-in-windows","chapter":"2 âœ¨Prerequisites","heading":"2.2.1 R in Windows","text":"install R Windows, click â€œDownload R Windowsâ€ link. click â€œbaseâ€ link. Next, click first link top new page. link say something like â€œDownload R 3.0.3 Windows,â€ except 3.0.3 replaced current version R. link downloads installer program, installs --date version R Windows. Run program step installation wizard appears. wizard install R program files folders place shortcut Start menu. Note â€™ll need appropriate administration privileges install new software machine. (detailed steps)","code":""},{"path":"prereq.html","id":"r-in-mac","chapter":"2 âœ¨Prerequisites","heading":"2.2.2 R in Mac","text":"also setup, feel free reach mail address something messed .Go www.r-project.org\nFigure 2.1: R mirrors website\nClick CRAN says Download.Click CRAN says Download.Choose server country (work, downloads perform quicker choose country one close ).Choose server country (work, downloads perform quicker choose country one close ).\nFigure 2.2: CRAN mirrors\nSelect operating system computer, example Download R macOS.\nFigure 2.3: OS choices available\nSelect version want install (recommend latest version)\nFigure 2.4: R versions available\nOpen downloaded file follow installation instructions. recommend leaving suggested settings .Binaries Versus SourceR can installed precompiled binaries built source operating system. Windows Mac machines, installing R binaries extremely easy. binary comes preloaded installer. Although can build R source platforms, process much complicated wonâ€™t provide much benefit users. Linux systems, opposite true. Precompiled binaries can found systems, much common build R source files installing Linux. download pages CRANâ€™s website provide information building R source Windows, Mac, Linux platforms.","code":""},{"path":"prereq.html","id":"r-in-linux","chapter":"2 âœ¨Prerequisites","heading":"2.2.3 R in Linux","text":"R comes preinstalled many Linux systems, â€™ll want newest version R date. CRAN website provides files build R source Debian, Redhat, SUSE, Ubuntu systems link â€œDownload R Linux.â€ Click link follow directory trail version Linux wish install . exact installation procedure vary depending Linux system use. CRAN guides process grouping set source files documentation README files explain install system.32-bit Versus 64-bitR comes 32-bit 64-bit versions. use? cases, wonâ€™t matter. versions use 32-bit integers, means compute numbers numerical precision. difference occurs way version manages memory. 64-bit R uses 64-bit memory pointers, 32-bit R uses 32-bit memory pointers. means 64-bit R larger memory space use (search ).\nrule thumb, 32-bit builds R faster 64-bit builds, though always. hand, 64-bit builds can handle larger files data sets fewer memory management problems. either version, maximum allowable vector size tops around 2 billion elements. operating system doesnâ€™t support 64-bit programs, RAM less 4 GB, 32-bit R . Windows Mac installers automatically install versions system supports 64-bit R.","code":""},{"path":"prereq.html","id":"using-r","chapter":"2 âœ¨Prerequisites","heading":"2.3 Using R","text":"R isnâ€™t program can open start using, like Microsoft Word Internet Explorer. Instead, R computer language, like C, C++, UNIX. use R writing commands R language asking computer interpret . old days, people ran R code UNIX terminal windowâ€”hackers movie 1980s. Now almost everyone uses R application called RStudio, recommend , .R UNIXYou can still run R UNIX BASH window (prompt Powershell) typing command:\nR\nopens R interpreter. can work close interpreter running q() finished.","code":""},{"path":"prereq.html","id":"using-rstudio","chapter":"2 âœ¨Prerequisites","heading":"2.4 Using RStudio","text":"R just â€˜beating heartâ€™ R programming, particular user interface. may heard saying stuff like: â€œR engine car, indeed RStudio car body, â€™s true, just donâ€™t need engine donâ€™t car body. say: want buttons click actually â€˜seeâ€™ , better way RStudio.\nRStudio integrated development environment (IDE) primary tool interact R. software need fun parts , course, follow along examples book.\nmay ask Posit, fair question. Back days Posit, company behind RStudio, actually named RStudio (product). 2023 rebranded Posit also include languages like Python,Howeveeeer install RStudio perform following steps:Go https://posit.co/\nFigure 2.5: Posit.co main page\nGo DOWNLOAD RSTUDIO upper right corner (download R still havenâ€™t).Go DOWNLOAD RSTUDIO upper right corner (download R still havenâ€™t).Select DOWNLOAD RSTUDIO, just left DOWNLOAD RSTUDIO SERVER.Select DOWNLOAD RSTUDIO, just left DOWNLOAD RSTUDIO SERVER.\nFigure 2.6: Choose RStudio version\npage, scroll select Download (download column) corresponding OS (mind different versions OS, say macOS 11.2 macOS 8.3 need different RStudio download installations).\nFigure 2.7: Choose RStudio version\nOpen downloaded file follow installation instructions. , keep default settings much possible.Congratulations, set learn R. now need start RStudio R. course, curious type, nothing shall stop try R without RStudio.","code":""},{"path":"prereq.html","id":"when-you-first-start-rstudio","chapter":"2 âœ¨Prerequisites","heading":"2.5 When you first start RStudio","text":"start programming away, might want make tweaks settings right away better experience (humble opinion). open Rstudio settings click onRStudio > Preferences press âŒ˜ + , Mac.RStudio > Preferences press âŒ˜ + , Mac.RStudio > Tools > Global Options press Ctrl + , work Windows computer.RStudio > Tools > Global Options press Ctrl + , work Windows computer.recommend least make following changes set success right beginning:Already first tab, .e.Â General > Basic, make one significant changes. Deactivate every option starts Restore. ensure every time start RStudio, begin clean slate. first sight, might sound counter-intuitive restart everything left , essential make projects easily reproducible. Furthermore, work together others, restoring personal settings also ensures programming works across different computers. Therefore, recommend following unticked:\nRestore recently opened project startup,\nRestore previsouly open source documents startup,\nRestore .Rdata workspace startup\nAlready first tab, .e.Â General > Basic, make one significant changes. Deactivate every option starts Restore. ensure every time start RStudio, begin clean slate. first sight, might sound counter-intuitive restart everything left , essential make projects easily reproducible. Furthermore, work together others, restoring personal settings also ensures programming works across different computers. Therefore, recommend following unticked:Restore recently opened project startup,Restore recently opened project startup,Restore previsouly open source documents startup,Restore previsouly open source documents startup,Restore .Rdata workspace startupRestore .Rdata workspace startup\nFigure 2.8: get RStudio preferences\ntab Workspace, select Never setting Save workspace .RData exit. One might think wise keep intermediary results stored one R session another. However, often found fixing issues due lazy method, code became less reliable , therefore, reproducible. experience, find avoids many headaches.tab Workspace, select Never setting Save workspace .RData exit. One might think wise keep intermediary results stored one R session another. However, often found fixing issues due lazy method, code became less reliable , therefore, reproducible. experience, find avoids many headaches.Code > Editing tab, make sure least first five options ticked, especially Auto-indent code paste. setting save time trying format coding appropriately, making easier read. Indentation primary way making code look readable less like series characters appear almost random.Code > Editing tab, make sure least first five options ticked, especially Auto-indent code paste. setting save time trying format coding appropriately, making easier read. Indentation primary way making code look readable less like series characters appear almost random.\nFigure 2.9: Pimp RStudio IDE\nDisplay tab, might want first three options selected. particular, Highlight selected line helpful , complicated code, helpful see cursor .\nFigure 2.10: Edit RStudio display preferences\ncourse, wish customise workspace , can . visually impactful way alter default appearance RStudio select Appearance pick completely different colour theme. Feel free browse various options see prefer. right wrong . Just make .\nFigure 2.11: get instantly nerd\n","code":""},{"path":"prereq.html","id":"updating-r-and-rstudio","chapter":"2 âœ¨Prerequisites","heading":"2.6 Updating R and RStudio: Living at the pulse of innovation","text":"strictly something helps become better programmer, advice might come handy avoid turning frustrated programmer. update software, need update R RStudio separately . R RStudio work closely , still constitute separate pieces software. Thus, essential keep mind updating RStudio automatically update R. can become problematic specific tools installed via RStudio (like fancy learning algorithm) might compatible earlier versions R. Also, additional R packages (see Chapter 3) developed developers separate pieces require updating , independently R RStudio.know thinking: already sounds complicated cumbersome. However, rest assured, take look can easily update packages RStudio. Thus, need remember : R needs updated separately everything else.","code":""},{"path":"r-packages.html","id":"r-packages","chapter":"3 ğŸ“¦ R Packages","heading":"3 ğŸ“¦ R Packages","text":"Many Râ€™s useful functions come preloaded start R, reside packages can installed top R. R packages similar libraries C, C++, Javascript, packages Python, gems Ruby. R package bundles together useful functions, help files, data sets. can use functions within R code load package live . Usually contents R package related single type task, package helps solve. R packages let take advantage Râ€™s useful features: large community package writers (many active data scientists) prewritten routines handling many common (exotic) data-science tasks.Base R\nmay hear R users () refer â€œbase R.â€ base R? just collection R functions gets loaded every time start R. functions provide basics language, donâ€™t load package can use .","code":""},{"path":"r-packages.html","id":"installing-packages","chapter":"3 ğŸ“¦ R Packages","heading":"3.1 Installing Packages","text":"use R package, must first install computer load current R session. easiest way install R package install.packages R function. Open R type following command line:search specified package collection packages hosted CRAN site. R finds package, download libraries folder computer. R can access package future R sessions without reinstalling . Anyone can write R package disseminate like; however, almost R packages published CRAN website. CRAN tests R package publishing . doesnâ€™t eliminate every bug inside package, mean can trust package CRAN run current version R OS.can install multiple packages linking names Râ€™s concatenate function, c.Â example, install ggplot2, reshape2, dplyr packages, run:first time installing package, R prompt choose online mirror install . Mirrors listed location. downloads quickest select mirror close . want download new package, try Austria mirror first. main CRAN repository, new packages can sometimes take couple days make around mirrors.","code":"\ninstall.packages(\"<package name>\")\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"carData\", \"spdep\"))"},{"path":"r-packages.html","id":"loading-packages","chapter":"3 ğŸ“¦ R Packages","heading":"3.2 Loading Packages","text":"Installing package doesnâ€™t immediately place functions fingertips. just places computer. use R package, next load R session command:Notice quotation marks disappeared. can use like, quotation marks optional library command. (true install.packages command).library make packageâ€™s functions, data sets, help files available close current R session. next time begin R session, â€™ll reload package library want use , wonâ€™t reinstall . install package . , copy package live R library. see packages currently R library, run:library() also shows path actual R library, folder contains R packages. may notice many packages donâ€™t remember installing. R automatically downloads set useful packages first install R.Install packages (almost) anywhereThe devtools R package makes easy install packages locations CRAN website. devtools provides functions like install_github, install_gitorious, install_bitbucket, install_url. work similar install.packages, search new locations R packages. install_github especially useful many R developers provide development versions packages GitHub. development version package contain sneak peek new functions patches may stable bug free CRAN version.R make bother installing loading packages? can imagine R every package came preloaded, large slow program. May 6, 2014, CRAN website hosts 5,511 packages. simpler install load packages want use want use . keeps copy R fast fewer functions help pages search one time. arrangement benefits well. example, possible update copy R package without updating entire copy R.â€™s best way learn R packages?difficult use R package donâ€™t know exists. go CRAN website click Packages link see list available packages, â€™ll wade thousands . Moreover, many R packages things.know package best? R-packages mailing list place start. sends announcements new packages maintains archive old announcements. Blogs aggregate posts R can also provide valuable leads. recommend R-bloggers. RStudio maintains list useful R packages Getting Started section http://support.rstudio.com. Finally, CRAN groups together usefulâ€”respectedâ€”packages subject area. excellent place learn packages designed area work.","code":"\nlibrary(\"<package name>\")\nlibrary()"},{"path":"r-packages.html","id":"updating-r-and-its-packages","chapter":"3 ğŸ“¦ R Packages","heading":"3.3 Updating R and Its Packages","text":"R Core Development Team continuously hones R language catching bugs, improving performance, updating R work new technologies. result, new versions R released several times year. easiest way stay current R periodically check CRAN website. website updated new release makes release available download. â€™ll install new release. process first installed R.Donâ€™t worry â€™re interested staying --date R Coreâ€™s doings. R changes slightly releases, â€™re likely notice differences. However, updating current version R good place start ever encounter bug canâ€™t explain.RStudio also constantly improves product. can acquire newest updates just downloading RStudio.","code":""},{"path":"r-packages.html","id":"r-packages-1","chapter":"3 ğŸ“¦ R Packages","heading":"3.3.1 R Packages","text":"Package authors occasionally release new versions packages add functions, fix bugs, improve performance. update.packages command checks whether current version package installs current version . syntax update.packages follows install.packages. already ggplot2, reshape2, dplyr computer, â€™d good idea check updates use :start new R session updating packages. package loaded update , â€™ll close R session open new one begin using updated version package.","code":"\nupdate.packages(c(\"ggplot2\", \"dplyr\", \"carData\", \"spdep\"))"},{"path":"nice-warm-up.html","id":"nice-warm-up","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4 ğŸ”¥ Nice warm-up","text":"Now going cover basic operations computer science concepts R. Hopefully get really cool starter pack function might reuse throughout R journey.","code":""},{"path":"nice-warm-up.html","id":"starting-your-fresh-new-r-project","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4.1 Starting your fresh new R project","text":"Every fresh attempt likely pique interest pique emotions. . uncover answers research questions, become knowledgeable consequence. However, likely dislike certain aspects data analysis. Two examples spring mind:Keeping track files generated projectA Keeping track files generated projectB Data manipulationB Data manipulationWhile go deeper detail data manipulation later chapter, â€™d like share ideas work helped stay organized , result, less frustrated. following applicable small large research projects, making extremely useful regardless circumstance size project.","code":""},{"path":"nice-warm-up.html","id":"creating-an-r-project","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4.2 Creating an R Project file","text":"working project, likely create many different files various purposes, especially R Scripts (File > New File > R Script). careful, file stored systemâ€™s default location, might want . RStudio allows manage entire project intuitively conveniently R Project files. Using R Project files comes couple perks, example:files create saved location. data, coding, exported charts, reports, one location, donâ€™t maintain files manually. RStudio sets root directory folder project stored.files create saved location. data, coding, exported charts, reports, one location, donâ€™t maintain files manually. RStudio sets root directory folder project stored.wish share project, may sharing entire folder, others can rapidly replicate study assist issue resolution. due fact file paths relative rather absolute.wish share project, may sharing entire folder, others can rapidly replicate study assist issue resolution. due fact file paths relative rather absolute.may utilize GitHub readily backups -called â€˜version controlâ€™ tools, allows trace changes code time. (btw really crucial work envirnoments, like know dedicated tutorial website git+GitHub+RStudio workflow set slides explain concepts). requirement course can skip . However let clarify : nice--skill whenever collaborating someone. happen job, writing thesis, name .may utilize GitHub readily backups -called â€˜version controlâ€™ tools, allows trace changes code time. (btw really crucial work envirnoments, like know dedicated tutorial website git+GitHub+RStudio workflow set slides explain concepts). requirement course can skip . However let clarify : nice--skill whenever collaborating someone. happen job, writing thesis, name .time , significant reason make R Project files ease file organization ability readily share co-investigators, supervisor, students.create R Project, need perform following steps:Select File > New Projectâ€¦ menu bar.\nFigure 4.1: Get R project\nSelect New Directory popup window.\nFigure 4.2: New Project Wizard pop menu\nNext, select New Project.\nFigure 4.3: full set project can initialize RStudio IDE\nPick meaningful name project folder, .e.Â Directory Name. Ensure project folder created right place. can change subdirectory clicking Browseâ€¦. Ideally subdirectory place usually store research projects.\nFigure 4.4: RProject specifications\noption Create git repository. relevant already GitHub account wish use version control. now, can happily ignore use GitHub.option Create git repository. relevant already GitHub account wish use version control. now, can happily ignore use GitHub.Lastly, tick Open new session. open R Project new RStudio window.Lastly, tick Open new session. open R Project new RStudio window.\nFigure 4.5: Choose directory name new project\nhappy choices, can click Create Project. open new R Session, can start working project.\nFigure 4.6: new RStudio Session pop just like magic!\nlook carefully, can see RStudio now â€˜brandedâ€™ project name. top window, see project name, files pane shows root directory files , even console shows top file path project. set manually, recommend , least easy swift work R Projects","code":""},{"path":"nice-warm-up.html","id":"workdir","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4.3 Working Directory with here","text":"bootstrap RProject way, RStudio going take care many headaches fresher sophmore developer beginning. matter fact time double click RStudio project file (one finishes .RProj) RStudio link directory computer specified creation project, previous case â€œtidy_tuesday_2021_08_03â€. called Working Directory.\ninteresting place R look files attempt load , R save files save . location working directory vary different computers.\nbase (rather vintage) way look working directory.\nunderstrand directory R using working directory, run:However since live 2022 going use convenient package .e.Â exactly thing prettier intuitively.() going look .RProj file Working Directory exactly placed.","code":"\ngetwd()\n## \"/Users/niccolo/Desktop/r_projects/sbd_22-23\"\ninstall.packages(\"here\")\nlibrary(here)\nhere()\n## \"/Users/niccolo/Desktop/r_projects/sbd_22-23\""},{"path":"nice-warm-up.html","id":"creating-an-r-script","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4.4 Creating an R Script","text":"Code may easily grow lengthy complicated. result, writing console inconvenient. alternative, may write code R Script. R Script document recognized RStudio R programming code. Non-R Script files, .txt,.rtf, .md, can also opened RStudio, code typed immediately recognized.open create new R script, appear Source pane. window sometimes referred â€˜script editorâ€™. R script begins empty file. Good coding etiquette requires us put comment # first line describe file . â€™s â€˜TidyTuesdayâ€™ R Project sample.\nFigure 4.7: Open R Script write \nexamples tutorial made copied pasted R script. However, need install R packages certain code. Letâ€™s give shot following code. plot produced code displays car company provides fuel-efficient vehicles. code copied pasted R script. â€™s simple script generates plot, copy paste file, execute .â€™re probably wondering happened plot. Copying code execute R script. However, required order develop plot. pressed Return â†µ, just add new line. Instead, choose code wish run hit Ctrl+Return â†µ (PC) Cmd+Return â†µ (Mac). may also use Run command top source window, keyboard shortcut far convenient. Furthermore, rapidly remember shortcut need utilize frequently. everything order, see following:can see, Honda automobiles appear travel furthest quantity fuel (gallon) vehicles. result, â€™re seeking cheap automobiles, now know look .â€™s worth noting R script editor includes handy features developing code. â€™ve undoubtedly noticed part code â€™ve pasted blue others green. distinct significance, colors aid making code understandable. default settings, green represents value â€œâ€œ, often represents characters. Syntax highlighting refers automatic coloring programming code.","code":"\n\nlibrary(tidyverse)\n\nmpg %>% \n  ggplot(aes(x = reorder(manufacturer, desc(hwy), FUN = median),\n                   y = hwy,\n                   fill = manufacturer)) +\n  geom_boxplot() +\n  coord_flip() +\n  theme_minimal() +\n  xlab(\"Manufacturer\") +\n  ylab(\"Highway miles per gallon\")"},{"path":"nice-warm-up.html","id":"using-r-markdown","chapter":"4 ğŸ”¥ Nice warm-up","heading":"4.5 Using R Markdown","text":"lot say R Markdown, â€™ll just mention exists highlight one feature persuade use instead plain R scripts: appear Word documents (almost).R Markdown files, name implies, mix R scripts â€˜Markdown.â€™ â€˜Markdownâ€™ method composing formatting text documents without use software Microsoft Word. instead write everything plain text. plain text may translated variety document forms, including HTML webpages, PDF files, Word documents. recommend checking R Markdown Cheatsheet learn works. Click File > New File > R Markdown create R Markdown file.R Markdown file inverse R script. default, R script treats everything code, can use language describe code commenting #. â€™ve seen previous code examples. R Markdown file, hand, treats everything text requires us declare code. may accomplish injecting â€˜code chunks.â€™ result, using comments # R Markdown files less necessary may write . Another advantage R Markdown files results analysis shown immediately underneath code chunk rather terminal. also sometimes called notebooks since can display code text together, Python equivalent someway exposed Python scripting Jupyter","code":""},{"path":"syllabus.html","id":"syllabus","chapter":"5 ğŸ—’ï¸ Syllabus","heading":"5 ğŸ—’ï¸ Syllabus","text":"lecture slides, notes, tutorials, assignments posted  drive , feel free jump . please anticipate questions address class, instead drop mail sure something.reasons trouble accessing G Drive, still please contact teaching assistant. One common issue students complain may need authorization accessm may forgotten switch open share option. see , knock shoulder!schedule subject change according pace class may schedule lab feel really ready intermediate exam.second edition course.ready:","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6 ğŸ’» Hypothesis Testing","text":"Hypothesis testing statistical method used make decisions inferences population parameter based sample statistic. involves formulating two competing hypothesesâ€”null hypothesis (Hâ‚€) alternative hypothesis (Hâ‚)â€”using sample data determine hypothesis supported evidence.","code":""},{"path":"hypothesis-testing.html","id":"understanding-hypothesis-testing","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.1 1. Understanding Hypothesis Testing","text":"Null Hypothesis (Hâ‚€): hypothesis effect difference. status quo assume true unless strong evidence . example, â€œmean body temperature humans 37Â°C.â€Null Hypothesis (Hâ‚€): hypothesis effect difference. status quo assume true unless strong evidence . example, â€œmean body temperature humans 37Â°C.â€Alternative Hypothesis (Hâ‚): hypothesis effect difference. represents trying find evidence . example, â€œmean body temperature humans 37Â°C.â€Alternative Hypothesis (Hâ‚): hypothesis effect difference. represents trying find evidence . example, â€œmean body temperature humans 37Â°C.â€Test Statistic: value calculated sample data used evaluate null hypothesis. example, t-statistic t-test.Test Statistic: value calculated sample data used evaluate null hypothesis. example, t-statistic t-test.p-Value: probability obtaining test statistic least extreme one observed, given null hypothesis true. small p-value (typically â‰¤ 0.05) indicates strong evidence null hypothesis.p-Value: probability obtaining test statistic least extreme one observed, given null hypothesis true. small p-value (typically â‰¤ 0.05) indicates strong evidence null hypothesis.Decision Rule: Based p-value, either reject null hypothesis (p â‰¤ 0.05) fail reject (p > 0.05).Decision Rule: Based p-value, either reject null hypothesis (p â‰¤ 0.05) fail reject (p > 0.05).","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing-on-one-mean","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.2 2. Hypothesis Testing on One Mean","text":"","code":""},{"path":"hypothesis-testing.html","id":"t.test-for-one-mean","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.2.1 2.1 t.test() for One Mean","text":"t.test() function R used test whether sample mean significantly different hypothesized population mean. â€™s general syntax:x: numeric vector data values.mu: hypothesized value population mean (default 0).alternative: Specifies alternative hypothesis. can \"two.sided\", \"greater\", \"less\".conf.level: Confidence level confidence interval (default 0.95).","code":"\nt.test(x, mu = 0, alternative = \"two.sided\", conf.level = 0.95)"},{"path":"hypothesis-testing.html","id":"example-testing-average-body-temperature","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.2.2 2.2 Example: Testing Average Body Temperature","text":"Letâ€™s test whether average body temperature sample patients significantly different 37Â°C.example, testing mean body temperature significantly different 37Â°C.","code":"\n# Sample data: body temperatures in degrees Celsius\nbody_temp <- c(36.7, 36.9, 37.1, 37.2, 37.3, 36.8, 37.0, 36.6)\n\n# Perform one-sample t-test\nt.test(body_temp, mu = 37)\n#> \n#>  One Sample t-test\n#> \n#> data:  body_temp\n#> t = -0.57735, df = 7, p-value = 0.5818\n#> alternative hypothesis: true mean is not equal to 37\n#> 95 percent confidence interval:\n#>  36.74522 37.15478\n#> sample estimates:\n#> mean of x \n#>     36.95"},{"path":"hypothesis-testing.html","id":"hypothesis-testing-on-two-means","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.3 3. Hypothesis Testing on Two Means","text":"comparing means two independent groups, use two-sample t-test.","code":""},{"path":"hypothesis-testing.html","id":"t.test-for-two-independent-means","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.3.1 3.1 t.test() for Two Independent Means","text":"t.test() function can also used compare two means. â€™s syntax:x, y: Numeric vectors data values representing two groups.var.equal: Logical value indicating whether assume equal variances (default FALSE).","code":"\nt.test(x, y, alternative = \"two.sided\", var.equal = FALSE, conf.level = 0.95)"},{"path":"hypothesis-testing.html","id":"example-comparing-treatment-and-control-groups","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.3.2 3.2 Example: Comparing Treatment and Control Groups","text":"Suppose data weight loss patients two groups: treatment group control group.example, testing average weight loss treatment group significantly different control group.","code":"\n# Sample data: weight loss in kg\ntreatment <- c(4.5, 5.0, 4.7, 6.2, 5.1)\ncontrol <- c(2.1, 2.4, 2.3, 2.0, 1.9)\n\n# Perform two-sample t-test\nt.test(treatment, control)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  treatment and control\n#> t = 9.5733, df = 4.7832, p-value = 0.0002676\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  2.154234 3.765766\n#> sample estimates:\n#> mean of x mean of y \n#>      5.10      2.14"},{"path":"hypothesis-testing.html","id":"parameters-of-t.test-in-detail","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.4 4. Parameters of t.test() in Detail","text":"t.test() function R several parameters can adjusted depending specific hypothesis tested:x: sample data one-sample test first group two-sample test.y: second group two-sample test. parameter left blank one-sample tests.alternative: Specifies alternative hypothesis. Options :\n\"two.sided\": default option. Tests sample mean different hypothesized mean.\n\"greater\": Tests sample mean greater hypothesized mean.\n\"less\": Tests sample mean less hypothesized mean.\n\"two.sided\": default option. Tests sample mean different hypothesized mean.\"greater\": Tests sample mean greater hypothesized mean.\"less\": Tests sample mean less hypothesized mean.mu: hypothesized population mean.paired: logical value indicating whether perform paired t-test. Defaults FALSE.var.equal: logical value indicating whether assume equal variances two-sample test. Defaults FALSE.conf.level: confidence level confidence interval, usually set 0.95.","code":""},{"path":"hypothesis-testing.html","id":"exercises","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.5 5. Exercises","text":"","code":""},{"path":"hypothesis-testing.html","id":"exercise-1-one-sample-t-test","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.6 Exercise 1: One-Sample t-Test","text":"Given following blood pressure measurements, test average systolic blood pressure significantly different 120.Exercise 6.1  Test hypothesis mean blood pressure different 120.Hint: Use t.test() function mu = 120.Answer Exercise 6.1:","code":"\nblood_pressure <- c(118, 122, 121, 119, 123, 125, 117, 124, 122, 120)\nt.test(blood_pressure, mu = 120)\n#> \n#>  One Sample t-test\n#> \n#> data:  blood_pressure\n#> t = 1.3372, df = 9, p-value = 0.214\n#> alternative hypothesis: true mean is not equal to 120\n#> 95 percent confidence interval:\n#>  119.2392 122.9608\n#> sample estimates:\n#> mean of x \n#>     121.1"},{"path":"hypothesis-testing.html","id":"exercise-2-two-sample-t-test","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.7 Exercise 2: Two-Sample t-Test","text":"Compare cholesterol levels two different diets.Exercise 6.2  Test mean cholesterol level different two diets.Hint: Use t.test() function x = diet_A y = diet_B.Answer Exercise 6.2:","code":"\ndiet_A <- c(180, 190, 200, 195, 185)\ndiet_B <- c(210, 220, 215, 225, 230)\nt.test(diet_A, diet_B)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  diet_A and diet_B\n#> t = -6, df = 8, p-value = 0.0003234\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -41.53002 -18.46998\n#> sample estimates:\n#> mean of x mean of y \n#>       190       220"},{"path":"hypothesis-testing.html","id":"exercise-3-one-sample-t-test-greater","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.8 Exercise 3: One-Sample t-Test (Greater)","text":"new drug claims lower blood sugar levels 100 mg/dL. Test claim following blood sugar levels drug administration.Exercise 6.3  Test hypothesis mean blood sugar level less 100.Hint: Use t.test() function mu = 100 alternative = \"less\".Answer Exercise 6.3:","code":"\nblood_sugar <- c(95, 99, 102, 98, 97, 96, 100, 99)\nt.test(blood_sugar, mu = 100, alternative = \"less\")\n#> \n#>  One Sample t-test\n#> \n#> data:  blood_sugar\n#> t = -2.198, df = 7, p-value = 0.03196\n#> alternative hypothesis: true mean is less than 100\n#> 95 percent confidence interval:\n#>      -Inf 99.75846\n#> sample estimates:\n#> mean of x \n#>     98.25"},{"path":"hypothesis-testing.html","id":"exercise-4-two-sample-t-test-paired","chapter":"6 ğŸ’» Hypothesis Testing","heading":"6.0.9 Exercise 4: Two-Sample t-Test (Paired)","text":"dietitian wants know new diet plan significantly reduces weight. weights 5 individuals following diet recorded :Exercise 6.4  Test hypothesis significant difference weight diet.Hint: Use t.test() function paired = TRUE.Answer Exercise 6.4:","code":"\nbefore <- c(72, 68, 74, 70, 76)\nafter <- c(70, 65, 72, 67, 73)\nt.test(before, after, paired = TRUE)\n#> \n#>  Paired t-test\n#> \n#> data:  before and after\n#> t = 10.614, df = 4, p-value = 0.000446\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  1.919913 3.280087\n#> sample estimates:\n#> mean difference \n#>             2.6"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"essentials-of-data-wrangling-and-basic-statistics","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","text":"Data wrangling, also known data cleaning, involves preparation steps prior fitting statistical model. essential since healthcare data often comes raw, messy format, usually donâ€™t need data available. might need subset, specific rows columns.tutorial, learn clean wrangle healthcare-related datasets using base R dplyr package, along basic descriptive statistics. Letâ€™s start basics!","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"basic-operations-in-r","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.1 1. Basic Operations in R","text":"Letâ€™s start simple operations:can store result objects:Print result:Modify result:","code":"\n3 + 5\n#> [1] 8\n12 / 7\n#> [1] 1.714286\nresult <- 3 + 5\nresult\n#> [1] 8\nprint(result)\n#> [1] 8\nresult <- result * 3.1415\nprint(result)\n#> [1] 25.132"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"exercise-1-basic-arithmetic","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.1.1 Exercise 1: Basic Arithmetic","text":"Compute BMI (Body Mass Index) person weighing 70 kg height 1.75 meters. Use formula: \\(BMI = \\frac{weight}{height^2}\\).","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"working-with-vectors","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.2 2. Working with Vectors","text":"Vectors fundamental data structure R. Letâ€™s create vector blood pressure readings:","code":"\nblood_pressure <- c(120, 135, 142, 130, 125)\nblood_pressure\n#> [1] 120 135 142 130 125"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"accessing-vector-elements","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.2.1 Accessing Vector Elements","text":"can access elements vector using square brackets []. example:","code":"\nfirst_reading <- blood_pressure[1]\nfirst_three <- blood_pressure[1:3]\n\nprint(first_reading)  # Output: [1] 120\n#> [1] 120\nprint(first_three)    # Output: [1] 120 135 142\n#> [1] 120 135 142"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"conditional-subsetting","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.2.2 Conditional Subsetting","text":"Letâ€™s select readings greater 130:","code":"\nhigh_bp <- blood_pressure[blood_pressure > 130]\nprint(high_bp)  # Output: [1] 135 142 130\n#> [1] 135 142"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"advanced-conditional-subsetting","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.2.2.1 Advanced Conditional Subsetting","text":"can combine multiple conditions filter complex subsets. example, letâ€™s select blood pressure readings greater 130 less 150:can also use | () operator select readings either 125 160:","code":"\nsubset_bp <- blood_pressure[blood_pressure > 130 & blood_pressure < 150]\nprint(subset_bp)  # Output: [1] 135 142\n#> [1] 135 142\nextreme_bp <- blood_pressure[blood_pressure < 125 | blood_pressure > 160]\nprint(extreme_bp)  # Output: [1] 120 165 170\n#> [1] 120"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"data-frames","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.3 3. Data Frames","text":"Data frames similar tables spreadsheets, making suitable storing patient data. Letâ€™s create simple data frame:","code":"\npatients <- data.frame(\n  ID = c(1, 2, 3),\n  Name = c(\"John\", \"Alice\", \"Bob\"),\n  Age = c(30, 25, 28),\n  BloodPressure = c(120, 130, 125)\n)\n\nprint(patients)\n#>   ID  Name Age BloodPressure\n#> 1  1  John  30           120\n#> 2  2 Alice  25           130\n#> 3  3   Bob  28           125"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"viewing-the-first-few-rows","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.3.1 Viewing the First Few Rows","text":"view first rows data frame, use head() function:can specify number rows want see:","code":"\nhead(patients)\n#>   ID  Name Age BloodPressure\n#> 1  1  John  30           120\n#> 2  2 Alice  25           130\n#> 3  3   Bob  28           125\nhead(patients, n = 2)\n#>   ID  Name Age BloodPressure\n#> 1  1  John  30           120\n#> 2  2 Alice  25           130"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"using-built-in-datasets","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.3.2 Using Built-in Datasets","text":"R comes several built-datasets can accessed using data() function. example, load iris dataset:","code":"\ndata(iris)\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"accessing-data-frame-elements","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.3.3 Accessing Data Frame Elements","text":"can access elements data frame using $ operator:using square brackets:","code":"\npatient_names <- patients$Name\nprint(patient_names)  # Output: [1] \"John\" \"Alice\" \"Bob\"\n#> [1] \"John\"  \"Alice\" \"Bob\"\nfirst_patient <- patients[1, ]\nprint(first_patient)\n#>   ID Name Age BloodPressure\n#> 1  1 John  30           120"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"creating-and-modifying-columns","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.3.4 Creating and Modifying Columns","text":"can create new columns based existing ones. example, letâ€™s add column categorize patients based blood pressure:remove column, simply assign NULL :","code":"\npatients$BP_Category <- ifelse(patients$BloodPressure > 140, \"High\", \"Normal\")\nprint(patients)\n#>   ID  Name Age BloodPressure BP_Category\n#> 1  1  John  30           120      Normal\n#> 2  2 Alice  25           130      Normal\n#> 3  3   Bob  28           125      Normal\npatients$BP_Category <- NULL\nprint(patients)\n#>   ID  Name Age BloodPressure\n#> 1  1  John  30           120\n#> 2  2 Alice  25           130\n#> 3  3   Bob  28           125"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"data-manipulation-with-dplyr","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.4 4. Data Manipulation with dplyr","text":"dplyr package provides intuitive way manipulate data. Letâ€™s load use filter select data:","code":"\nlibrary(dplyr)\n\n# Filter patients with blood pressure > 125\nhigh_bp_patients <- patients %>%\n  filter(BloodPressure > 125)\n\nprint(high_bp_patients)\n#>   ID  Name Age BloodPressure\n#> 1  2 Alice  25           130"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"selecting-columns","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.4.1 Selecting Columns","text":"can select specific columns using select():","code":"\nnames_and_age <- patients %>%\n  select(Name, Age)\n\nprint(names_and_age)\n#>    Name Age\n#> 1  John  30\n#> 2 Alice  25\n#> 3   Bob  28"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"importing-and-exporting-data","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.5 5. Importing and Exporting Data","text":"crucial know import export data, â€™ll often work datasets stored files.","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"importing-data-from-a-csv-file","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.5.1 Importing Data from a CSV File","text":"import CSV file R, use read.csv() function:","code":"\n# Import a CSV file\ndata <- read.csv(\"path/to/file.csv\")\nhead(data)  # Display the first few rows"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"exporting-data-to-a-csv-file","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.5.2 Exporting Data to a CSV File","text":"save data frame CSV file:","code":"\n# Export the data frame to a CSV file\nwrite.csv(patients, \"path/to/save_file.csv\", row.names = FALSE)"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"data-cleaning","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.6 6. Data Cleaning","text":"","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"handling-missing-data","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.6.1 Handling Missing Data","text":"healthcare, missing data can occur several reasons. example, nurse might forget record patientâ€™s temperature routine checkup, field might left empty due lack information. Missing data can affect quality analysis may appear R different forms, NA (Available) NULL (indicating absence object).R, NA used represent missing values vectors data frames. â€™s important handle missing data appropriately, can bias results. Common strategies include removing missing data, imputing values, using statistical models handle missingness.","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"checking-for-missing-values","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.6.2 Checking for Missing Values","text":"can use .na() check missing values:","code":"\n# Check for missing values in the BloodPressure column\nmissing_values <- is.na(patients$BloodPressure)\nprint(missing_values)  # Output: [1] FALSE FALSE FALSE\n#> [1] FALSE FALSE FALSE"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"removing-missing-values","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.6.3 Removing Missing Values","text":"can remove rows missing values using na.omit():","code":"\n# Remove rows with missing values\ncleaned_data <- na.omit(patients)\nprint(cleaned_data)\n#>   ID  Name Age BloodPressure\n#> 1  1  John  30           120\n#> 2  2 Alice  25           130\n#> 3  3   Bob  28           125"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"working-with-factors","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.7 7. Working with Factors","text":"healthcare, many variables categorical, disease type treatment group. Factors used R handle categorical data.","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"creating-and-modifying-factors","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.7.1 Creating and Modifying Factors","text":"can also modify factor levels:","code":"\n# Create a factor for disease type\npatients$Disease <- factor(c(\"Diabetes\", \"Hypertension\", \"None\"))\nlevels(patients$Disease)\n#> [1] \"Diabetes\"     \"Hypertension\" \"None\"\n# Rename factor levels\nlevels(patients$Disease) <- c(\"DM\", \"HTN\", \"Healthy\")\nprint(patients$Disease)\n#> [1] DM      HTN     Healthy\n#> Levels: DM HTN Healthy"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"descriptive-statistics","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8 8. Descriptive Statistics","text":"","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"measures-of-central-tendency","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8.1 Measures of Central Tendency","text":"Mean: mean sum values divided total number observations.Median: median central value ordered set data.Mode: R, built-function mode, can calculate follows:","code":"\nmean_bp <- mean(patients$BloodPressure)\nprint(mean_bp)\n#> [1] 125\nmedian_bp <- median(patients$BloodPressure)\nprint(median_bp)\n#> [1] 125\n# Calculate the mode\nmode <- function(x) {\n  uniq_vals <- unique(x)\n  uniq_vals[which.max(tabulate(match(x, uniq_vals)))]\n}\n\nmode_bp <- mode(patients$BloodPressure)\nprint(mode_bp)\n#> [1] 120"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"measures-of-variability","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8.2 Measures of Variability","text":"Variance: Variance measures dispersion data mean.Standard Deviation: Standard deviation square root variance indicates spread data around mean.","code":"\nvar_bp <- var(patients$BloodPressure)\nprint(var_bp)\n#> [1] 25\nsd_bp <- sd(patients$BloodPressure)\nprint(sd_bp)\n#> [1] 5"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"correlation","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8.3 Correlation","text":"Correlation measures strength direction relationship two variables. example, relationship age blood pressure.","code":"\ncorrelation <- cor(patients$Age, patients$BloodPressure)\nprint(correlation)\n#> [1] -0.9933993"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"scatter-plot-for-correlation","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8.4 Scatter Plot for Correlation","text":"","code":"\nplot(patients$Age, patients$BloodPressure, \n     main = \"Scatter Plot of Age vs Blood Pressure\",\n     xlab\n\n = \"Age\",\n     ylab = \"Blood Pressure (mm Hg)\",\n     pch = 19)"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"interpreting-graphs","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.8.5 Interpreting Graphs","text":"Histogram: Shows distribution single numeric variable. height bar represents frequency values bin.Histogram: Shows distribution single numeric variable. height bar represents frequency values bin.Scatter Plot: Displays relationship two numeric variables. point represents observation, pattern points shows nature relationship.Scatter Plot: Displays relationship two numeric variables. point represents observation, pattern points shows nature relationship.Boxplot: Visualizes distribution variable quartiles, identifying outliers. Useful comparing distributions different groups.Boxplot: Visualizes distribution variable quartiles, identifying outliers. Useful comparing distributions different groups.Pie Chart: Represents proportions categories within whole, showing category contributes total.Pie Chart: Represents proportions categories within whole, showing category contributes total.","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"visualizing-data-with-boxplots","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.9 9. Visualizing Data with Boxplots","text":"boxplot, box--whisker plot, graphical representation used display distribution data based five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), maximum. helps identify outliers spread data.","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"clinical-example-visualizing-blood-pressure","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.9.1 Clinical Example: Visualizing Blood Pressure","text":"Letâ€™s visualize distribution systolic blood pressure readings group patients:example:\n- box represents interquartile range (IQR), containing middle 50% data.\n- line inside box shows median value.\n- â€œwhiskersâ€ extend minimum maximum values within 1.5 * IQR Q1 Q3.\n- Data points outside range considered outliers, useful identifying patients unusually high low blood pressure.","code":"\nblood_pressure <- c(120, 130, 135, 140, 145, 150, 155, 160, 165, 170)\n\nboxplot(blood_pressure, \n        main = \"Boxplot of Systolic Blood Pressure\",\n        ylab = \"Blood Pressure (mm Hg)\",\n        col = \"lightblue\")"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"other-basic-plots","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.10 10. Other Basic Plots","text":"","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"histogram","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.10.1 Histogram","text":"easy way visualize distribution numeric data using histogram. example:","code":"\n# Histogram of blood pressure readings\nhist(blood_pressure, \n     main = \"Distribution of Blood Pressure\",\n     xlab = \"Blood Pressure (mm Hg)\",\n     col = \"lightgreen\")"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"bar-plot","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.10.2 Bar Plot","text":"categorical data, bar plot can useful:","code":"\n# Count of diseases\nbarplot(table(patients$Disease), \n        main = \"Count of Diseases\",\n        xlab = \"Disease\",\n        ylab = \"Number of Patients\",\n        col = \"lightcoral\")"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"pie-chart","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.10.3 Pie Chart","text":"pie chart can used represent proportions categories within whole. example, letâ€™s visualize proportion different diseases:","code":"\n# Pie chart of diseases\npie(table(patients$Disease), \n    main = \"Proportion of Diseases\",\n    col = c(\"lightblue\", \"lightgreen\", \"lightpink\"))"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"summarizing-data","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.11 11. Summarizing Data","text":"","code":""},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"data-frame-structure","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.11.1 Data Frame Structure","text":"Use str() view structure data frame:","code":"\n# View structure of data frame\nstr(patients)\n#> 'data.frame':    3 obs. of  5 variables:\n#>  $ ID           : num  1 2 3\n#>  $ Name         : chr  \"John\" \"Alice\" \"Bob\"\n#>  $ Age          : num  30 25 28\n#>  $ BloodPressure: num  120 130 125\n#>  $ Disease      : Factor w/ 3 levels \"DM\",\"HTN\",\"Healthy\": 1 2 3"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"summary-statistics","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.11.2 Summary Statistics","text":"can get quick summary columns using summary():","code":"\n# Summary of the data frame\nsummary(patients)\n#>        ID          Name                Age       \n#>  Min.   :1.0   Length:3           Min.   :25.00  \n#>  1st Qu.:1.5   Class :character   1st Qu.:26.50  \n#>  Median :2.0   Mode  :character   Median :28.00  \n#>  Mean   :2.0                      Mean   :27.67  \n#>  3rd Qu.:2.5                      3rd Qu.:29.00  \n#>  Max.   :3.0                      Max.   :30.00  \n#>  BloodPressure      Disease \n#>  Min.   :120.0   DM     :1  \n#>  1st Qu.:122.5   HTN    :1  \n#>  Median :125.0   Healthy:1  \n#>  Mean   :125.0              \n#>  3rd Qu.:127.5              \n#>  Max.   :130.0"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"using-dplyr-for-summary","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.11.3 Using dplyr for Summary","text":"","code":"\n# Summary statistics using dplyr\nbp_summary <- patients %>%\n  summarise(\n    mean_bp = mean(BloodPressure, na.rm = TRUE),\n    sd_bp = sd(BloodPressure, na.rm = TRUE)\n  )\n\nprint(bp_summary)\n#>   mean_bp sd_bp\n#> 1     125     5"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"merging-data-frames","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.12 12. Merging Data Frames","text":"Sometimes, â€™ll need combine patient data different sources. Use merge() function:","code":"\n# Patient details\npatient_details <- data.frame(\n  ID = c(1, 2, 3),\n  Name = c(\"John\", \"Alice\", \"Bob\")\n)\n\n# Patient records\npatient_records <- data.frame(\n  ID = c(1, 3, 4),\n  BloodPressure = c(120, 125, 135)\n)\n\n# Merge on \"ID\"\nmerged_data <- merge(patient_details, patient_records, by = \"ID\", all = TRUE)\nprint(merged_data)\n#>   ID  Name BloodPressure\n#> 1  1  John           120\n#> 2  2 Alice            NA\n#> 3  3   Bob           125\n#> 4  4  <NA>           135"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"data-export","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.13 13. Data Export","text":"Finally, save cleaned modified data back file:","code":"\n# Save the merged data frame to a CSV file\nwrite.csv(merged_data, \"merged_patient_data.csv\", row.names = FALSE)"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"exercises-1","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.14 Exercises","text":"Exercise 6.1  Given vector numeric data representing cholesterol levels, access print third fifth elements.Exercise 6.2  Create data frame following patient data print entire data frame:Name: Emma, Oliver, AvaAge: 45, 50, 37BMI: 22.5, 28.7, 25.3Exercise 6.3  Using data frame patients, filter print rows BloodPressure greater 125.Exercise 6.4  given data frame, select print Name Age columns using dplyrâ€™s select() function.Exercise 7.1  Create data frame patient IDs blood pressure readings. Introduce NA values, use na.omit() remove print cleaned data frame.Exercise 7.2  Create two data frames: one patient IDs ages, another patient IDs cholesterol levels. Merge using merge() function.Exercise 7.3  Using data frame patients, calculate mean standard deviation BloodPressure column.Exercise 7.4  Create scatter plot Age vs.Â BMI using ggplot2 data frame created Exercise 2.","code":"cholesterol_levels <- c(180, 220, 195, 250, 205)"},{"path":"essentials-of-data-wrangling-and-basic-statistics.html","id":"solutions","chapter":"7 ğŸ’» Essentials of Data Wrangling and Basic Statistics","heading":"7.15 Solutions","text":"Answer Exercise 6.1:Answer Exercise 6.2:Answer Exercise 6.3:Answer Exercise 6.4:Answer Exercise 7.1:Answer Exercise 7.2:Answer Exercise 7.3:Answer Exercise 7.4:","code":"element3 <- cholesterol_levels[3]\nelement5 <- cholesterol_levels[5]\n\nprint(element3)  # Output: [1] 195\nprint(element5)  # Output: [1] 205patient_data <- data.frame(\n    Name = c(\"Emma\", \"Oliver\", \"Ava\"),\n    Age = c(45, 50, 37),\n    BMI = c(22.5, 28.7, 25.3)\n)\n\nprint(patient_data)high_bp_patients <- subset(patients, BloodPressure > 125)\nprint(high_bp_patients)names_and_ages <- patients %>%\n    select(Name, Age)\nprint(names_and_ages)patient_bp <- data.frame(\n    ID = 1:5,\n    BloodPressure = c(120, 130, NA, 140, NA)\n)\n\ncleaned_bp <- na.omit(patient_bp)\nprint(cleaned_bp)patient_ages <- data.frame(\n    ID = c(1, 2, 3),\n    Age = c(30, 25, 28)\n)\n\npatient_chol <- data.frame(\n    ID = c(2, 3, 4),\n    Cholesterol = c(220, 195, 250)\n)\n\nmerged_data <- merge(patient_ages, patient_chol, by = \"ID\", all = TRUE)\nprint(merged_data)bp_summary <- patients %>%\n    summarise(\n        mean_bp = mean(BloodPressure, na.rm = TRUE),\n        sd_bp = sd(BloodPressure, na.rm = TRUE)\n    )\nprint(bp_summary)ggplot(patient_data, aes(x = Age, y = BMI)) +\n    geom_point() +\n    labs(title = \"Age vs. BMI\", x = \"Age\", y = \"BMI\")"},{"path":"lin-reg.html","id":"lin-reg","chapter":"8 Linear Regression","heading":"8 Linear Regression","text":"","code":""},{"path":"lin-reg.html","id":"introduction","chapter":"8 Linear Regression","heading":"8.1 Introduction","text":"","code":""},{"path":"lin-reg.html","id":"background","chapter":"8 Linear Regression","heading":"8.1.1 Background","text":"Linear regression 8 one common statistical analyses medical health sciences. Linear regression 8 models linear (.e.Â straight line) relationship :outcome: numerical variable (e.g.Â blood pressure, BMI, cholesterol level).predictors/independent variables: numerical variables categorical variables (e.g.Â gender, race, education level).simple words, might interested knowing relationship cholesterol level associated factors, example gender, age, BMI lifestyle. can explored Linear regression analysis.Linear regression type Generalized linear models (GLMs), also includes outcome types, example categorical count. subsequent chapters, cover outcome types form logistic regression Poisson regression. Basically, relationship outcome predictors linear regression structured follows,\\[\\begin{aligned}\nnumerical\\ outcome = &\\ numerical\\ predictors \\\\\n& + categorical\\ predictors\n\\end{aligned}\\]appropriate forms relationship explained later simple multiple linear regressions sections.","code":""},{"path":"lin-reg.html","id":"objectives","chapter":"8 Linear Regression","heading":"8.1.2 Objectives","text":"completing chapter, readers expected tounderstand concept simple multiple linear regressionperform simple linear regressionperform multiple linear regressionperform model fit assessment linear regression modelspresent interpret results linear regression analyses","code":""},{"path":"lin-reg.html","id":"prepare-r-environment-for-analysis","chapter":"8 Linear Regression","heading":"8.2 Prepare R Environment for Analysis","text":"","code":""},{"path":"lin-reg.html","id":"libraries","chapter":"8 Linear Regression","heading":"8.2.1 Libraries","text":"chapter, using following packages:foreign: reading SPSS STATA datasetstidyverse: general powerful package data transformationpsych: descriptive statisticsgtsummary: coming nice tables results plotting graphsggplot2, ggpubr, GGally: plotting graphsrsq: getting \\(R^2\\) value GLM modelbroom: tidying resultscar: vif() functionThese loaded follows using function library(),","code":"\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(rsq)\nlibrary(broom)\nlibrary(car)"},{"path":"lin-reg.html","id":"dataset","chapter":"8 Linear Regression","heading":"8.2.2 Dataset","text":"use coronary.dta dataset STATA format. dataset contains total cholesterol level, individual characteristics intervention groups hypothetical clinical trial. dataset contains 200 observations nine variables:id: Subjectsâ€™ ID.cad: Coronary artery disease status (categorical) {cad, cad}.sbp : Systolic blood pressure mmHg (numerical).dbp : Diastolic blood pressure mmHg (numerical).chol: Total cholesterol level mmol/L (numerical).age: Age years (numerical).bmi: Body mass index (numerical).race: Race subjects (categorical) {malay, chinese, indian}.gender: Gender subjects (categorical) {woman, man}.dataset loaded follows,look basic structure dataset,","code":"\ncoronary = read.dta(\"data/coronary.dta\")\nstr(coronary)\n#> 'data.frame':    200 obs. of  9 variables:\n#>  $ id    : num  1 14 56 61 62 64 69 108 112 134 ...\n#>  $ cad   : Factor w/ 2 levels \"no cad\",\"cad\": 1 1 1 1 1 1 2 1 1 1 ...\n#>  $ sbp   : num  106 130 136 138 115 124 110 112 138 104 ...\n#>  $ dbp   : num  68 78 84 100 85 72 80 70 85 70 ...\n#>  $ chol  : num  6.57 6.33 5.97 7.04 6.66 ...\n#>  $ age   : num  60 34 36 45 53 43 44 50 43 48 ...\n#>  $ bmi   : num  38.9 37.8 40.5 37.6 40.3 ...\n#>  $ race  : Factor w/ 3 levels \"malay\",\"chinese\",..: 3 1 1 1 3 1 1 2 2 2 ...\n#>  $ gender: Factor w/ 2 levels \"woman\",\"man\": 1 1 1 1 2 2 2 1 1 2 ...\n#>  - attr(*, \"datalabel\")= chr \"Written by R.              \"\n#>  - attr(*, \"time.stamp\")= chr \"\"\n#>  - attr(*, \"formats\")= chr [1:9] \"%9.0g\" \"%9.0g\" \"%9.0g\" \"%9.0g\" ...\n#>  - attr(*, \"types\")= int [1:9] 100 108 100 100 100 100 100 108 108\n#>  - attr(*, \"val.labels\")= chr [1:9] \"\" \"cad\" \"\" \"\" ...\n#>  - attr(*, \"var.labels\")= chr [1:9] \"id\" \"cad\" \"sbp\" \"dbp\" ...\n#>  - attr(*, \"version\")= int 7\n#>  - attr(*, \"label.table\")=List of 3\n#>   ..$ cad   : Named int [1:2] 1 2\n#>   .. ..- attr(*, \"names\")= chr [1:2] \"no cad\" \"cad\"\n#>   ..$ race  : Named int [1:3] 1 2 3\n#>   .. ..- attr(*, \"names\")= chr [1:3] \"malay\" \"chinese\" \"indian\"\n#>   ..$ gender: Named int [1:2] 1 2\n#>   .. ..- attr(*, \"names\")= chr [1:2] \"woman\" \"man\""},{"path":"lin-reg.html","id":"simple-linear-regression","chapter":"8 Linear Regression","heading":"8.3 Simple Linear Regression","text":"","code":""},{"path":"lin-reg.html","id":"about-simple-linear-regression","chapter":"8 Linear Regression","heading":"8.3.1 About Simple Linear Regression","text":"Simple linear regression (SLR) models linear (straight line) relationship :outcome: numerical variable.ONE predictor: numerical/categorical variable.Note: predictor categorical variable, typically analyzed one-way ANOVA. However, SLR can also handle categorical variable GLM framework.may formally represent SLR form equation follows,\\[numerical\\ outcome = intercept + coefficient \\times predictor\\]\nshorter form using mathematical notations,\\[\\hat y = b_0 + b_1x_1\\]\n\\(\\hat y\\) (pronounced y hat) predicted value outcome y.","code":""},{"path":"lin-reg.html","id":"data-exploration","chapter":"8 Linear Regression","heading":"8.3.2 Data exploration","text":"Let say, SLR interested knowing whether diastolic blood pressure (predictor) associated cholesterol level (outcome). explore variables obtaining descriptive statistics plotting data distribution.obtain descriptive statistics variables,histograms box--whiskers plots,","code":"\ncoronary %>% select(chol, dbp) %>% describe()\n#>      vars   n  mean    sd median trimmed   mad min    max\n#> chol    1 200  6.20  1.18   6.19    6.18  1.18   4   9.35\n#> dbp     2 200 82.31 12.90  80.00   81.68 14.83  56 120.00\n#>      range skew kurtosis   se\n#> chol  5.35 0.18    -0.31 0.08\n#> dbp  64.00 0.42    -0.33 0.91\nlibrary(patchwork)\n\nhist_chol = ggplot(coronary, aes(chol)) + \n  geom_histogram(color = \"blue\", fill = \"white\")\nhist_dbp = ggplot(coronary, aes(dbp)) + \n  geom_histogram(color = \"red\", fill = \"white\")\nbplot_chol = ggplot(coronary, aes(chol)) + \n  geom_boxplot(color = \"blue\", )\nbplot_dbp = ggplot(coronary, aes(dbp)) + \n  geom_boxplot(color = \"red\")\n\n(hist_chol + hist_dbp)/(bplot_chol + bplot_dbp)"},{"path":"lin-reg.html","id":"univariable-analysis","chapter":"8 Linear Regression","heading":"8.3.3 Univariable analysis","text":"analysis, fit SLR model, consists one predictor (univariable). , chol specified outcome, dbp predictor. glm, formula specified outcome ~ predictor. , specify chol ~ dbp formula glm.fit view summary information model ,can tidy glm output obtain 95% confidence interval (CI) using tidy() broom package,output , pay attention results:coefficients, \\(b\\) â€“ column estimate.95% CI â€“ columns conf.low conf.high.P-value â€“ column p.value.","code":"\nslr_chol = glm(chol ~ dbp, data = coronary)\nsummary(slr_chol)\n#> \n#> Call:\n#> glm(formula = chol ~ dbp, data = coronary)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.9967  -0.8304  -0.1292   0.7734   2.8470  \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value       Pr(>|t|)    \n#> (Intercept) 2.995134   0.492092   6.087 0.000000005876 ***\n#> dbp         0.038919   0.005907   6.589 0.000000000392 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 1.154763)\n#> \n#>     Null deviance: 278.77  on 199  degrees of freedom\n#> Residual deviance: 228.64  on 198  degrees of freedom\n#> AIC: 600.34\n#> \n#> Number of Fisher Scoring iterations: 2\ntidy(slr_chol, conf.int = TRUE)\n#> # A tibble: 2 Ã— 7\n#>   term        estimate std.error statistic  p.value conf.low\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n#> 1 (Intercept)   3.00     0.492        6.09 5.88e- 9   2.03  \n#> 2 dbp           0.0389   0.00591      6.59 3.92e-10   0.0273\n#> # â„¹ 1 more variable: conf.high <dbl>"},{"path":"lin-reg.html","id":"model-fit-assessment","chapter":"8 Linear Regression","heading":"8.3.4 Model fit assessment","text":"important assess extend SLR model reflects data. First, can assess \\(R^2\\), percentage variance outcome explained predictor. simpler words, extend variation values outcome caused/explained predictor. ranges 0% (predictor explain outcome ) 100% (predictor explains outcome perfectly). , obtain \\(R^2\\) values,Next, can assess model fit scatter plot,\nplot allows assessment normality, linearity equal variance assumptions. expect elliptical/oval shape (normality) equal scatter dots prediction line (equal variance). aspects indicate linear relationship chol dbp (linearity).","code":"\nrsq(slr_chol)\n#> [1] 0.1798257\nplot_slr = ggplot(coronary, aes(x = dbp, y = chol)) + \n  geom_point() + geom_smooth(method = lm)\nplot_slr"},{"path":"lin-reg.html","id":"presentation-and-interpretation","chapter":"8 Linear Regression","heading":"8.3.5 Presentation and interpretation","text":"present result, can use tbl_regression() come nice table. use slr_chol glm output tbl_regression() gtsummary package., use intercept = TRUE include intercept value table. default, omitted tbl_regression().also informative present model equation,\\[chol = 3.0 + 0.04\\times dbp\\]Based \\(R^2\\) (0.18), table model equation, may interpret results follows:1mmHg increase DBP causes 0.04mmol/L increase cholesterol level.DBP explains 18% variance cholesterol level.","code":"\ntbl_regression(slr_chol, intercept = TRUE)"},{"path":"lin-reg.html","id":"multiple-linear-regression","chapter":"8 Linear Regression","heading":"8.4 Multiple Linear Regression","text":"","code":""},{"path":"lin-reg.html","id":"about-multiple-linear-regression","chapter":"8 Linear Regression","heading":"8.4.1 About Multiple Linear Regression","text":"Multiple linear regression (MLR) models linear relationship :outcome: numerical variable.one predictors: numerical categorical variables.may formally represent MLR form equation,\\[\\begin{aligned}\nnumerical\\ outcome = &\\ intercept \\\\\n& + coefficients \\times numerical\\ predictors \\\\\n& + coefficients \\times categorical\\ predictors\n\\end{aligned}\\]\nshorter form,\\[\\hat y = b_0 + b_1x_1 + b_2x_2 + ... + b_px_p\\]\np predictors.Whenever predictor categorical variable two levels, use dummy variable(s). issue binary categorical variable. variable two levels, number dummy variables (.e.Â turned several binary variables) equals number levels minus one. example, whenever four levels, obtain three dummy (binary) variables. see later, glm automatically factor variable provide separate estimates dummy variable.","code":""},{"path":"lin-reg.html","id":"data-exploration-1","chapter":"8 Linear Regression","heading":"8.4.2 Data exploration","text":"Now, MLR longer restricted one predictor. Let say, interested knowing relationship blood pressure (SBP DBP), age, BMI, race render predictors cholesterol level (outcome). , explore variables descriptive statistics,pairs plot, focus distribution data histograms box--whiskers plots. pairs plot also includes information bivariate correlation statistics numerical variables.","code":"\n# numerical\ncoronary %>% select(-id, -cad, -race, -gender) %>% describe()\n#>      vars   n   mean    sd median trimmed   mad   min\n#> sbp     1 200 130.18 19.81 126.00  128.93 17.79 88.00\n#> dbp     2 200  82.31 12.90  80.00   81.68 14.83 56.00\n#> chol    3 200   6.20  1.18   6.19    6.18  1.18  4.00\n#> age     4 200  47.33  7.34  47.00   47.28  8.15 32.00\n#> bmi     5 200  37.45  2.68  37.80   37.65  2.37 28.99\n#>         max range  skew kurtosis   se\n#> sbp  187.00 99.00  0.53    -0.37 1.40\n#> dbp  120.00 64.00  0.42    -0.33 0.91\n#> chol   9.35  5.35  0.18    -0.31 0.08\n#> age   62.00 30.00  0.05    -0.78 0.52\n#> bmi   45.03 16.03 -0.55     0.42 0.19\n# categorical\ncoronary %>% select(race, gender) %>% tbl_summary()\ncoronary %>% select(-id, -cad) %>% ggpairs()"},{"path":"lin-reg.html","id":"univariable-analysis-1","chapter":"8 Linear Regression","heading":"8.4.3 Univariable analysis","text":"univariable analysis context MLR, aim select variables worthwhile included multivariable model.context exploratory research, want choose variables P-values < 0.25 included MLR. obtain P-values, may perform separate SLRs predictors (). However, obtaining P-value predictor easy add1() function. , use likelihood ratio test (LRT) using test = \"LRT\" option obtain P-values. start intercept model slr_chol0 using chol ~ 1 formula specification glm followed add1(). add1() test predictor one one.output, variables important P < .25 except gender. variables, excluding gender, candidates variable selection step.However, please keep mind context confirmatory research, variables want include merely based P-values alone. important consider expert judgement well.","code":"\nslr_chol0 = glm(chol ~ 1, data = coronary)  # intercept only model\nadd1(slr_chol0, scope = ~ sbp + dbp + age + bmi + race + gender, \n     test = \"LRT\")\n#> Single term additions\n#> \n#> Model:\n#> chol ~ 1\n#>        Df Deviance    AIC scaled dev.        Pr(>Chi)    \n#> <none>      278.77 637.99                                \n#> sbp     1   235.36 606.14      33.855 0.0000000059384 ***\n#> dbp     1   228.64 600.34      39.648 0.0000000003042 ***\n#> age     1   243.68 613.08      26.911 0.0000002130129 ***\n#> bmi     1   272.17 635.20       4.792         0.02859 *  \n#> race    2   241.68 613.43      28.561 0.0000006280216 ***\n#> gender  1   277.45 639.04       0.952         0.32933    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lin-reg.html","id":"multivariable-analysis","chapter":"8 Linear Regression","heading":"8.4.4 Multivariable analysis","text":"Multivariable analysis involves one predictors. univariable variable selection, decided several potential predictors. MLR, (judiciously) included variables MLR model. present dataset, following considerations:including SBP DBP redundant, represent blood pressure. variables also highly correlated. indicated correlation value, r = 0.828 scatter plot SBP-DBP pair pairs plot data exploration step.gender sigficant, thus may exclude variable.let say, advised experts field, exclude age modelling.Now, given considerations, perform MLR selected variables,output , variable, focus results:coefficients, \\(b\\)s â€“ column estimate.95% CIs â€“ columns conf.low conf.high.P-values â€“ column p.value.Note categorical variable two categories, estimates obtained dummy variable. case, race consists Malay, Chinese Indian. output, dummy variables racechinese representing Chinese vs Malay raceindian representing Indian vs Malay dummy variables, Malay set baseline comparison group.also notice variables significant significance level 0.05, namely bmi racechinese. racechinese dummy variable, forms part race variable, accept variable marginally insignificant (0.0512 vs 0.05) dummy variable raceindian significant.","code":"\nmlr_chol = glm(chol ~ dbp + bmi + race, data = coronary)\nsummary(mlr_chol)\n#> \n#> Call:\n#> glm(formula = chol ~ dbp + bmi + race, data = coronary)\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -2.18698  -0.73076  -0.01935   0.63476   2.91524  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value   Pr(>|t|)    \n#> (Intercept)  4.870859   1.245373   3.911   0.000127 ***\n#> dbp          0.029500   0.006203   4.756 0.00000383 ***\n#> bmi         -0.038530   0.028099  -1.371   0.171871    \n#> racechinese  0.356642   0.181757   1.962   0.051164 .  \n#> raceindian   0.724716   0.190625   3.802   0.000192 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 1.083909)\n#> \n#>     Null deviance: 278.77  on 199  degrees of freedom\n#> Residual deviance: 211.36  on 195  degrees of freedom\n#> AIC: 590.63\n#> \n#> Number of Fisher Scoring iterations: 2"},{"path":"lin-reg.html","id":"stepwise-automatic-variable-selection","chapter":"8 Linear Regression","heading":"Stepwise automatic variable selection","text":"noted variables included model significant. case, may remove bmi statistically significant. exploratory research hundreds variables, impossible select variables eye-ball judgement. , case, perform variable selection? explore significance variables, may perform stepwise automatic selection. important know stepwise selection meant exploratory research. confirmatory analysis, important rely expert opinion variable selection. may perform forward, backward forward backward selection combined.Forward selection starts intercept empty model without variable. proceeds adding one variable another. R, Akaike information criterion (AIC) used comparative goodness--fit measure model quality. stepwise selection, seeks find model lowest AIC iteratively steps shown output. information AIC can referred Shuhua Hu3 Wikipedia4.Backward selection starts model containing variables. , proceeds removing one variable another, aims find model lowest AIC.Bidirectional selection, implemented R, starts model variables. , proceeds removing adding variables, combines forward backward selection methods. stops finds model lowest AIC.","code":"\n# forward\nmlr_chol_stepforward = step(slr_chol0, scope = ~ dbp + bmi + race, \n                            direction = \"forward\")\n#> Start:  AIC=637.99\n#> chol ~ 1\n#> \n#>        Df Deviance    AIC\n#> + dbp   1   228.64 600.34\n#> + race  2   241.68 613.43\n#> + bmi   1   272.17 635.20\n#> <none>      278.77 637.99\n#> \n#> Step:  AIC=600.34\n#> chol ~ dbp\n#> \n#>        Df Deviance    AIC\n#> + race  2   213.40 590.55\n#> <none>      228.64 600.34\n#> + bmi   1   227.04 600.94\n#> \n#> Step:  AIC=590.55\n#> chol ~ dbp + race\n#> \n#>        Df Deviance    AIC\n#> <none>      213.40 590.55\n#> + bmi   1   211.36 590.63\n# backward\nmlr_chol_stepback = step(mlr_chol, direction = \"backward\")\n#> Start:  AIC=590.63\n#> chol ~ dbp + bmi + race\n#> \n#>        Df Deviance    AIC\n#> - bmi   1   213.40 590.55\n#> <none>      211.36 590.63\n#> - race  2   227.04 600.94\n#> - dbp   1   235.88 610.58\n#> \n#> Step:  AIC=590.55\n#> chol ~ dbp + race\n#> \n#>        Df Deviance    AIC\n#> <none>      213.40 590.55\n#> - race  2   228.64 600.34\n#> - dbp   1   241.68 613.43\n# both\nmlr_chol_stepboth = step(mlr_chol, direction = \"both\")\n#> Start:  AIC=590.63\n#> chol ~ dbp + bmi + race\n#> \n#>        Df Deviance    AIC\n#> - bmi   1   213.40 590.55\n#> <none>      211.36 590.63\n#> - race  2   227.04 600.94\n#> - dbp   1   235.88 610.58\n#> \n#> Step:  AIC=590.55\n#> chol ~ dbp + race\n#> \n#>        Df Deviance    AIC\n#> <none>      213.40 590.55\n#> + bmi   1   211.36 590.63\n#> - race  2   228.64 600.34\n#> - dbp   1   241.68 613.43"},{"path":"lin-reg.html","id":"preliminary-model","chapter":"8 Linear Regression","heading":"Preliminary model","text":"Let say, considering P-value, stepwise selection (exploratory research) expert opinion, decided preliminary model ,chol ~ dbp + raceand fit model view basic information model,","code":"\nmlr_chol_sel = glm(chol ~ dbp + race, data = coronary)\nsummary(mlr_chol_sel)\n#> \n#> Call:\n#> glm(formula = chol ~ dbp + race, data = coronary)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1378  -0.7068  -0.0289   0.5997   2.7778  \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value       Pr(>|t|)    \n#> (Intercept) 3.298028   0.486213   6.783 0.000000000136 ***\n#> dbp         0.031108   0.006104   5.096 0.000000813502 ***\n#> racechinese 0.359964   0.182149   1.976       0.049534 *  \n#> raceindian  0.713690   0.190883   3.739       0.000243 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 1.088777)\n#> \n#>     Null deviance: 278.77  on 199  degrees of freedom\n#> Residual deviance: 213.40  on 196  degrees of freedom\n#> AIC: 590.55\n#> \n#> Number of Fisher Scoring iterations: 2\nrsq(mlr_chol_sel)\n#> [1] 0.2345037"},{"path":"lin-reg.html","id":"interaction","chapter":"8 Linear Regression","heading":"8.4.5 Interaction","text":"Interaction combination predictors requires interpretation regression coefficients separately based levels predictor. example, need separate interpretation coefficient dbp depending race group: Malay, Chinese Indian. makes interpreting analysis complicated can longer interpret coefficient . , time, pray interaction regression model. fit model two-way interaction term,output, evidence suggests presence interaction included interaction term insignificant. R, easy fit interaction *, e.g.Â dbp * race automatically includes variables involved. equal specifiying glm(chol ~ dbp + race + dbp:race, data = coronary), can use : include interaction.","code":"\nsummary(glm(chol ~ dbp * race, data = coronary))\n#> \n#> Call:\n#> glm(formula = chol ~ dbp * race, data = coronary)\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -2.10485  -0.77524  -0.02423   0.58059   2.74380  \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      2.11114    0.92803   2.275 0.024008 *  \n#> dbp              0.04650    0.01193   3.897 0.000134 ***\n#> racechinese      1.95576    1.28477   1.522 0.129572    \n#> raceindian       2.41530    1.25766   1.920 0.056266 .  \n#> dbp:racechinese -0.02033    0.01596  -1.273 0.204376    \n#> dbp:raceindian  -0.02126    0.01529  -1.391 0.165905    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 1.087348)\n#> \n#>     Null deviance: 278.77  on 199  degrees of freedom\n#> Residual deviance: 210.95  on 194  degrees of freedom\n#> AIC: 592.23\n#> \n#> Number of Fisher Scoring iterations: 2"},{"path":"lin-reg.html","id":"model-fit-assessment-1","chapter":"8 Linear Regression","heading":"8.4.6 Model fit assessment","text":"MLR, assess model fit \\(R^2\\) histogram scatter plots residuals. Residuals, simple term, discrepancies observed values (dots) predicted values (fit MLR model). , lesser discrepancies, better model fit.","code":""},{"path":"lin-reg.html","id":"percentage-of-variance-explained-r2","chapter":"8 Linear Regression","heading":"Percentage of variance explained, \\(R^2\\)","text":"First, obtain \\(R^2\\) values. comparison \\(R^2\\) obtained SLR, include adj = TRUE obtain adjusted \\(R^2\\). adjusted \\(R^2\\) \\(R^2\\) penalty number predictors p. discourages including many variables, might unnecessary.","code":"\nrsq(mlr_chol_sel, adj = TRUE)\n#> [1] 0.2227869"},{"path":"lin-reg.html","id":"histogram-and-box-and-whiskers-plot","chapter":"8 Linear Regression","heading":"Histogram and box-and-whiskers plot","text":"Second, plot histogram box--whiskers plot assess normality raw/unstandardized residuals MLR model. expect normally distributed residuals indicate good fit MLR model. , normally distributed residuals.","code":"\nrraw_chol = resid(mlr_chol_sel)\nhist(rraw_chol)\nboxplot(rraw_chol)"},{"path":"lin-reg.html","id":"scatter-plots","chapter":"8 Linear Regression","heading":"Scatter plots","text":"Third, plot standardized residuals (Y-axis) vs standardized predicted values (X-axis). Similar one SLR, plot allows assessment normality, linearity equal variance assumptions. dots form elliptical/oval shape (normality) scattered roughly equal zero line (equal variance). indicate linearity. plot shows assumptions met.addition standardized residuals vs standardized predicted values plot, numerical predictors, assess linear relationship raw residuals observed values numerical predictors. plot raw residuals vs numerical predictor . plot interpreted similar way standardized residuals vs standardized predicted values plot. plot shows good linearity residuals numerical predictor.","code":"\nrstd_chol = rstandard(mlr_chol_sel)  # standardized residuals\npstd_chol = scale(predict(mlr_chol_sel))  # standardized predicted values\nplot(rstd_chol ~ pstd_chol, xlab = \"Std predicted\", ylab = \"Std residuals\")\nabline(0, 0)  # normal, linear, equal variance\nplot(rraw_chol ~ coronary$dbp, xlab = \"DBP\", ylab = \"Raw Residuals\")\nabline(0, 0)"},{"path":"lin-reg.html","id":"presentation-and-interpretation-1","chapter":"8 Linear Regression","heading":"8.4.7 Presentation and interpretation","text":"passing assumption checks, may now decide final model. may rename preliminary model mlr_chol_sel mlr_chol_final easier reference.Similar SLR, use tbl_regression() come nice table present results.useful able save output spreadsheet format later use. can use tidy() function case export .csv file,, present model equation. Cholesterol level mmol/L can predicted predictors given ,\\[chol = 3.30 + 0.03\\times dbp + 0.36\\times race\\ (chinese) + 0.71\\times race\\ (indian)\\]\nBased adjusted \\(R^2\\), table model equation, may interpret results follows:1mmHg increase DBP causes 0.03mmol/L increase cholesterol, controlling effect race.Likewise, 10mmHg increase DBP causes 0.03 x 10 = 0.3mmol/L increase cholesterol, controlling effect race.Chinese causes 0.36mmol/L increase cholesterol comparison Malay, controlling effect DBP.Indian causes 0.71mmol/L increase cholesterol comparison Malay, controlling effect DBP.DBP race explains 22.3% variance cholesterol.interpretation, please keep mind also consider 95% CI coefficient. example, Indian causes 0.71mmol/L increase cholesterol comparison Malay, may range 0.34mmol/L 1.1mmol/L based 95% CI.","code":"\nmlr_chol_final = mlr_chol_sel\ntbl_regression(mlr_chol_final, intercept = TRUE)\ntib_mlr = tidy(mlr_chol_final, conf.int = TRUE)\nwrite.csv(tib_mlr, \"mlr_final.csv\")"},{"path":"lin-reg.html","id":"prediction","chapter":"8 Linear Regression","heading":"8.5 Prediction","text":"situations, useful use SLR/MLR model prediction. example, may want predict cholesterol level patient given clinical characteristics. can use final model prediction. starter, let us view predicted values sample,Compare predicted values observed cholesterol level. Recall already checked model fit assessment .useful predict newly observed data. Let us try predicting cholesterol level Indian patient DBP = 90mmHg,Now, also data many patients,","code":"\ncoronary$pred_chol = predict(mlr_chol_final)\nhead(coronary)\n#>   id    cad sbp dbp   chol age  bmi   race gender pred_chol\n#> 1  1 no cad 106  68 6.5725  60 38.9 indian  woman  6.127070\n#> 2 14 no cad 130  78 6.3250  34 37.8  malay  woman  5.724461\n#> 3 56 no cad 136  84 5.9675  36 40.5  malay  woman  5.911109\n#> 4 61 no cad 138 100 7.0400  45 37.6  malay  woman  6.408839\n#> 5 62 no cad 115  85 6.6550  53 40.3 indian    man  6.655908\n#> 6 64 no cad 124  72 5.9675  43 37.6  malay    man  5.537812\npredict(mlr_chol_final, list(dbp = 90, race = \"indian\"))\n#>        1 \n#> 6.811448\nnew_data = data.frame(dbp = c(90, 90, 90), \n                      race = c(\"malay\", \"chinese\", \"indian\"))\npredict(mlr_chol_final, new_data)\n#>        1        2        3 \n#> 6.097758 6.457722 6.811448\nnew_data$pred_chol = predict(mlr_chol_final, new_data)\nnew_data\n#>   dbp    race pred_chol\n#> 1  90   malay  6.097758\n#> 2  90 chinese  6.457722\n#> 3  90  indian  6.811448"},{"path":"lin-reg.html","id":"summary","chapter":"8 Linear Regression","heading":"8.6 Summary","text":"chapter, went basics SLR MLR. performed analysis learned assess model fit regression models. learned nicely present interpret results. addition, also learned utilize model prediction.","code":""},{"path":"lin-reg-class.html","id":"lin-reg-class","chapter":"9 ğŸ’» Linear Regression in class","heading":"9 ğŸ’» Linear Regression in class","text":"","code":""},{"path":"lin-reg-class.html","id":"general-linear-regression-workflow-caveats","chapter":"9 ğŸ’» Linear Regression in class","heading":"9.1 General Linear Regression Workflow & Caveats ğŸ—’","text":"fix memory following steps:install package already dot load .e.Â install.package(\"<package>\") library(<package>). remember also install.package needs quotation, indeed library donâ€™t. Make sure also internet connection, otherwise canâ€™t download anything.install package already dot load .e.Â install.package(\"<package>\") library(<package>). remember also install.package needs quotation, indeed library donâ€™t. Make sure also internet connection, otherwise canâ€™t download anything.Load data environment data(\"<dataset>\")Load data environment data(\"<dataset>\")bis data manipulation step:bis data manipulation step:select columns ()filter rowsencode factor/ numericdo model lm(formula = Y ~ X + X2 + X3, data = <dataset>) formula model fitting (understand question) data actual datasetdo model lm(formula = Y ~ X + X2 + X3, data = <dataset>) formula model fitting (understand question) data actual datasetsummary modelsummary modelPlease also donâ€™t just skip parenthesis, quotations .e.Â â€œâ€œ, Capital lowercase characters. matters! Prior throwing outside window laptop least check â€™s issues code.","code":""},{"path":"lin-reg-class.html","id":"collinearity-and-vif","chapter":"9 ğŸ’» Linear Regression in class","heading":"9.2 collinearity and VIF","text":"Collinearity phenomenon multiple linear regression two independent variables regression model highly correlated. can lead multicollinearity, making challenging interpret individual effects predictor variable. Variance Inflation Factor (VIF) measure used detect quantify collinearity regression analysis. high VIF indicates predictor variable highly correlated predictor variables model.calculate VIF via function VIF() contained package regclass, donâ€™t please install (.e.Â install.packages(\"regclass\")).output provide VIF values predictor variable. see high VIF (typically greater 5 10), indicates potential issue collinearity, may need consider removing one correlated predictor variables model address problem. Lower VIF values preferred, indicate less multicollinearity.Keep mind VIF calculated predictor variable separately, considering correlation predictor variables. helps assess impact collinearity variableâ€™s variance.","code":"\nlibrary(regclass)\n#> Loading required package: bestglm\n#> Loading required package: leaps\n#> Loading required package: VGAM\n#> Loading required package: stats4\n#> Loading required package: splines\n#> \n#> Attaching package: 'VGAM'\n#> The following object is masked from 'package:webexercises':\n#> \n#>     round2\n#> Loading required package: rpart\n#> Loading required package: randomForest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> Important regclass change from 1.3:\n#> All functions that had a . in the name now have an _\n#> all.correlations -> all_correlations, cor.demo -> cor_demo, etc.\n\ndata(mtcars)\n\n# Fit a multiple linear regression model\nmodel <- lm(mpg ~ wt + hp + qsec + disp, data = mtcars)\n\n# Calculate VIF for each predictor variable in the model\nvif_values <- VIF(model)\nvif_values\n#>       wt       hp     qsec     disp \n#> 6.916942 5.166758 3.133119 7.985439"},{"path":"lin-reg-class.html","id":"class-exercies-2024-10-26","chapter":"9 ğŸ’» Linear Regression in class","heading":"9.3 Class exercies @ 2024-10-26 ğŸ›","text":"Exercise 9.1  Using dataset baltimore downloadable library spdep\nEstimate simple linear regression model explains price (PRICE)\nfunction â€œageâ€ house (AGE)\n> slope significant 5%?Exercise 9.2  coleman data set robustbase library lists summary statistics 20 different schools northeast US.\nsix variables measured school include demographic information (percent white-collar fathers) \ncharacteristics school (staff salaries per pupil).\n> 1. Code regression model explaining variable Y, using varibles.\n> 2. variable significant?Exercise 9.3  Using dataset Bikeshare library ISLR2,\nEstimate regression explains causal users function registered users\n> p-value estimate intercept?Bikeshare data descr:\ndata set contains hourly daily count rental bikes years 2011 2012 Capital bikeshare system, along weather seasonal information.Exercise 9.4  Given dataset â€œDuncanâ€ library â€œcarDataâ€ estimate regression model \nvariable prestige regressed variables income education.\n>variable significant?Duncan data descr:\nDuncan data frame 45 rows 4 columns. Data prestige characteristics 45 U. S. occupations 1950.Answer Exercise 9.4the significant variables income education 0 alpha level.\nmight worth noting also two positive signs .e.Â 0.59, 0.54\nsuggests prestige positively linked income education level.Exercise 9.5  Given dataset state.x77 already present R perform following tasks\n> 1. load state datasets.\n> 2. Convert state.x77 dataset dataframe.\n> 3. Rename Life Exp variable Life.Exp, HS Grad HS.Grad. (avoids\nproblems referring variables specifying model.)Exercise 9.6  Suppose wanted enter variables first-order linear regression model Life Expectancy dependent variable.\n> 1. Fit modelExercise 9.7  Letâ€™s assume settled model HS.Grad Murder predictors.Fit model.Add interaction term previous model.Predict Life Expectancy state 55% population High School graduates, murder rate 8 per 100,000.Exercise 9.8  exercise, â€™ll perform multiple linear regression interaction terms using mtcars dataset. Create model interaction â€œwtâ€ â€œhpâ€ calculate R-squared adjusted R-squared.Exercise 9.9  Using dataset swiss, fit model whose response variable â€œFertilityâ€ whose explanatory variable rest.\nverify absence presence collinearity predictors. find model without collinear predictors?","code":"\n# if you don't have that, install it at first\n# install.packages(\"spdep\")\nlibrary(spdep)\ndata(\"baltimore\")\n\n# let's look at baltimore\n# View(baltimore)\ndim(baltimore)\nstr(baltimore)\ncolnames(baltimore)\n\n## estimate regression\n\nbaltimore_regression = lm(PRICE ~ AGE, data = baltimore)\nsummary(baltimore_regression)\n# install.packages(\"robustbase\") if you dont already have the package\nlibrary(robustbase)\ndata(\"coleman\")\n\n## explore data, how many rows it has?\n## View(coleman)\nstr(coleman)\n\n\nmodel_coleman = lm(formula = Y ~ ., data = coleman)\nsummary(model_coleman)\n# for those who can not install that, don't worry,\n# manifest tyour problem and I will make avaible for you the dataset\n# in a different way! \n# install.packages(\"ISLR2\")\nlibrary(ISLR2)\ndata(\"Bikeshare\")\n\n\n## !! for those we cant install ISLR2 then execte the follwing !!\nBikeshare = read.csv(\"https://raw.githubusercontent.com/NiccoloSalvini/sbd_22-23/main/data/bikeshare.csv?token=GHSAT0AAAAAABZG6GFQDNXQRWROYOGNIKCIY2X7Y4Q\")\n\n## explore data, focus on column types\nstr(Bikeshare)\n\n## fit model\nbikeshare_model = lm(formula = casual~registered, data = Bikeshare)\nsummary(bikeshare_model)\n## install package if already dont have it\n## install.packages(\"carData\")  please notice that the \"D\" is uppercase\nlibrary(carData)\ndata(\"Duncan\")\n\n\n##explore Duncan dataset\nstr(Duncan) ## 45 rows x 4 columns\n\n## any prep needed\n\n## fit model\n\nduncan_model = lm(prestige ~ income + education, data  = Duncan)\nsummary(duncan_model)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***\n--- "},{"path":"lin-reg-class.html","id":"solutions-1","chapter":"9 ğŸ’» Linear Regression in class","heading":"9.4 solutions","text":"Answer Exercise 9.1Answer Exercise 9.2:significant coefficient regression model fitted â€œsstatusâ€ nearly 0 alpha,\nhowever â€™s also â€œteacherScâ€ significant witha 5% alpha.Answer Exercise 9.3:estimate value intercept 7.393972Answer Exercise 9.5dataset state.x77 comes default R. donâ€™t need install library. Point 1 done.need convert (sometimes people like say â€œcoerceâ€) object class another class, say matrix dataframe (common case) need apply function.\nnew_state.x77 = data.frame(state.x77)need convert (sometimes people like say â€œcoerceâ€) object class another class, say matrix dataframe (common case) need apply function.new_state.x77 = data.frame(state.x77)looking column name conversion dataframe data.frame() may notice something weird. print commands:\ncolnames(state.x77)\ncolnames(data.frame(state.x77))looking column name conversion dataframe data.frame() may notice something weird. print commands:colnames(state.x77)\ncolnames(data.frame(state.x77))conversion R took care also renaming column names desired way. happens? R needs column name type notation. R neither want â€ â€ empty spaces accents â€œÃ â€ column names. Essentially R undestand signs complains .Answer Exercise 9.6Answer Exercise 9.7fit model HS.Grad Murder predictors\nlinear_model_1 = lm(Life.Exp~ HS.Grad + Murder, data = new_state.x77)\nsummary(linear_model_1)fit model HS.Grad Murder predictorslinear_model_1 = lm(Life.Exp~ HS.Grad + Murder, data = new_state.x77)\nsummary(linear_model_1)add interaction term model\nlinear_model_interaction = lm(Life.Exp~ HS.Grad + Murder + HS.Grad*Murder, data = new_state.x77)\nsummary(linear_model_interaction)add interaction term modellinear_model_interaction = lm(Life.Exp~ HS.Grad + Murder + HS.Grad*Murder, data = new_state.x77)\nsummary(linear_model_interaction)calculate Life expectancy given 55% pop 8 10^5 rate. Look help state.x77 see scales numbers expressed.\ny = 67.831203 + 0.08936855 + 80.023510 -0.004959558calculate Life expectancy given 55% pop 8 10^5 rate. Look help state.x77 see scales numbers expressed.y = 67.831203 + 0.08936855 + 80.023510 -0.004959558this results 70.75256Answer Exercise 9.8Answer Exercise 9.9print console vif. collinearity detected. model suffer collinerity.","code":" Coefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 55.08500    2.82833   19.48 < 0.0000000000000002 ***\nAGE         -0.35802    0.07851   -4.56            0.0000087 ***\n---\ncoefficient AGE is significant up to a 0% alpha level, therefore it also is at 5% Coefficients:\n            Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 19.94857   13.62755   1.464    0.1653    \nsalaryP     -1.79333    1.23340  -1.454    0.1680    \nfatherWc     0.04360    0.05326   0.819    0.4267    \nsstatus      0.55576    0.09296   5.979 0.0000338 ***\nteacherSc    1.11017    0.43377   2.559    0.0227 *  \nmotherLev   -1.81092    2.02739  -0.893    0.3868    Coefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept) 7.393972   0.518483   14.26 <0.0000000000000002 ***\nregistered  0.184095   0.003263   56.42 <0.0000000000000002 ***\n... simple_linear_model = lm(Life.Exp~.,  data =  new_state.x77)\nsummary(simple_linear_model)# Load the regclass package\nlibrary(regclass)\n\n# Create interaction term between \"wt\" and \"hp\"\nmtcars$wt_hp_interaction <- mtcars$wt * mtcars$hp\n\n# Perform multiple linear regression with the interaction term\nmodel <- lm(mpg ~ wt + hp + wt_hp_interaction, data = mtcars)\n\n# Calculate R-squared\nr_squared <- R2(model)\n\n# Calculate adjusted R-squared\nadjusted_r_squared <- adjR2(model)\n\n# Print R-squared and adjusted R-squared values\ncat(\"R-squared:\", r_squared, \"\\n\")\ncat(\"Adjusted R-squared:\", adjusted_r_squared, \"\\n\")# Load the regclass package\nlibrary(regclass)\n\n# Load the 'swiss' dataset (data on fertility and socio-economic indicators in the Swiss cantons)\ndata(swiss)\n\n# Perform multiple linear regression with \"Fertility\" as the dependent variable\nmodel <- lm(Fertility ~ ., data = swiss)\n\n# Calculate Variance Inflation Factors (VIF) for independent variables\nvif <- VIF(model)"},{"path":"int-samp-q-second.html","id":"int-samp-q-second","chapter":"10 ğŸ’» Second Intermediate Sample Questions","heading":"10 ğŸ’» Second Intermediate Sample Questions","text":"Hi guys, sample questions prof Dabo gave us exercise . may notice open questions superifical theory concepts, indeep math heavy calculations (matrix products, dot products etc.). please suggestion review carefully slides just learn basic R commands execute analysis higer level! ğŸ€","code":""},{"path":"int-samp-q-second.html","id":"nd-intermediate","chapter":"10 ğŸ’» Second Intermediate Sample Questions","heading":"10.1 ğŸ‘¨â€ğŸ“ 2023/2024 (2nd intermediate)","text":"Exercise 10.1  Basic Understanding:PCA stand ?Briefly explain primary objective Principal Component Analysis.PCA help dimensionality reduction?Exercise 10.2  Library Data Loading:R library commonly used performing PCA?Write command load library FactomineR PCA.read dataset R PCA analysis?Exercise 10.3  Data Preparation:Explain importance scaling standardizing variables applying PCA.Write R command standardize data matrix.Exercise 10.4  PCA Execution:function R used perform PCA?Provide basic syntax running PCA dataset named Â«Â my_data.â€Exercise 10.5  Interpretation Results:can access proportion variance explained principal component following R script?significance eigenvalues eigenvectors PCA?Exercise 10.6  Selecting Principal Components:can determine optimal number principal components retain R?Write R command extract loadings principal components.Exercise 10.7  inertia centered matrix n individuals p quantitative variables ispThe sum variances p variablesNone responses trueExercise 10.8  principal components (coordinates individuals) un-correlatedTRUEFALSEExercise 10.9  normed PCA, mean eigen-values 123Exercise 10.10  Let Z matrix (50 rows 4 columns) centered reduced quantitative data, correlation matrix R (dimension 4) three eigenvalues 2, 1 0.4.Give maximum number eigen-valuesGive remaining eigen-valuesExercise 10.11  dataset X gives, 23 Charolais Zebus cattles, 6 different weights, kg: live weight (W_LIV), carcass weight (W_CAR), prime meat weight (W_QUALI), total meat weight (W_TOTAL), fat meat weight (W_FAT), bone weight (W_BO) cattle type (Type).interpret following correlation matrix plot?many components choose regarding following figures (giving eigen-values correlation components variables)Interpret following figure:Exercise 10.12  Scree Plot:purpose scree plot PCA?purpose scree plot PCA?generate interpret following scree plot ?generate interpret following scree plot ?Exercise 10.13  Scree Plot:Briefly explain main objective Correspondence Analysis (CA).CA different Principal Component Analysis (PCA)?Provide example scenario CA suitable analysis.Exercise 10.14  Correspondence Analysis:Briefly explain main objective Correspondence Analysis (CA).CA different Principal Component Analysis (PCA)?Provide example scenario CA suitable analysis.Exercise 10.14  CA Execution:Provide basic syntax running CA contingency table named â€œmy_table.â€Exercise 10.14  Interpretation Results:can access row column scores CA results R?Exercise 10.15  Visualization:Write R command create biplot Correspondence Analysis result.can visually assess relationships rows columns CA plot?Exercise 10.16  E3:Write R command extract contributions dimensions CA (write general)Exercise 10.17  Disjunctive table:Construct disjunctive table following datalibrary(tibble)\ndisj_table = tribble(\n~Var1, ~Var2, ~Var3,\nâ€œCBâ€, â€œYBâ€, â€œFâ€,\nâ€œCBâ€, â€œYVâ€, â€œFâ€,\nâ€œCCâ€, â€œYBâ€, â€œMâ€,\nâ€œCCâ€, â€œYMâ€, â€œFâ€,\nâ€œCRâ€, â€œYVâ€, â€œMâ€,\nâ€œCBâ€, â€œYBâ€, â€œMâ€\n)Exercise 10.18  Chi-Square Test:role chi-square test Correspondence Analysis?can perform chi-square test CA result R?Exercise 10.19  Clustering:main goal clustering algorithms?Exercise 10.20  K-Means Clustering:fundamental concept behind K-means clustering?Explain meaning centroids context K-means clustering.Write R command perform K-means clustering dataset named â€œmy_data.â€Exercise 10.21  Hierarchical Clustering:Briefly explain hierarchical clustering works.Write R command conduct hierarchical clustering dataset.Exercise 10.22  Interpretation Clustering Results:interpret following output clustering analysis cattle data?Exercise 10.23  Classification:main goal classification ?Provide example real-world application classification analysis beneficial.can classification used medical diagnosis fraud detection?Exercise 10.24  PCA stand ?Primary Component AnalysisPrincipal Component AlgorithmPrincipal Component AnalysisPrimary Component AlgorithmExercise 10.25  PCA, primary goal?Reduce dimensionality preserving varianceIncrease dimensionality better visualizationMinimize components equallyFocus individual components onlyExercise 10.26  R function commonly used perform PCA?kmeans()PCA()prcomp()corresp()Exercise 10.27  purpose scree plot PCA?Visualize clusters dataAssess quality clusteringEvaluate distribution dataDisplay eigenvalues principal componentsExercise 10.28  determine optimal number principal components retain PCA?Use hierarchical clusteringExamine scree plotApply k-means clusteringPerform chi-square testExercise 10.29  primary application Correspondence Analysis (CA)?Reducing dimensionality numerical dataAnalyzing relationships categorical dataClassifying data points clustersPredicting future values time seriesExercise 10.30  R library commonly used Correspondence Analysis?clustercaretcafactoextraFactomineRExercise 10.31  role chi-square test Correspondence Analysis?Assess significance relationshipsDetermine optimal number clustersEvaluate distribution dataVisualize proximity data pointsExercise 10.32  primary goal clustering algorithms?Dimensionality reductionClassificationGrouping similar data pointsVisualization dataExercise 10.33  R function commonly used K-means clustering?hierarch()PCA()kmeans()prcomp()Exercise 10.34  can visually assess relationships rows columns clustering plot?Scree plotDendrogramSilhouette plotBiplotExercise 10.35  primary goal classification algorithm?Group similar data pointsPredict numerical valuesAssign labels data pointsVisualize high-dimensional dataExercise 10.36  algorithm commonly used binary classification tasks? (Answers can mnore one)Decision TreesK-meansLDALogistic RegressionExercise 10.37  metric commonly used evaluate performance classification model?R-squaredMean Absolute ErrorSilhouette ScoreAccuracy","code":""},{"path":"int-samp-q-second.html","id":"solutions-2","chapter":"10 ğŸ’» Second Intermediate Sample Questions","heading":"10.2 solutions","text":"Answer Question 10.7:pThe sum variances p variablesNone responses trueAnswer Question 10.8:TRUEFALSEAnswer Question 10.9:123Answer Question 10.30:clustercaretcafactoextraFactomineRAnswer Question 10.32:Dimensionality reductionClassificationGrouping similar data pointsVisualization dataAnswer Question 10.33:hierarch()PCA()kmeans()prcomp()aAnswer Question 10.34:Scree plotDendrogramSilhouette plotBiplotAnswer Question 10.35:Group similar data pointsPredict numerical valuesAssign labels data pointsVisualize high-dimensional dataAnswer Question 10.36:Decision TreesK-meansLDALogistic RegressionAnswer Question 10.37:R-squaredMean Absolute ErrorSilhouette ScoreAccuracy","code":""},{"path":"int-samp-q.html","id":"int-samp-q","chapter":"11 ğŸ’» Intermediate Sample Questions","heading":"11 ğŸ’» Intermediate Sample Questions","text":"Hi guys, favourite TA, just aggregating questions asked previous exam sessions previous years .e.Â 2020/2021 2021/2022. representative actual exam, know, take like grain salt.also make sure provide exercises still anxious.","code":""},{"path":"int-samp-q.html","id":"section","chapter":"11 ğŸ’» Intermediate Sample Questions","heading":"11.1 ğŸ‘¨â€ğŸ“ 2020/2021","text":"Exercise 11.1  Write line R command use produce boxplot variable XExercise 11.2  want test statistically hypothesis performances students UCSC Rome graduated last year better graduated year. Can say paired sample test ?Exercise 11.3  Without using formulae, describe can calculate test statistics hypothesis testing procedure single mean known variance.Exercise 11.4  Using dataset Boston downloaded library spdep, write correlation matrix variables MEDV, NOX CRIM.Exercise 11.5  define confidence statistical test?Exercise 11.6  Given following 2 variables X = (1,5,3,3,5,5) Y= (4,4,6,3,2,3), write cross-tabulation X Y.Exercise 11.7  Write line R command use simulate 1000 random observation normal distribution 0 mean variance = 0.5.Exercise 11.8  law company evaluating performances two departments measuring terms time required solving conflict last year. observed values reported following table:â€¦can accept hypothesis H0: (mean Dept 1 equal mean Dept 2) versus bilateral alternative hypothesis? (F)Exercise 11.9  company recorded number costumers 10 sample stores (variable X) (Variable Y) new advertising campaign introduced. observed values reported following tableâ€¦write p-value test H0: (mean X equal mean Y) versus bilateral alternative hypothesis. ( 0,000341138)Exercise 11.10  HR office cleaning company wants test gender discrimination employees. Call X = income set 20 male workers Y = income set 35 female workers. Write line R command run appropriate test hypothesis.Exercise 11.11  power statistical test?Exercise 11.12  Using dataset boston.c downloaded library spdep, calculate coefficient skewness variable RM.Answer Exercise 11.12:Exercise 11.13  define significance statistical test?","code":"library(moments)\nskewness(boston.c$RM)\n\n0,4024147"},{"path":"int-samp-q.html","id":"section-1","chapter":"11 ğŸ’» Intermediate Sample Questions","heading":"11.2 ğŸ‘¨â€ğŸ“ 2021/2022","text":"Exercise 11.14  Given dataset â€œDuncanâ€ library â€œcarDataâ€ estimate regression model variable prestige regressed variables income Looking following information,residuals display.Exercise 11.15  consequences collinearity among regressors?Estimators become biasedEstimators become inefficientEstimators become inconsistentEstimators become unstableExercise 11.16  correct definition variance inflation factor .e.Â VIF?\\(1-R2\\)\\(\\frac{1}{R2}\\)\\(\\frac{1}{1-R2}\\)\\(1-\\frac{1}{R2}\\)Answer Exercise 11.16:general guideline VIF larger 5 10 large, indicating model problems estimating coefficient. However, general degrade quality predictions. VIF larger 1/(1-R2), R2 Multiple R-squared regression, predictor related predictors response.alternatively can use library car use vif() functionExercise 11.17  Using following variables minority , crime , poverty , language highschool housing Ericksen data library carData, run factor analysis. percentage explained first two factors?risposta: 90.130.001Exercise 11.18  multiple linear regression model y= +bx1+cx2, Correlation(x1,x2)=0.9, discard one two variables collinearity?risposta: FExercise 11.19  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education. variable significant?EducationincomeAnswer Exercise 11.19:first load data Duncan datasetThen specify model produce sumamries:look pvalues andeducation significant income since 0.00000173 < 0.00001053Exercise 11.20  multiple linear regression model y= +bx1+cx2, level correlation x1 x2 beyond discard one two variables collinearity?risposta: 0.948Exercise 11.21  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education. p-value coefficient variable education?Answer Exercise 11.21:first load data Duncan datasetThen specify model produce sumamries:look pvalues andThe pvalue coefficient 0.00000173you may want directly access instead just copying pasting console sumamry outputExercise 11.22  reason adjusting R2 multiple regressionTo account number degrees freedomTo account number parametersTo reduce uncertaintyTo adjust variance inflation factorrispoasta: account number degrees freedomExercise 11.23  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income. Using VIF, exclude variable due collinearity?result: FAnswer Exercise 11.23:first load data Duncan datasetThen specify model produce sumamries:output look like something like.Since 10 rule thumb gave assess multicollinearity conclude neither income education collinear.Exercise 11.24  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income. value t value coefficient variable education?Answer Exercise 11.24:first load data Duncan datasetThen specify model produce sumamries:output look like something like.inspecting summary wee obtain t value (t value column summary) dor variable education 5.555Exercise 11.24  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using k-means method. divide observations 4 classes frequency largest class ?result: 26Exercise 11.25  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using k-means method. percentage explained first factor?risposta: 7.391.719Exercise 11.26  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using hierarchical method. divide observations 10 classes frequency largest class ?risposta: 27Exercise 11.27  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education report \\(R^2\\).Answer Exercise 11.27:first load data Duncan datasetThen specify model produce sumamries:output look like something like.inspecting lowe end summary obtain R2 (multiple) model 0.8282, high.","code":"Residuals:\n\nMin      1Q  Median      3Q     Max\n\n-29.538  -6.417   0.655   6.605  34.641install.packages(\"regclass\")\nlibrary(regclass)\nVIF(modello_regressione)install.packges(\"car\")\nlibrary(car)\nvif(modello_regressione)library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\nlibrary(car)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nvif(duncan_regression) income education \n 2.1049    2.1049 library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Residual standard error: 13.37 on 42 degrees of freedom\nMultiple R-squared:  0.8282,    Adjusted R-squared:   0.82 \nF-statistic: 101.2 on 2 and 42 DF,  p-value: < 0.00000000000000022\n    "},{"path":"int-samp-q.html","id":"section-2","chapter":"11 ğŸ’» Intermediate Sample Questions","heading":"11.3 ğŸ‘¨â€ğŸ“ 2022/2023","text":"Exercise 11.28  Using dataset Boston downloaded library spdep,\ncalculate coefficient skewness variable RM.Exercise 11.29  define significance statistical test?Exercise 11.30  power statistical test?Exercise 11.31  define confidence statistical test?Exercise 11.32  law company evaluating performances two departments\nmeasuring terms time required solving conflict \nlast year. observed values reported following table:can reject hypothesis H0: (mean Dept 1 equal \nmean Dept 2) versus bilateral alternative hypothesis?Exercise 11.33  company recorded number costumers 10 sample stores\n(variable X) (Variable Y) new advertising campaign\nintroduced. observed values reported following table:can reject hypothesis H0: (mean X, .e.Â equal \nmean Y, .e.Â ) versus bilateral alternative hypothesis?Exercise 11.34  Write line R command use simulate 2000 random\nobservation normal distribution 0 mean variance = 0.1Many fall trap!. Tip: always use â€œtabâ€ \nautomatic suggestion also check arguments. case\nexercise wants sample normal distribution 2000\ninstances (data points), 0 mean variance = 0.1. argument\nrnorm sd var, apply square root!Answer Question 11.34:Exercise 11.35  Write line R command use produce boxplot \nvariable XExercise 11.36  Given following 2 variables X = (5,5,3,3,5,5) \nY= (4,4,3,3,3,3), test mean X significantly different\nmean Y. Report p-value appropriate test \ndecision.Exercise 11.37  Using dataset boston.c downloaded library spdep, write\nelements correlation matrix variables MEDV, NOX \nCRIM.Exercise 11.38  Without using formulae, describe can calculate test\nstatistics hypothesis testing procedure single mean known\nvariance.Exercise 11.39  HR office cleaning company wants test significant difference salary males females. Call X = salary set 2000 male workers Y = salary set 150 female workers. previous survey know variances two groups equal. Write line R command run appropriate test hypothesis.Exercise 11.40  want test statistically hypothesis students UCSC\nRome better performances second year first year\nyear. Can say paired sample test?Exercise 11.41  Using dataset iris test significant difference\nmean Petal.Length mean Sepal.Width \nreport outcome value t-test.Exercise 11.42  Using dataset iris calculate correlation \nSepal.Length Sepal.Width.Exercise 11.43  Using dataset iris report highest correlation coefficient \nfind four variables.Exercise 11.44  Using dataset iris report highest correlation coefficient \nfind four variables.Exercise 11.45  Using dataset iris report variance Sepal.LengthExercise 11.46  Using dataset iris report third quartile Sepal.LengthExercise 11.47  reason adjusting R2 multiple regression?Exercise 11.48  correct definition variance inflation factor?Exercise 11.49  consequences collinearity among regressors?Exercise 11.50  Using dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.variables retained model? (retained means\ntratteresti)Exercise 11.51  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.value adjusted R squared best modelExercise 11.52  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.estimated coefficient variable duration best\nmodel?Exercise 11.53  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.estimated value intercept best model?Exercise 11.54  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.p-value variable duration best model?Exercise 11.55  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.value R square best modelExercise 11.56  Using dataset iris, test average variable\nSepal.Length changes significantly three Species considered.\nReport p-value appropiate test.look Species (already gone lecture)\ninspecting dataset. see Species three\ncategories setosa, versicolor virginica. like \ncompare means across 3 different categories canâ€™t use\nt.test() since 3. Instead use ANOVA aov().\nSintax similar linear models. saw trying \ntackle â€œlongâ€ format data vs.Â â€œwideâ€ format data.Answer Question 11.56:resulting 0.0000000000000002, significant. can conclude\n: ANOVA (formula: Sepal.Length ~ Species) suggests \nmain effect Species statistically significant large.Exercise 11.57  Using dataset iris, test average variable\nSepal.Length differs significantly three Species, Report \nvalue test statistic.","code":"perf_table = data.frame(\n  stringsAsFactors = FALSE,\n             month = c(\"january\",\"febraury\",\"march\",\n                       \"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\n                       \"october\",\"november\",\"december\"),\n            dept_1 = c(NA, NA, NA, 3L, 6L, 9L, 7L, 5L, 7L, 3L, 4L, 6L),\n            dept_2 = c(4L, 3L, 9L, 5L, 7L, 2L, 6L, 3L, 6L, 7L, 4L, 1L)\n)\n)stores = data.frame(\n     n_store = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L),\n      before = c(113L, 110L, 108L, 108L, 103L, 101L, 96L, 101L, 104L, 98L),\n       after = c(125L, 113L, 115L, 117L, 105L, 112L, 100L, 103L, 116L, 104L)\n)rnorm(n = 2000, mean = 0, sd = 0.1^(1/2))test_species = aov(Sepal.Length~Species, data = iris)\nsummary(test_species)"},{"path":"int-samp-q.html","id":"solutions-3","chapter":"11 ğŸ’» Intermediate Sample Questions","heading":"11.4 solutions","text":"Answer Question 11.28:required load data case since package already\n. just need type boston.c find .\nneed extract RMThere nunber packages makes able compute skewness,\n: e1071, moments, PerformanceAnalytics etc. \nsuggest use moments. dont installed execute:use teh function skewnes RM_varAnswer Question 11.29:probability type error, probability rejecting H0 H0\ntrueAnswer Question 11.30:1 minus probability type II error, probability accepting\nH0 H0 true.Answer Question 11.31:probability accepting H0 H0 true.Answer Question 11.32:H0: \\(\\mu_{dept1} = \\mu_{dept2}\\) H1: \\(\\mu_{dept1} \\neq \\mu_{dept2}\\)Remember always test alternative hypothesis H1. pvalue\nt test statistically significant reject H1 \nconversely accept H0, case means (\ndifferent â€™s randomness data, .e.Â sampling\nvariation).look p-value test see something like:\n0.4076, can conclude Two Sample t-test testing \ndifference dept_1 dept_2 (mean dept_1 = 5.56, mean \ndept_2 = 4.75) suggests effect positive, statistically \nsignificant, small. reject alt. hypo H1 accept H0.question tells can reject Null Hypo, \ncase since just accepted !Answer Question 11.33:exactly reasoning except \npaired t test. â€œtalking individuals? \nchecking individuals pre treatment?â€ YESLook p-value: p-value = 0.0004646.really small. \ncan conclude Paired t-test testing difference \n(mean difference = -6.80) suggests effect \nnegative, statistically significant, large.Answer Question 11.34:Answer Question 11.35:Answer Question 11.36:first define vectors X Y executing:answer may look something like: (Welch, remember \ncheck variance rely default R behavior applying \ntransformation t.test) Sample t-test testing difference X\nY (mean x = 4.33, mean y = 3.33) suggests effect \npositive, statistically significant, large given pvalue\n0.0697. However also significant alpha\nlevel significance 10%.Answer Question 11.37:Note principal diag matrix 1s. \nvariables perfect correlation . just\ninterested upper triangle. might also interested \nvisualizing corrplot. Install \ninstall.packages(\"corrplot\") pass matrix argument\ncorrplot(cor(new_boston))Answer Question 11.38:test statistic calculated seeing, example, many times \nabsolute difference sample mean population mean\n(sm-mu) embodies standard error = sqrt[(known variance)/n]. \nvalue allow us standardize distribution allocate value \nNormal distribution (variance known) T di Student\ndistribution (variance unknown) - looking value can now\ncalculate probability within range values\nestablished level confidence statistical test.Answer Question 11.39:Answer Question 11.40:FALSEAnswer Question 11.42:simple correlation aight?!Answer Question 11.43:Please note correlation cor() can computed \nnumeric values. Looking iris see variable species \nfactor (aka grouping variable) used ANOVA aov() \ninterested comparing means across 2 groups. \nresult need select variables Species \ncor().â€™s another way filtering stuff, just deselect\nSpecies :also might want visualize correlation (advanced trick):Answer Question 11.44:â€™s another way filtering stuff, just deselect\nSpecies :Answer Question 11.45:Answer Question 11.46:Answer Question 11.47:Adjusted R2 corrected goodness--fit (model accuracy) measure linear models. identifies percentage\nvariance target field explained input \ninputs. R2 tends optimistically estimate fit linear\nregression. always increases number effects included \nmodel. Adjusted R2 attempts correct overestimation.\nAdjusted R2 might decrease specific effect improve \nmodel. guessed account number parameters \nalso get points, precisely talking \ndegrees freedom.\\(R_{adj}^2 = 1- \\frac{(1-R^2)(n-1)}{n-k-1}\\)account number degrees freedom!Answer Question 11.48:may know Multicollinearity problem can run \nâ€™re fitting regression model, linear model. refers \npredictors correlated predictors model.\nUnfortunately, effects multicollinearity can feel murky \nintangible, makes unclear whether â€™s important fix.\nMulticollinearity results unstable parameter estimates makes \ndifficult assess effect independent variables \ndependent variables.Letâ€™s see another pov:Consider simplest case \\(Y\\) regressed \\(X\\) \\(Z\\)\n\\(Y = \\alpha + \\beta_1X +\\beta_2Z + \\epsilon\\) \\(Z\\) \n\\(Z\\) highly positively correlated. effect \\(X\\) \\(Y\\) \nhard distinguish effect \\(Z\\) \\(Y\\) increase\n\\(X\\) tends associated increase \\(Z\\). Now letâ€™s also\nconsider th pathological case \\(X = Z\\) highlights .\n\\(Y = \\alpha + \\beta_1X + \\beta_2Z + \\epsilon\\) ->\n\\(Y = \\alpha + (\\beta_1 + \\beta_2)X + 0Z + \\epsilon\\) two\nvariables indistinguishable.\\(\\frac{1}{1-R^2}\\)Answer Question 11.49:Estimators become unstableAnswer Question 11.50:first attempt:see age days significant, indeed\nduration . However days havign .13 p values much \nsignificant age accounts .38 Letâ€™s also check\ncollinearity uncorrectly specified model. look\ngood since values <10.mnay want see model, behaves cancelling age\nkeeping days, :iteration verify duration becomes even important\nsince now ***. However days just got worst. finally remove\n. donâ€™t check vif() already done \nexpect subset non collinear varibales () now become\ncollinear. result:Ã¹In end retain durationAnswer Question 11.51:resulting 0.02618Answer Question 11.52:resulting -0.09918208you can also directly look summaryAnswer Question 11.53:resulting 88.97380549you can also directly look summaryAnswer Question 11.54:resulting duration p-value: 0.00183Answer Question 11.55:result : 0.02913Answer Question 11.56:look Species (already gone lecture)\ninspecting dataset. see Species three\ncategories setosa, versicolor virginica. like \ncompare means across 3 different categories canâ€™t use\nt.test() since 3. Instead use ANOVA aov().\nSintax similar linear models. saw trying \ntackle â€œlongâ€ format data vs.Â â€œwideâ€ format data.resulting 0.0000000000000002, significant. can conclude\n: ANOVA (formula: Sepal.Length ~ Species) suggests \nmain effect Species statistically significant large.Answer Question 11.57:take exact test look statisticHere look F statistics: F = 119.3","code":"library(spdep)RM_var = boston.c$RMinstall.packages(\"moments\")\nlibrary(moments)skewness(RM_var)t.test(x = perf_table$dept_1, y = perf_table$dept_2, paired = F, alternative = \"two.sided\")t.test(x = stores$before, y = stores$after, paired = T, alternative = \"two.sided\")rnorm(n = 2000, mean = 0, sd = 0.1^(1/2))boxplot(X)X = c(5,5,3,3,5,5)\nY = c(4,4,3,3,3,3)\nt.test(X, Y, alternative = \"two.sided\", paired = F)library(spdep)\nlibrary(dplyr)\nnew_boston = select(boston.c, MEDV, NOX, CRIM)\ncor(new_boston)t.test(X, Y, paired = F, alternative =\"less\", var.equal = T)cor(x = iris$Sepal.Length, y = iris$Sepal.Width)iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\ncor(iris_filtr)iris_filtr2 = select(iris, -Species)\ncor(iris_filtr2)library(corrplot)\niris_filtr2 = select(iris, -Species)\ncorrplot::corrplot(cor(iris_filtr2))\n    iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\ncor(iris_filtr)iris_filtr2 = select(iris, -Species)\ncor(iris_filtr2)\n    var(iris$Sepal.Length)summary(iris$Sepal.Length)library(carData)\nlibrary(car)\ndata(\"Wong\")\nwong_regression = lm(piq ~ age + days + duration, data = Wong)\nsummary(wong_regression)wong_regression_2 = lm(piq ~ days + duration, data = Wong)wong_regression_3 = lm(piq ~ duration, data = Wong)summary(wong_regression_3)wong_regression_3$coefficients[2]summary(wong_regression_3)wong_regression_3$coefficients[1]summary(wong_regression_3)summary(wong_regression_3)summary(wong_regression_3)test_species = aov(Sepal.Length~Species, data = iris)\nsummary(test_species)summary(test_species)"},{"path":"anova.html","id":"anova","chapter":"12 ğŸ’» ANOVA","heading":"12 ğŸ’» ANOVA","text":"ANOVA (ANalysis VAriance) statistical test determine whether two population means different. words, used compare two groups see significantly different.practice, however, :Student t-test used compare 2 groups;ANOVA generalizes t-test beyond 2 groups, used compare 3 groups.Although ANOVA used make inference means different groups, method called â€œanalysis variance.â€ called like compares â€œâ€ variance (variance different groups) variance â€œwithinâ€ (variance within group). variance significantly larger within variance, group means declared different. Otherwise, conclude one way . two variances compared taking ratio:\\(\\frac{var()}{var(within)}\\)assumptions ANOVA, test tests exist assumption met:variable type: ANOVA requires mix one continuous quantitative dependent variable (corresponds measurements question relates) one qualitative independent variable (least 2 levels determine groups compare).Independence: data, collected representative randomly selected portion total population, independent groups within group.NormalityEquality variances: variances different groups equal populations (assumption called homogeneity variances, even sometimes referred homoscedasticity, opposed heteroscedasticity variances different across groups)Outliers: significant outliers different groups.bunch cases dealing , :Check observations independent.Sample sizes:\ncase small samples, test normality residuals:\nnormality assumed, test homogeneity variances:\nvariances equal, use ANOVA.\nvariances equal, use Welch ANOVA.\n\nnormality assumed, use Kruskal-Wallis test.\n\ncase small samples, test normality residuals:\nnormality assumed, test homogeneity variances:\nvariances equal, use ANOVA.\nvariances equal, use Welch ANOVA.\n\nnormality assumed, use Kruskal-Wallis test.\nnormality assumed, test homogeneity variances:\nvariances equal, use ANOVA.\nvariances equal, use Welch ANOVA.\nvariances equal, use ANOVA.variances equal, use Welch ANOVA.normality assumed, use Kruskal-Wallis test.case large samples normality assumed, test homogeneity variances:\nvariances equal, use ANOVA.\nvariances equal, use Welch ANOVA.\nvariances equal, use ANOVA.variances equal, use Welch ANOVA.","code":""},{"path":"anova.html","id":"aov-function","chapter":"12 ğŸ’» ANOVA","heading":"12.1 aov() function","text":"Data exercise penguins dataset (alternative well-known iris dataset), accessible via palmerpenguins package:dataset contains data 344 penguins 3 different species (Adelie, Chinstrap Gentoo). dataset contains 8 variables, focus flipper length .e.Â flipper_length_mm species .e.Â species, keep 2 variables.may want give quick check groups visual inspection:aov(). fuction take 2 arguments:formula: left side numeric variable, right side groups .e.Â var ~ groups (much like linear regression)data: dataPlease refer (tips) undestand data needs come longer format able use .","code":"\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n\npinguini = penguins[,c(\"species\", \"flipper_length_mm\")]\n boxplot(flipper_length_mm ~ species,\n  data = pinguini\n)\nanova_pinguini =aov(flipper_length_mm ~ species,\n  data = pinguini\n)\n\nanova_pinguini\n#> Call:\n#>    aov(formula = flipper_length_mm ~ species, data = pinguini)\n#> \n#> Terms:\n#>                  species Residuals\n#> Sum of Squares  52473.28  14953.26\n#> Deg. of Freedom        2       339\n#> \n#> Residual standard error: 6.641529\n#> Estimated effects may be unbalanced\n#> 2 observations deleted due to missingness"},{"path":"anova.html","id":"exercises-2","chapter":"12 ğŸ’» ANOVA","heading":"12.2 exercises","text":"Exercise 12.1  Letâ€™s assume sample data respective belonging group:perform anova test aov() function testing significant differences group 1, 2 3.specify formula, x continous variable y group variable.\nsolve .Exercise 12.2  Letâ€™s consider diet dataset link data set contains information 76 people undertook one three diets (referred diet , B C). background information age, gender, height. aim study see diet best losing weight.read data first dowload link, move data inside R project. run commands:using variable Diet, pre.weight weight6weeksread data kagglecompute mean weights groupcalculate anova Diet weight cutExercise 12.3  recruit 90 people participate experiment randomly assign 30 people follow either program , program B, program C one month.plot boxplot weight_loss ~ program Hint: use boxplot() function specifying formula.fit 1 way anova test difference weight loss program.Exercise 12.4  Consider maximum size 4 fish 3 populations (n=12). want use model help us examine question whether mean maximum fish size differs among populations.visualize boxplotUsing ANOVA model test whether group means differ another.Exercise 12.5  Letâ€™s consider 6 different insect sprays InsectSprays contained R. Letâ€™s assume interested testing difference number insects found field spraying, use varibales count spray.","code":"x<-c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16)\n\nz<-c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)diet = read.csv(\"< the dataset name>.csv\")#make this example reproducible\nset.seed(0)\n\n#create data frame\ndata <- data.frame(program = rep(c(\"A\", \"B\", \"C\"), each = 30),\n                   weight_loss = c(runif(30, 0, 3),\n                                   runif(30, 0, 5),\n                                   runif(30, 1, 7)))\n                                   size <- c(3,4,5,6,4,5,6,7,7,8,9,10)\npop <- c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\")"},{"path":"anova.html","id":"solutions-4","chapter":"12 ğŸ’» ANOVA","heading":"12.3 solutions","text":"Answer Exercise 12.1:ANOVA (formula: x ~ z) suggests main effect z statistically significant small\n(F(1, 13) = 0.05, p = 0.832; Eta2 = 3.57e-03, 95% CI [0.00, 1.00]). means group means different\none otherAnswer Exercise 12.2:first download data https://www.kaggle.com/code/evitaginiyatullina/one-way-anova-comparison/data, read data :compute mean weights groupAnswer Exercise 12.3:Plot boxplot weight_loss programPerform one-way ANOVA test differences weight loss programThe boxplot provide visual representation weight loss distribution program, ANOVA test statistically significant differences weight loss among programs.Answer Exercise 12.4:first need dataframe right?Visualize data boxplotPerform one-way ANOVA test differences among group meansSummary ANOVAAnswer Exercise 12.5:First, load dataset â€™s already loaded:Visualize data boxplot get initial understanding distribution counts type insect spray:Perform one-way ANOVA test differences number insects found among different insect sprays:boxplot provide visual representation distribution counts insect spray, ANOVA analysis help determine statistically significant differences means insect counts among different sprays.","code":"x <- c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16) \nz <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)\nanova <- aov( x ~ z ) \nsummary(anova)\n\n        Df Sum Sq Mean Sq F value Pr(>F)\nz            1    1.6    1.60   0.047  **0.832**\nResiduals   13  446.1   34.32diet <- read.csv(\"<your_dataset_name>.csv\")# Compute mean weights for each group\nmean_weights <- tapply(diet$weight6weeks, diet$Diet, mean)\n\n# Calculate one-way ANOVA on Diet against weight cut\nanova_result <- aov(weight6weeks ~ Diet, data = diet)\n\n# Summary of ANOVA\nsummary(anova_result)boxplot(weight_loss ~ program, data = data, xlab = \"Program\", ylab = \"Weight Loss\", main = \"Weight Loss by Program\")anova_result <- aov(weight_loss ~ program, data = data)\nsummary(anova_result)# Create a data frame\ndata <- data.frame(size, pop)boxplot(size ~ pop, data = data, xlab = \"Population\", ylab = \"Maximum Fish Size\", main = \"Boxplot of Maximum Fish Size by Population\")anova_result <- aov(size ~ pop, data = data)summary(anova_result)data(InsectSprays)boxplot(count ~ spray, data = InsectSprays, xlab = \"Insect Spray\", ylab = \"Number of Insects\",\n        main = \"Number of Insects by Insect Spray\")anova_result <- aov(count ~ spray, data = InsectSprays)\n\n# Summary of ANOVA\nsummary(anova_result)"},{"path":"dummy-vars.html","id":"dummy-vars","chapter":"13 ğŸ’» Dummy Variables","heading":"13 ğŸ’» Dummy Variables","text":"Categorical variables (also known factor qualitative variables) variables classify observations groups. limited number different values, called levels. example gender individuals categorical variable can take two levels: Male Female.Regression analysis requires numerical variables. , researcher wishes include categorical variable regression model, supplementary steps required make results interpretable.","code":""},{"path":"dummy-vars.html","id":"examples-data-from-car-pkg-salaries","chapter":"13 ğŸ’» Dummy Variables","heading":"13.1 examples data from car pkg Salaries","text":"â€™ll use Salaries data set car pkg, contains 2008/2009 nine-month academic salary Assistant Professors, Associate Professors Professors college U.S.","code":""},{"path":"dummy-vars.html","id":"categorical-variables-with-two-levels","chapter":"13 ğŸ’» Dummy Variables","heading":"13.2 Categorical variables with two levels","text":"Since now pretty much seen regression numeric predictors, times going deal categorical predictors like regression iris dataset.Recall , regression equation, predicting outcome variable \\(Y\\) basis predictor variable \\(X\\), can simply written \\(Y = \\alpha + \\beta_1*X\\).Suppose , wish investigate differences salaries males females.Based gender variable, can create new dummy variable takes value:1 person male0 person femaleand use variable predictor regression equation, leading following model:\\(\\alpha + \\beta_1\\) person male\\(\\alpha\\) person femaleThe coefficients can interpreted follow:\\(\\alpha\\) average salary among females,\\(\\alpha + \\beta_1\\) average salary among males,\\(\\beta_1\\) average difference salary males females.simple demonstration purpose, following example models salary difference males females computing simple linear regression model Salaries data set.\nR creates dummy variables automatically:output , average salary female estimated 101002, whereas males estimated total 101002 + 14088 = 115090.p-value dummy variable sexMale significant, suggesting statistical evidence difference average salary genders.happened taht R created sexMale dummy variable takes value 1 sex Male, 0 otherwise (Females). decision code males 1 females 0 (baseline) arbitrary, effect regression computation, alter interpretation coefficients.willign change factor orders may expect find negative coefficient SexFemale.","code":"\nmodel <- lm(salary ~ sex, data = Salaries)\nsummary(model)$coef\n#>              Estimate Std. Error   t value\n#> (Intercept) 101002.41   4809.386 21.001103\n#> sexMale      14088.01   5064.579  2.781674\n#>                                                                               Pr(>|t|)\n#> (Intercept) 0.000000000000000000000000000000000000000000000000000000000000000002683482\n#> sexMale     0.005667106519338906828187063524637778755277395248413085937500000000000000"},{"path":"dummy-vars.html","id":"categorical-variables-with-more-than-two-levels","chapter":"13 ğŸ’» Dummy Variables","heading":"13.3 Categorical variables with more than two levels","text":"Generally, categorical variable \\(n\\) levels transformed n-1 variables two levels. \\(n-1\\) new variables contain information single variable.Lets take example rank Salaries data three levels: AsstProf, AssocProf Prof. variable dummy coded two variables, one called AssocProf one Prof (.e.Â \\(n-1\\))\nsay:rank = AssocProf, column AssocProf coded 1 Prof 0.rank = Prof, column AssocProf coded 0 Prof coded 1.rank = AsstProf, columns AssocProf Prof coded 0.dummy coding automatically performed R. demonstration purpose, can use function model.matrix() create contrast matrix factor variable, look like (remember R handles ):building linear model, different ways encode categorical variables, known contrast coding systems. default option R use first level factor reference interpret remaining levels relative level.Now letâ€™s fit model see results:example, can seen discipline B (applied departments) significantly associated average increase 13473.38 salary compared discipline (theoretical departments).","code":"\nres <- model.matrix(~rank, data = Salaries)\nhead(res[, -1])\n#>   rankAssocProf rankProf\n#> 1             0        1\n#> 2             0        1\n#> 3             0        0\n#> 4             0        1\n#> 5             0        1\n#> 6             1        0\nlibrary(car)\nmodel2 <- lm(salary ~ yrs.service + rank + discipline + sex,\n             data = Salaries)\nsummary(model2)\n#> \n#> Call:\n#> lm(formula = salary ~ yrs.service + rank + discipline + sex, \n#>     data = Salaries)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -64202 -14255  -1533  10571  99163 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value\n#> (Intercept)   68351.67    4482.20  15.250\n#> yrs.service     -88.78     111.64  -0.795\n#> rankAssocProf 14560.40    4098.32   3.553\n#> rankProf      49159.64    3834.49  12.820\n#> disciplineB   13473.38    2315.50   5.819\n#> sexMale        4771.25    3878.00   1.230\n#>                           Pr(>|t|)    \n#> (Intercept)   < 0.0000000000000002 ***\n#> yrs.service               0.426958    \n#> rankAssocProf             0.000428 ***\n#> rankProf      < 0.0000000000000002 ***\n#> disciplineB           0.0000000124 ***\n#> sexMale                   0.219311    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 22650 on 391 degrees of freedom\n#> Multiple R-squared:  0.4478, Adjusted R-squared:  0.4407 \n#> F-statistic: 63.41 on 5 and 391 DF,  p-value: < 0.00000000000000022"},{"path":"dummy-vars.html","id":"oh-theres-another-way-to-say-this..-onehoteconding","chapter":"13 ğŸ’» Dummy Variables","heading":"13.4 oh thereâ€™s another way to say this.. OneHotEconding","text":"One-hot encoding process converting categorical variable multiple categories multiple variables, value 1 0. many packages even R handles .methods outlined , following simple dataframe required:Now starting dataframe can one hot encode .e.Â convert factors 0s 1s :","code":"\nset.seed(28)\n\ndata <- data.frame(\n  Outcome = seq(1,100,by=1),\n  Variable = sample(c(\"Red\",\"Green\",\"Blue\"), 100, replace = TRUE)\n)\nlibrary(caret)\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n\ndummy <- dummyVars(\" ~ .\", data=data)\nnewdata <- data.frame(predict(dummy, newdata = data)) "},{"path":"dummy-vars.html","id":"handling-factors-in-r","chapter":"13 ğŸ’» Dummy Variables","heading":"13.5 handling factors in R","text":"Factors used represent categorical data. Factors can ordered unordered important class statistical analysis plotting.Factors stored integers, labels associated unique integers. factors look (often behave) like character vectors, actually integers hood, need careful treating like strings.created, factors can contain pre-defined set values, known levels. default, R always sorts levels alphabetical order. instance, factor 2 levels:now 3","code":"\nsex <- factor(c(\"male\", \"female\", \"female\", \"male\"))\nlevels(sex)\n#> [1] \"female\" \"male\"\nnlevels(sex)\n#> [1] 2\nfood <- factor(c(\"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\", \"high\"))\nlevels(food)\n#> [1] \"high\"   \"low\"    \"medium\""},{"path":"dummy-vars.html","id":"exercises-3","chapter":"13 ğŸ’» Dummy Variables","heading":"13.6 exercises","text":"Exercise 13.1  Suppose dataset categorical variable named â€œcolorâ€ following levels: â€œRed,â€ â€œBlue,â€ â€œGreen,â€ â€œYellow.â€ Create set dummy variables categorical variable R, making sure use appropriate method encoding levels.Exercise 13.2  Using â€œSalariesâ€ dataset, perform linear regression analysis investigate effect â€œdisciplineâ€ (levels â€œâ€ â€œBâ€) salary. Interpret coefficients level â€œdiscipline.â€Exercise 13.3  Suppose dataset two categorical variables, â€œregionâ€ â€œlanguage,â€ want apply one-hot encoding variables. Provide R code perform one-hot encoding variables simultaneously.Exercise 13.4  dataset ordered factor variable â€œsizeâ€ representing â€œSmall,â€ â€œMedium,â€ â€œLargeâ€ sizes. can change order levels â€œLarge,â€ â€œMedium,â€ â€œSmallâ€? Provide R code modify factor levels.","code":"\n# Create a sample dataset\nyour_dataset <- data.frame(\n  color = c(\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Red\")\n)\n# Create a sample dataset\nyour_dataset <- data.frame(\n  region = c(\"North\", \"South\", \"East\", \"West\"),\n  language = c(\"English\", \"Spanish\", \"French\", \"German\")\n)\n# Create a sample dataset\nyour_dataset <- data.frame(\n  size = factor(c(\"Small\", \"Medium\", \"Large\", \"Small\", \"Large\"), ordered = TRUE)\n)"},{"path":"dummy-vars.html","id":"solutions-5","chapter":"13 ğŸ’» Dummy Variables","heading":"13.7 solutions","text":"Answer Question 13.1:can create dummy variables â€œcolorâ€ variable using model.matrix function follows:Answer Question 13.2:can perform linear regression analysis interpret coefficients â€œdisciplineâ€ levels follows:Answer Question 13.3:can apply one-hot encoding multiple categorical variables simultaneously using dummyVars function caret package follows:Answer Question 13.4:can change order levels ordered factor variable â€œsizeâ€ follows:code reorder levels â€œsizeâ€ factor variable specified.","code":"# Create dummy variables for color\ncolor_dummies <- model.matrix(~ color - 1, data = your_dataset)\ncolnames(color_dummies) <- levels(your_dataset$color)# Fit the linear regression model\nmodel4 <- lm(salary ~ discipline, data = Salaries)\nsummary(model4)\n\n# Interpretation of coefficients# Load the necessary packages\nlibrary(caret)\n\n# Apply one-hot encoding for \"region\" and \"language\"\ndummy <- dummyVars(\"~ region + language\", data = your_dataset)\nnewdata <- data.frame(predict(dummy, newdata = your_dataset))# Change the order of factor levels\nyour_dataset$size <- factor(your_dataset$size, levels = c(\"Large\", \"Medium\", \"Small\"), ordered = TRUE)"},{"path":"int-prep.html","id":"int-prep","chapter":"14 ğŸ’» Intermediate preparation","heading":"14 ğŸ’» Intermediate preparation","text":"Intermediate test topics include:\n- hypothesis tests means\n- anova\n- simple + multiple regression (adj-R2 + collinearity + dummy vars)\n- logistic regressionBelow find review core concepts bunch exercises way may resemble intermediate ones.","code":""},{"path":"int-prep.html","id":"core-concepts-of-t-test","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.1 ğŸª¨ core concepts of T test","text":"recall Arbiaâ€™s slide 69 focusing test mean steps calculating statistical tests generally :Develop null HypothesisSpecify level Significance \\(\\alpha\\)Collect sample data compute test statistic.pvalues:\n4. Use value test statistic compute p-value\n5. Reject \\(H_0\\) p-value $ < $Notice Z-test really used since almost always know variance \\(\\sigma^2\\) distribution. instead Standard Normal distribution use Studentâ€™s T distribution, t-tests. â€™s t test stat:\\[\nt = \\frac{(m - \\mu_0)}{S/\\sqrt{n}}\n\\]possible cases dealing :\nFigure 14.1: t.test vademecum\n","code":""},{"path":"int-prep.html","id":"t.test-function","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.2 t.test() function","text":"t.test() function may run one two sample t-tests data vectors. function several parameters invoked follows:case, x numeric vector data values, y optional. y included, function one-sample t-test data x; included, function runs two-sample t-test x y.mu (.e.Â \\(\\mu\\)) argument returns number showing real value mean (difference means two sample test used) null hypothesis. test conducts two-sided t-test default; however, may perform alternative hypothesis changing alternative argument \"greater\" \"less\", depending whether alternative hypothesis mean larger smaller mu. Consider following:â€¦performs one-sample t-test data contained x null hypothesis $ = 25$ alternative $ < 25 $paired argument indicate whether want paired t-test. default set FALSE can set TRUE desire perform paired t-test. paired test considering pre-post treatment test considering couples individuals.two-sample t-test, var.equal option specifies whether assume equal variances. default assumption unequal variance Welsh approximation degrees freedom; however, may change TRUE pool variance.Finally, conf.level parameter specifies degree confidence reported confidence interval \\(\\mu\\) one-sample \\(\\mu_1 - \\mu_2\\) two-sample cases. simple example:use lm() function R linear modeling, can create Markdown document explain usage various arguments. example create conceptual guide RMarkdown format:##Â Linear Modeling lm() R","code":"\nt.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, \n        paired = FALSE, var.equal = FALSE, conf.level = 0.95)\nt.test(x, alternative = \"less\", mu = 25)"},{"path":"int-prep.html","id":"introduction-1","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.3 Introduction","text":"lm() function R used performing linear regression modeling. Linear regression statistical method models relationship dependent variable one independent variables. document, explore use lm() function various arguments linear modeling.","code":""},{"path":"int-prep.html","id":"basic-usage","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.4 Basic Usage","text":"perform linear modeling lm(), need specify formula linear regression model. basic syntax follows:dependent_variable: variable want predict.independent_variable: variable(s) used make prediction.data: data frame containing variables.","code":"\nmodel <- lm(dependent_variable ~ independent_variable, data = your_data)"},{"path":"int-prep.html","id":"arguments","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5 Arguments","text":"","code":""},{"path":"int-prep.html","id":"formula","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.1 Formula","text":"formula core lm() function specifies relationship dependent independent variables. follows Y ~ X pattern, Y dependent variable, X independent variable. can include multiple independent variables interactions using + *. example:","code":"\nmodel <- lm(y ~ x1 + x2 + x1*x2, data = your_data)"},{"path":"int-prep.html","id":"data","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.2 Data","text":"data argument point data frame variables located. argument helps R locate variables specified formula.","code":""},{"path":"int-prep.html","id":"subset","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.3 Subset","text":"can use subset argument specify subset data modeling. useful want focus specific portion dataset.","code":"\nmodel <- lm(y ~ x, data = your_data, subset = condition)"},{"path":"int-prep.html","id":"weight","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.4 Weight","text":"weight argument allows assign different weights data point. can useful want give importance certain observations.","code":"\nmodel <- lm(y ~ x, data = your_data, weights = weight_variable)"},{"path":"int-prep.html","id":"na.action","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.5 Na.action","text":"na.action argument controls missing values treated. default, na.action = na.fail, means model fail missing values data. can also use na.omit automatically remove rows missing values.","code":"\nmodel <- lm(y ~ x, data = your_data, na.action = na.omit)"},{"path":"int-prep.html","id":"other-arguments","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.5.6 Other Arguments","text":"additional arguments controlling aspects modeling process, offset, method, control. can refer R documentation complete list available arguments descriptions.","code":""},{"path":"int-prep.html","id":"vif-variance-inflation-factor","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.6 VIF (Variance Inflation Factor)","text":"Variance Inflation Factor (VIF) measure used detect multicollinearity linear regression model. Multicollinearity occurs independent variables model highly correlated, making challenging determine individual effect variable dependent variable. VIF function regclass package R can help us identify multicollinearity linear regression model.","code":""},{"path":"int-prep.html","id":"calculating-vif","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.6.1 Calculating VIF","text":"calculate VIF independent variables model, can use VIF() function. First, make sure regclass package installed loaded:Next, can calculate VIF linear regression model follows:VIF() function takes linear regression model argument returns data frame VIF values independent variable. Higher VIF values indicate stronger multicollinearity, typically threshold 5 10 rule thumb.","code":"\ninstall.packages(\"regclass\")\nlibrary(regclass)\n# Fit a linear regression model\nmodel <- lm(formula  = y ~ x1 + x2 + x3, data = your_data)\n\n# Calculate VIF\nvif_results <- VIF(model)"},{"path":"int-prep.html","id":"interpreting-vif","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.6.2 Interpreting VIF","text":"VIF close 1, suggests variable highly correlated independent variables, indicating significant multicollinearity.VIF close 1, suggests variable highly correlated independent variables, indicating significant multicollinearity.VIF greater 1 less chosen threshold (e.g., 5 10), suggests correlation necessarily problematic multicollinearity.VIF greater 1 less chosen threshold (e.g., 5 10), suggests correlation necessarily problematic multicollinearity.VIF significantly greater chosen threshold (e.g., 10), indicates high degree multicollinearity, may need consider removing combining variables address issue.VIF significantly greater chosen threshold (e.g., 10), indicates high degree multicollinearity, may need consider removing combining variables address issue.","code":""},{"path":"int-prep.html","id":"addressing-multicollinearity","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.6.3 Addressing Multicollinearity","text":"detect problematic multicollinearity high VIF values, can take several steps address issue:Remove one correlated variables: two variables highly correlated, removing one can often resolve multicollinearity issue.Remove one correlated variables: two variables highly correlated, removing one can often resolve multicollinearity issue.Combine correlated variables: can create new variables combinations highly correlated variables, reducing multicollinearity.Combine correlated variables: can create new variables combinations highly correlated variables, reducing multicollinearity.Collect data: Sometimes, multicollinearity can alleviated collecting data, especially sample size small.Collect data: Sometimes, multicollinearity can alleviated collecting data, especially sample size small.Regularization techniques: Consider using regularization techniques like Ridge Lasso regression, can handle multicollinearity adding penalties coefficients correlated variables.Regularization techniques: Consider using regularization techniques like Ridge Lasso regression, can handle multicollinearity adding penalties coefficients correlated variables.VIF analysis crucial step assessing quality linear regression model ensuring independence independent variables.","code":""},{"path":"int-prep.html","id":"anova-analysis-of-variance","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.7 ANOVA (Analysis of Variance)","text":"ANOVA, Analysis Variance, statistical technique used analyze differences among group means dataset. particularly useful want compare means two groups. aov() function R commonly used perform ANOVA.","code":""},{"path":"int-prep.html","id":"performing-anova-with-aov","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.7.1 Performing ANOVA with aov()","text":"perform ANOVA using aov() function, need specify formula describes relationship dependent variable grouping factor (categorical variable). basic syntax follows:dependent_variable: continuous variable want analyze.grouping_factor: categorical variable defines groups.data: data frame containing variables.","code":"\nanova_model <- aov(dependent_variable ~ grouping_factor, data = your_data)"},{"path":"int-prep.html","id":"anova-tables","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.7.2 ANOVA Tables","text":"â€™ve created ANOVA model, can obtain ANOVA table using summary() function applied aov object:table provides various statistics, including sum squares, degrees freedom, F-statistic, p-value, allow assess significance differences among group means.","code":"\nsummary(anova_model)"},{"path":"int-prep.html","id":"interpreting-anova","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.7.3 Interpreting ANOVA","text":"F-statistic ANOVA table tests whether significant differences among group means. small p-value (< 0.05) suggests significant differences.F-statistic ANOVA table tests whether significant differences among group means. small p-value (< 0.05) suggests significant differences.ANOVA statistically significant, indicates significant differences among group means.ANOVA statistically significant, indicates significant differences among group means.","code":""},{"path":"int-prep.html","id":"assumptions-of-anova","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.7.4 Assumptions of ANOVA","text":"ANOVA assumes variances groups equal (homogeneity variances) data normally distributed. Violations assumptions may lead inaccurate results. can check homogeneity variances using tests like Leveneâ€™s test Bartlettâ€™s test assess normality data using normal probability plots statistical tests like Shapiro-Wilk test.","code":""},{"path":"int-prep.html","id":"logistic-regression-with-glm","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8 Logistic Regression with glm()","text":"Logistic regression statistical technique used modeling relationship binary dependent variable (0/1, Yes/, True/False) one independent variables. glm() function R commonly used perform logistic regression.","code":""},{"path":"int-prep.html","id":"performing-logistic-regression-with-glm","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8.1 Performing Logistic Regression with glm()","text":"perform logistic regression using glm() function, need specify formula describes relationship binary dependent variable independent variables. basic syntax follows:dependent_variable: binary dependent variable want model.independent_variable1, independent_variable2, etc.: independent variables influence probability binary outcome.family: Specify family argument binomial indicate logistic regression.data: data frame containing variables.","code":"\nlogistic_model <- glm(formula = dependent_variable ~ independent_variable1 + independent_variable2, family = \"binomial\", data = your_data)"},{"path":"int-prep.html","id":"model-summary","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8.2 Model Summary","text":"creating logistic regression model, can obtain summary modelâ€™s coefficients, standard errors, z-values, p-values using summary() function applied glm object:summary provides valuable information influence independent variables log-odds binary outcome.","code":"\nsummary(logistic_model)"},{"path":"int-prep.html","id":"interpreting-logistic-regression","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8.3 Interpreting Logistic Regression","text":"coefficients summary indicate direction strength relationship independent variables log-odds binary outcome.coefficients summary indicate direction strength relationship independent variables log-odds binary outcome.Positive coefficients suggest increase log-odds, negative coefficients suggest decrease.Positive coefficients suggest increase log-odds, negative coefficients suggest decrease.odds ratio (exp(coef)) can used interpret change odds binary outcome one-unit change independent variable.odds ratio (exp(coef)) can used interpret change odds binary outcome one-unit change independent variable.significant p-value (< 0.05) coefficient suggests independent variable significant effect binary outcome.significant p-value (< 0.05) coefficient suggests independent variable significant effect binary outcome.null hypothesis logistic regression relationship independent variable binary outcome.null hypothesis logistic regression relationship independent variable binary outcome.","code":""},{"path":"int-prep.html","id":"model-evaluation","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8.4 Model Evaluation","text":"evaluate performance logistic regression model, can assess accuracy, sensitivity, specificity, metrics using techniques like cross-validation ROC analysis. can also plot ROC curve calculate AUC (Area Curve) assess modelâ€™s predictive power.","code":""},{"path":"int-prep.html","id":"assumptions-of-logistic-regression","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.8.5 Assumptions of Logistic Regression","text":"Logistic regression assumes log-odds binary outcome linear combination independent variables. important check violations assumption, can done residual analysis.","code":""},{"path":"int-prep.html","id":"poisson-regression-with-glm","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.9 Poisson Regression with glm()","text":"Poisson regression statistical technique used model relationship count-dependent variable (typically non-negative integers) one independent variables. glm() function R commonly used perform Poisson regression.","code":""},{"path":"int-prep.html","id":"performing-poisson-regression-with-glm","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.9.1 Performing Poisson Regression with glm()","text":"perform Poisson regression using glm() function, need specify formula describes relationship count-dependent variable independent variables. basic syntax follows:count_dependent_variable: count-dependent variable want model.independent_variable1, independent_variable2, etc.: independent variables influence count-dependent variable.family: Specify family argument poisson indicate Poisson regression.data: data frame containing variables.","code":"\npoisson_model <- glm(formula = count_dependent_variable ~ independent_variable1 + independent_variable2, family = \"poisson\", data = your_data)"},{"path":"int-prep.html","id":"model-summary-1","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.9.2 Model Summary","text":"creating Poisson regression model, can obtain summary modelâ€™s coefficients, standard errors, z-values, p-values using summary() function applied glm object:summary provides information influence independent variables expected count dependent variable.","code":"\nsummary(poisson_model)"},{"path":"int-prep.html","id":"interpreting-poisson-regression","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.9.3 Interpreting Poisson Regression","text":"coefficients summary indicate direction strength relationship independent variables expected count dependent variable.coefficients summary indicate direction strength relationship independent variables expected count dependent variable.Positive coefficients suggest increase expected count, negative coefficients suggest decrease.Positive coefficients suggest increase expected count, negative coefficients suggest decrease.exponential coefficients (exp(coef)) can used interpret multiplicative effect one-unit change independent variable expected count.exponential coefficients (exp(coef)) can used interpret multiplicative effect one-unit change independent variable expected count.significant p-value (< 0.05) coefficient suggests independent variable significant effect expected count.significant p-value (< 0.05) coefficient suggests independent variable significant effect expected count.null hypothesis Poisson regression relationship independent variable expected count.null hypothesis Poisson regression relationship independent variable expected count.","code":""},{"path":"int-prep.html","id":"class-exercises-do-it-in-groups","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.10 class exercises, do it in groups ğŸ‘¯","text":"first guided,Exercise 14.1  state Highway Patrol periodically samples vehicle various location \nparticular roadway. sample vehicle speed used test hypothesis H0\nmean less equal 65The locations \\(H_0\\) rejected deemed best locations radar traps.\nlocation F, sample 64 vehicles shows mean speed 66.2 mph std\ndev 4.2 mph. Use \\(\\alpha = 0.05\\) test hypothesis.Exercise 14.2  Letâ€™s assume dataset midwest ggplot2 package: contains demographic information midwest counties 2000 US census.\nBesides variables interested percollege describes Percent college educated midwest.test midwest average less national average (.e.Â *35%) p-value < .02.Exercise 14.3  Download datafile â€˜prawnGR.CSVâ€™ Data link save data directory (Remember R projects working directory). Import data R assign variable appropriate name. data collected experiment investigate difference growth rate giant tiger prawn (Penaeus monodon) fed either artificial natural diet.quick look structure dataset.plot growth rate versus diet using appropriate plot.many observations diet treatment?want compare difference growth rate two diets using two sample t-test.Conduct two sample t-test using t.test() using argument var.equal = TRUE perform t-test assuming equal variances. null hypothesis want test? reject fail reject null hypothesis? value t statistic, degrees freedom p value? summarise summary statistics report?Exercise 14.4  new coach hired athletics school, effectiveness new type training evaluated comparing average times 10 centimeters. times seconds athleteâ€™s competition repeated.Ã¹We two groups trained competitors, measurements taken athletes competition. determine improvement, deterioration, time averages remained essentially constant (.e., H0). Conduct test t student paired changes difference significant 95% confidence level?Exercise 14.5  Following exercise 4 Assume club management, based statistics, fires coach improved hires another promising coach. Following second training session, record athletesâ€™ times:Exercise 14.6  Letâ€™s assume genderweight datarium package, containing weight 40 individuals (20 women 20 men).mean weights male females?test statistically significant 95% confidence levelExercise 12.1  Letâ€™s assume sample data respective belonging group:perform anova test aov() function testing significant differences group 1, 2 3.specify formula, x continous variable y group variable.\nsolve .Exercise 12.2  Assume dataset named PlantGrowth variables weight (dependent variable) group (categorical independent variable).Perform ANOVA analysis compare means weight among different group levels.Check p-value determine whether significant differences among group means.Exercise 12.3  recruit 90 people participate experiment randomly assign 30 people follow either program , program B, program C one month.plot boxplot weight_loss ~ program Hint: use boxplot() function specifying formula.fit 1 way anova test difference weight loss program.Exercise 12.4  Consider maximum size 4 fish 3 populations (n=12). want use model help us examine question whether mean maximum fish size differs among populations.visualize boxplotUsing ANOVA model test whether group means differ another.Exercise 12.5  Letâ€™s consider 6 different insect sprays InsectSprays contained R. Letâ€™s assume interested testing difference number insects found field spraying, use varibales count spray.Exercise 14.7  Letâ€™s consider diet dataset link data set contains information 76 people undertook one three diets (referred diet , B C). background information age, gender, height. aim study see diet best losing weight.read data first dowload link, move data inside R project. run commands:using variable Diet, pre.weight weight6weeksread data kagglecompute mean weights groupcalculate anova Diet weight cutExercise 14.8  Use built-mtcars dataset variables mpg (miles per gallon) vs (engine type: 0 = V-shaped, 1 = straight).Fit logistic regression model predict vs (engine type) based mpg.Interpret coefficients logistic regression model.Exercise 14.9  Use built-mtcars dataset variables mpg (miles per gallon) gear (number forward gears).Fit Poisson regression model predict gear based mpg.Interpret coefficients Poisson regression model.","code":"before_training: =  c(12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3)\nafter_training = c( 12.7, 13.6, 12.0, 15.2, 16.8, 20.0, 12.0, 15.9, 16.0, 11.1)before training: 12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3\nafter training: 12.0, 12.2, 11.2, 13.0, 15.0, 15.8, 12.2, 13.4, 12.9, 11.0x<-c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16)\n\nz<-c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)#make this example reproducible\nset.seed(0)\n\n#create data frame\ndata <- data.frame(program = rep(c(\"A\", \"B\", \"C\"), each = 30),\n                   weight_loss = c(runif(30, 0, 3),\n                                   runif(30, 0, 5),\n                                   runif(30, 1, 7)))\n                                   size <- c(3,4,5,6,4,5,6,7,7,8,9,10)\npop <- c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\")diet = read.csv(\"< the dataset name>.csv\")"},{"path":"int-prep.html","id":"solutions-6","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.11 solutions","text":"Answer Exercise 14.1:Sample random data Normal distribution given mean sd.\ndefine H0 H1. end run hte test.One Sample t-test testing difference x (mean = 65.85)\nmu = 65 suggests effect positive, statistically \nsignificant, small (difference = 0.85, 95% CI [-Inf, 66.81],\nt(63) = 1.47, p = 0.927Answer Exercise 12.1:ANOVA (formula: x ~ z) suggests main effect z statistically significant small\n(F(1, 13) = 0.05, p = 0.832; Eta2 = 3.57e-03, 95% CI [0.00, 1.00]). means group means different\none otherAnswer Exercise 14.7:Assuming â€˜plant_growthâ€™ datasetAnswer Exercise 14.8:Convert vs factor variableFit logistic regression modelInterpret coefficientsAnswer Exercise 14.9:Fit Poisson regression modelInterpret coefficients","code":"x <- rnorm(n = 64, mean = 66.2, sd = 4.2)\ntest<-t.test(x, mu = 65, alternative = \"less\")\n\n\nt = 1.469, df = 63, p-value = 0.9266\nalternative hypothesis: true mean is less than 65\n95 percent confidence interval:\n -Inf 66.80805\nsample estimates:\nmean of x \n65.84629 x <- c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16) \nz <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)\nanova <- aov( x ~ z ) \nsummary(anova)\n\n        Df Sum Sq Mean Sq F value Pr(>F)\nz            1    1.6    1.60   0.047  **0.832**\nResiduals   13  446.1   34.32# 1. Perform ANOVA\nanova_model <- aov(weight ~ group, data = plant_growth)\n\n# 2. Check p-value\nsummary(anova_model)data(mtcars)mtcars$vs <- as.factor(mtcars$vs)logistic_model <- glm(vs ~ mpg, family = binomial, data = mtcars)summary(logistic_model)data(mtcars)poisson_model <- glm(gear ~ mpg, family = poisson, data = mtcars)summary(poisson_model)"},{"path":"int-prep.html","id":"tips","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.12 ğŸ¬ tips and tricks","text":"Yout might interested standardizing/fornalize say things statistical jargon, report .\nsimply pass test, wether ANOVA t.test object inside report report(). rememer function summary() using linear regression? exactly , ANOVA t tests.data longer fromat different syntax specify t test pretty much follows one linear models .e.Â lm().\nletâ€™s look .may something like:exaclty dataset arranged different format. used wider format iut might happen bump longer one. ? â€™sa trick , letâ€™s say want test mean statistically different 95% confidence level, instead supplying x y t.test() follow pretty much syntax linear models lm():conclude? conclude : Effect sizes labelled following Cohenâ€™s (1988) recommendations.Welch Two Sample t-test testing difference var group (mean group = 31.00, mean group b = 52.50) suggests effect negative, statistically significant, medium (difference = -21.50, 95% CI [-78.99, 35.99], t(5.92) = -0.92, p = 0.394; Cohenâ€™s d = -0.75, 95% CI [-2.39, 0.94])","code":"\n# install.packages(\"remotes\")\n# remotes::install_github(\"easystats/report\")\nlibrary(report)\n\nx <- rnorm(n = 64, mean = 66.2, sd = 4.2)\ntest<-t.test(x, mu = 65, alternative = \"less\")\nreport(test)\n#> Effect sizes were labelled following Cohen's (1988)\n#> recommendations.\n#> \n#> The One Sample t-test testing the difference between x\n#> (mean = 66.99) and mu = 65 suggests that the effect is\n#> positive, statistically not significant, and small\n#> (difference = 1.99, 95% CI [-Inf, 67.86], t(63) = 3.83, p >\n#> .999; Cohen's d = 0.48, 95% CI [-Inf, 0.69])\nlibrary(tibble)\nlonger = tribble( \n  ~group, ~var,\n  \"a\",   10,\n  \"b\",   24, \n  \"a\",   31,\n  \"a\",   75,\n  \"b\",   26,\n  \"a\",   8,\n  \"b\",   98,\n  \"b\",   62,\n  )\nwider = tribble( \n  ~group_a, ~group_b,\n   10,       24,\n   31,       26,\n   75,       98,\n   8,        62\n  )\n\ntest_for_wider_format = t.test(var~group, data =  longer)\ntest_for_wider_format\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  var by group\n#> t = -0.91809, df = 5.9192, p-value = 0.3944\n#> alternative hypothesis: true difference in means between group a and group b is not equal to 0\n#> 95 percent confidence interval:\n#>  -78.99271  35.99271\n#> sample estimates:\n#> mean in group a mean in group b \n#>            31.0            52.5"},{"path":"int-prep.html","id":"further-exercies","chapter":"14 ğŸ’» Intermediate preparation","heading":"14.13 further exercies ğŸ‹ï¸","text":"Exercise 14.10  test scores students intervention. can assess intervention statistically significant effect scores? Specify type test use assumptions involved.Exercise 14.11  Using dataset mtcars, write R command calculate mean standard deviation disp variable grouped cyl.(#exr:mean_diff1) Explain assess mean difference two groups statistically significant, without running R code.Exercise 14.12  Use Boston dataset MASS create histogram variable crim. Report R code used.Exercise 14.13  Describe meaning p-value context hypothesis testing.(#exr:temp_diff1) Given dataset daily temperatures recorded two cities one year, write R code perform hypothesis test determine significant difference mean temperature cities. Specify test used.Exercise 14.14  Given X = c(10, 12, 15, 20) Y = c(11, 14, 16, 18), calculate Pearson correlation coefficient X Y using R.Exercise 14.15  Explain concept Type Type II errors hypothesis testing.Exercise 14.16  Using PlantGrowth dataset R, calculate mean weight level group plot boxplot weight grouped group.Exercise 14.1  Run one-sample t-test test mean hp mtcars different 120. Write R code.Exercise 14.17  Using dataset iris, write R code test significant difference average Sepal.Length setosa versicolor species.Exercise 14.18  Write R function simulate 500 observations Poisson distribution lambda 3 plot histogram.Exercise 14.19  Describe check multicollinearity multiple regression model R.Exercise 14.20  Using airquality dataset, calculate correlation matrix Ozone, Solar.R, Wind, Temp.Exercise 14.21  Perform paired t-test using variables = c(5, 7, 8, 6, 10) = c(6, 8, 9, 7, 12). Report p-value.Exercise 14.8  Using dataset mtcars, perform linear regression mpg dependent variable hp wt independent variables. Report adjusted R-squared.Exercise 14.22  Write R code create density plot variable Sepal.Length species iris dataset.Exercise 12.1  Using iris, perform one-way ANOVA test mean Sepal.Length differs across three species.Exercise 14.23  Define term â€œconfidence intervalâ€ context statistical estimation.Exercise 14.24  Using mtcars dataset, refit multiple linear regression model mpg dependent variable hp, wt, drat independent variables. Use stepwise regression iteratively remove insignificant predictors. Report final model significant coefficients.Exercise 12.2  Using dataset ToothGrowth, perform one-way ANOVA function aov() test mean tooth length differs across supplement types doses.Exercise 12.3  recruit 90 people participate experiment randomly assign 30 people follow either program , program B, program C one month.plot boxplot weight_loss ~ program Hint: use boxplot() function specifying formula.fit 1 way anova test difference weight loss program.Answer Exercise 14.10:appropriate test use paired t-test. test used two related samples, measurements individuals, want determine statistically significant difference means. assumptions include differences normally distributed data paired.Answer Exercise 14.11:dplyr package used calculate mean standard deviation disp variable grouped cyl:Answer Exercise @ref(exr:mean_diff1):assess mean difference two groups statistically significant, can use hypothesis test independent t-test. involves setting null alternative hypotheses, calculating test statistic, comparing critical value using p-value determine significance, typically using significance level (e.g., 0.05).Answer Exercise 14.12:create histogram crim variable Boston dataset, use following code:Answer Exercise 14.13:p-value probability obtaining test results least extreme observed results, assumption null hypothesis true. smaller p-value indicates stronger evidence null hypothesis, p-value chosen significance level (e.g., 0.05), reject null hypothesis.Answer Exercise @ref(exr:temp_diff1):appropriate test use two-sample t-test, comparing means two independent groups. R code follows:Answer Exercise 14.14:calculate Pearson correlation coefficient vectors X Y:Answer Exercise 14.15:Type error occurs reject true null hypothesis (false positive), Type II error occurs fail reject false null hypothesis (false negative).Answer Exercise 14.16:calculate mean weight level group plot boxplot weight grouped group:Answer Exercise 14.1:run one-sample t-test test mean hp mtcars different 120:Answer Exercise 14.17:test significant difference average Sepal.Length setosa versicolor species:Answer Exercise 14.18:simulate 500 observations Poisson distribution lambda 3 plot histogram:Answer Exercise 14.19:check multicollinearity, can use Variance Inflation Factor (VIF). VIF value greater 10 indicates high multicollinearity:Answer Exercise 14.20:calculate correlation matrix Ozone, Solar.R, Wind, Temp airquality dataset:Answer Exercise 14.21:perform paired t-test using variables:Answer Exercise 14.8:perform linear regression mpg dependent variable hp wt independent variables, report adjusted R-squared:Answer Exercise 14.22:create density plot variable Sepal.Length species iris dataset:Answer Exercise 12.1:perform one-way ANOVA test mean Sepal.Length differs across three species:Answer Exercise 14.23:confidence interval range values, derived sample statistics, likely contain population parameter specified level confidence (e.g., 95%). provides estimated range expected include true parameter value.Answer Exercise 14.24:refit multiple linear regression model mpg dependent variable hp, wt, drat independent variables, using stepwise regression:Answer Exercise 12.2:perform one-way ANOVA using ToothGrowth dataset test mean tooth length differs across supplement types doses:Answer Exercise 12.3:plot boxplot weight_loss program fit one-way ANOVA test difference weight loss program:","code":"#make this example reproducible\nset.seed(0)\n\n#create data frame\ndata <- data.frame(program = rep(c(\"A\", \"B\", \"C\"), each = 30),\n                   weight_loss = c(runif(30, 0, 3),\n                                   runif(30, 0, 5),\n                                   runif(30, 1, 7)))library(dplyr)\nmtcars %>% group_by(cyl) %>% summarise(mean_disp = mean(disp), sd_disp = sd(disp))library(MASS)\ndata(Boston)\nhist(Boston$crim, main = \"Histogram of crim\", xlab = \"Crime rate per capita\")t.test(temp_city1, temp_city2, var.equal = TRUE)X <- c(10, 12, 15, 20)\nY <- c(11, 14, 16, 18)\ncor(X, Y)data(PlantGrowth)\naggregate(weight ~ group, data = PlantGrowth, mean)\nboxplot(weight ~ group, data = PlantGrowth, main = \"Boxplot of Weight by Group\")t.test(mtcars$hp, mu = 120)t.test(Sepal.Length ~ Species, data = subset(iris, Species %in% c(\"setosa\", \"versicolor\")))set.seed(0)\npoisson_data <- rpois(500, lambda = 3)\nhist(poisson_data, main = \"Histogram of Poisson Distribution\", xlab = \"Values\")library(car)\nvif(model)airquality_subset <- airquality[, c(\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\")]\ncor(airquality_subset, use = \"complete.obs\")before <- c(5, 7, 8, 6, 10)\nafter <- c(6, 8, 9, 7, 12)\nt.test(before, after, paired = TRUE)model <- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model)$adj.r.squaredlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Sepal.Length by Species\")aov_model <- aov(Sepal.Length ~ Species, data = iris)\nsummary(aov_model)library(MASS)\ninitial_model <- lm(mpg ~ hp + wt + drat, data = mtcars)\nstepwise_model <- stepAIC(initial_model, direction = \"both\")\nsummary(stepwise_model)data(ToothGrowth)\naov_model <- aov(len ~ supp * dose, data = ToothGrowth)\nsummary(aov_model)# Plot boxplot\nboxplot(weight_loss ~ program, data = data, main = \"Boxplot of Weight Loss by Program\", xlab = \"Program\", ylab = \"Weight Loss\")\n\n# Fit one-way ANOVA\naov_model <- aov(weight_loss ~ program, data = data)\nsummary(aov_model)"},{"path":"log-reg.html","id":"log-reg","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15 ğŸ’» Binary Logistic Regression","text":"","code":""},{"path":"log-reg.html","id":"objectives-1","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.1 Objectives","text":"end chapter, readers beto understand concept simple multiple binary logistic regression 15to perform simple binary logistic regression 15to perform multiple binary logistic regression 15to perform model assessment binary logistic regressionto present interpret results binary logsitic regression","code":""},{"path":"log-reg.html","id":"background-1","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.2 Background","text":"logistic model (logit model) used model probability certain class event existing pass fail, win lose, alive dead healthy sick. specifically, binary logistic regression used model relationship covariate set covariates outcome variables binary variable.binary variable categorical outcome two categories levels. medical health research, binary outcome variable common. example outcome binary include:survival status status cancer patients end treatment coded either alive deadrelapse status status patient coded either relapse relapsesatisfaction level patients come clinics asked satisfied satisfied serviceglucose control patients categorized either good control poor control based Hba1cIn binary logistic regression model, dependent variable two levels (categorical). outcome variable two levels categories, analysis modeled multinomial logistic regression , multiple categories ordered, ordinal logistic regression (example proportional odds ordinal logistic model).","code":""},{"path":"log-reg.html","id":"further-readings","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.3 Further readings","text":"number good references help readers understand binary logistic regression better. references list also contains workflow useful readers modelling logistic regression.highly recommend readers readthe Applied Logistic Regression book5the Logistic Regression: Self Learning Text6the workflow Handbook Statistical Analyses Using R7","code":""},{"path":"log-reg.html","id":"dataset-1","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.4 Dataset","text":"use dataset named stroke.dta STATA format. data come study hospitalized stroke patients. original dataset contain 12 variables main variables interest :status : Status patient hospitalization (alive dead)gcs : Glasgow Coma Scale admission (range 3 15)stroke_type : (Ischaemic Stroke) HS (Haemorrhagic Stroke)sex : female maledm : History Diabetes (yes )sbp : Systolic Blood Pressure (mmHg)age : age patient admissionThe outcome variable variable status. labelled either dead alive outcome patient hospitalization.","code":""},{"path":"log-reg.html","id":"logit-and-logistic-models","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.5 logit and logistic models","text":"simple binary logit logistic models refer model one covariate (also known independent variable). example, covariate gcs (Glasgow Coma Scale), simple logit model written :\\[\\hat{g}(x)= ln\\left[ \\frac{\\hat\\pi(x)}{1 - {\\hat\\pi(x)}} \\right]\\]\\(\\hat{g}(x)\\) log odds death given value gcs. odds death given value GCS written \\(= \\hat\\beta_0 + \\hat\\beta_1(gcs)\\)simple logistic model also written :\\[\\hat{\\pi}(x) = \\frac{exp^{\\hat{\\beta}_{0} + \\hat{\\beta}_{1}{gcs}}}{1 + exp^{\\hat{\\beta}_{0} + \\hat{\\beta}_{1}{gcs}}}\\]\n\\(\\pi(x) = E(Y|x)\\) represents conditional mean \\(Y\\) given \\(x\\) logistic distribution used. also simply known predicted probability death given value gcs.decided (based clinical expertise literature review) model explain death consists gcs, stroke type, sex, dm, age sbp, logit model can expanded :\\[\\hat{g}(x)  = \\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) +  \\hat\\beta_6(age)\\]\nodds death given certain value gcs, sbp age certain categories stroke stype, sex diabetes. probability deaths \\[\\hat{\\pi}(x) = \\frac{exp^{\\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) + \\hat\\beta_6(age)})}{1 + exp^{\\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) + \\hat\\beta_6(age)}}\\]many datasets, independent variables discrete, nominal scale variables race, sex, treatment group, forth. inappropriate include model interval scale variables. Though many software, represented numbers, numbers used merely identifiers.situation, use method called design variables (dummy variables). Suppose, example, assuming one independent variables obesity type, now coded â€œClass 1â€, â€œClass 2â€ â€œClass 3â€. case, 3 levels categories, hence two design variables (\\(D - 1\\)) necessary, letâ€™s say D1 D2. One possible coding strategy patient â€œClass 1â€ two design variables, D1 D2 set equal zero. example, â€œClass 1â€ reference category. patient â€œClass 2â€, D1 set 1 D2 0; patient â€œClass 3â€, set D1 0 D2 1. coding assignment can done automatically software. interpret, must know category reference.","code":""},{"path":"log-reg.html","id":"prepare-environment-for-analysis","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.6 Prepare environment for analysis","text":"","code":""},{"path":"log-reg.html","id":"creating-a-rstudio-project","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.6.1 Creating a RStudio project","text":"Start new analysis task creating new RStudio project. ,Go FileClick New ProjectChoose New Directory Existing Directory.directory points folder usually contains dataset analyzed. called working directory. Make sure folder named data folder. , create one. Make sure dataset stroke.dta inside data folder working directory.","code":""},{"path":"log-reg.html","id":"loading-libraries","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.6.2 Loading libraries","text":"Next, load necessary packages. use 5 packagesthe built stat package - run Generalized Linear Model. already loaded default.haven - read SPSS, STATA SAS datasettidyverse - perform data transformationgtsummary - provide nice results tablebroom - tidy resultsLogisticDx - model assessmenthere - ensure proper directoryTo load packages, use function library():","code":"\nlibrary(haven)\nlibrary(tidyverse)\n#> â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\n#> âœ” ggplot2 3.4.3     âœ” purrr   1.0.2\n#> âœ” tibble  3.2.1     âœ” stringr 1.5.0\n#> âœ” tidyr   1.3.0     âœ” forcats 1.0.0\n#> âœ” readr   2.1.2     \n#> â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\n#> âœ– lubridate::as.difftime() masks base::as.difftime()\n#> âœ– lubridate::date()        masks base::date()\n#> âœ– dplyr::filter()          masks stats::filter()\n#> âœ– kableExtra::group_rows() masks dplyr::group_rows()\n#> âœ– lubridate::intersect()   masks base::intersect()\n#> âœ– dplyr::lag()             masks stats::lag()\n#> âœ– lubridate::setdiff()     masks base::setdiff()\n#> âœ– lubridate::union()       masks base::union()\nlibrary(gtsummary)\n#> #BlackLivesMatter\nlibrary(broom)\nlibrary(LogisticDx)\nlibrary(here)\n#> here() starts at /Users/niccolo/Desktop/r_projects/sbd_24_25"},{"path":"log-reg.html","id":"read-data","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.7 Read data","text":"read data working directory R environment. Remember dataset STATA format.Take peek data. Checkvariable namesvariable types","code":"\nfatal <- read_dta(here('data','stroke.dta'))\nglimpse(fatal)\n#> Rows: 226\n#> Columns: 7\n#> $ sex         <dbl+lbl> 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2â€¦\n#> $ status      <dbl+lbl> 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1â€¦\n#> $ gcs         <dbl> 13, 15, 15, 15, 15, 15, 13, 15, 15, 10â€¦\n#> $ sbp         <dbl> 143, 150, 152, 215, 162, 169, 178, 180â€¦\n#> $ dm          <dbl+lbl> 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1â€¦\n#> $ age         <dbl> 50, 58, 64, 50, 65, 78, 66, 72, 61, 64â€¦\n#> $ stroke_type <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦"},{"path":"log-reg.html","id":"explore-data","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.8 Explore data","text":"Variables sex, status, dm stroke type labelled variable. means eventhough coded numbers numbers represent groups categories levels variables. Basically, categorical variables.transform factor variables. can quickly using function across(). , transform labelled variables factor variables:Now, can look summary statisticsTable 15.1:  CharacteristicN = 226or get summary statistics status category:Table 15.2:  Characteristicalive, N = 171dead, N = 55","code":"\nfatal <- \n  fatal %>%\n  mutate(across(where(is.labelled), as_factor))\nfatal %>%\n  tbl_summary() %>%\n  as_hux_table()\nfatal %>%\n  tbl_summary(by = status) %>%\n  as_hux_table()"},{"path":"log-reg.html","id":"estimate-the-regression-parameters","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.9 Estimate the regression parameters","text":"now can perform binary logistic regression estimate regression parameters \\(\\hat\\beta_s\\) log odds. Usually, can two steps:simple binary logistic regression univariable logistic regression. analysis, one independent variable covariate model. also known crude unadjusted analysis.multiple binary logistic regression multivariable logistic regression. , expand model include two independent variables (covariates). adjusted model can obtain estimate particular covariate independent covariates model.","code":""},{"path":"log-reg.html","id":"simple-binary-logistic-regression","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.10 Simple binary logistic regression","text":"simple binary logistic regression dependent variable one independent (covariate) variable. dataset, example, can havestatus dependent variable.gcs independent variable.independent variable can numerical categorical variable. estimate log odds (regression parameters, \\(\\beta\\)) covariate Glasgow Coma Scale (GCS), can write logit model :\\[log\\frac{p(status = dead)}{1 - p(status = dead)}  = \\hat\\beta_0 + \\hat\\beta_1(gcs)\\]R, use glm() function estimate regression parameters parameters interest. Letâ€™s run model gcs covariate name model fatal_glm_1To get summarized result model, use summary() function:get model summary data frame format, can edit easily, can use tidy() function broom package. package also contains functions provide parameters useful us later.function conf.int() provide confidence intervals (CI). default set \\(95%\\) level:estimates log odds death given value gcs. example, unit increase gcs, crude unadjusted log odds death due stroke change factor \\(-0.388\\) \\(95%\\) CI ranges \\(-0.497 -0.292\\).Now, letâ€™s use another covariate, stroke_type. Stroke type 2 levels categories; Haemorrhagic Stroke (HS) Ischaemic Stroke (). HS known cause higher risk deaths stroke. model stroke type (stroke_type), name model fatal_glm_2 show result using tidy()seems patients Haemorrhagic Stroke (HS) higher log odds death admission - factor \\(2.02\\) - patients Ischaemic Stroke ().","code":"\nfatal_glm_1 <- \n  glm(status ~ gcs, \n      data = fatal, \n      family = binomial(link = 'logit'))\nsummary(fatal_glm_1)\n#> \n#> Call:\n#> glm(formula = status ~ gcs, family = binomial(link = \"logit\"), \n#>     data = fatal)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1179  -0.3921  -0.3921  -0.3921   2.2820  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value           Pr(>|z|)\n#> (Intercept)  3.29479    0.60432   5.452 0.0000000497803390\n#> gcs         -0.38811    0.05213  -7.446 0.0000000000000964\n#>                \n#> (Intercept) ***\n#> gcs         ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 250.83  on 225  degrees of freedom\n#> Residual deviance: 170.92  on 224  degrees of freedom\n#> AIC: 174.92\n#> \n#> Number of Fisher Scoring iterations: 5\ntidy(fatal_glm_1, conf.int = TRUE)\n#> # A tibble: 2 Ã— 7\n#>   term        estimate std.error statistic  p.value conf.low\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n#> 1 (Intercept)    3.29     0.604       5.45 4.98e- 8    2.17 \n#> 2 gcs           -0.388    0.0521     -7.45 9.64e-14   -0.497\n#> # â„¹ 1 more variable: conf.high <dbl>\nfatal_glm_2 <- \n  glm(status ~ stroke_type, \n      data = fatal, \n      family = binomial(link = 'logit'))\ntidy(fatal_glm_2, conf.int = TRUE)\n#> # A tibble: 2 Ã— 7\n#>   term        estimate std.error statistic  p.value conf.low\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n#> 1 (Intercept)    -2.05     0.258     -7.95 1.80e-15    -2.59\n#> 2 stroke_typâ€¦     2.02     0.344      5.88 4.05e- 9     1.36\n#> # â„¹ 1 more variable: conf.high <dbl>"},{"path":"log-reg.html","id":"multiple-binary-logistic-regression","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.11 Multiple binary logistic regression","text":"strong motivation include covariates model. becauseIt unlikely one variable (gcs stroke type) related stroke. example, cardiovascular disease many factors affect outcome. , makes sense consider adding seemingly important independent variable model.adding covariates model, can estimate adjusted log odds. log odds particular covariate independent covariates.can add covariate adjust confounding effectsinteraction (product two covariates) can also estimatedTo add add variables big subject . Usually governed clinical experience, subject matter experts preliminary analysis.Letâ€™s expand model include gcs, stroke type, sex, dm, sbp age model. name model fatal_mv. run model get estimates R:get cleaner result data frame format (can edit spreadsheet easily) using tidy():see multivariable model, thatwith one unit increase Glasgow Coma Scale (GCS), log odds death hospitalization equals \\(-0.328\\), adjusting covariatespatients HS \\(1.266\\) times log odds death compared patients , adjusting covariates.female patients \\(0.430\\) times log odds death compared male patients, adjusting covariatespatients diabetes mellitus \\(0.474\\) times log odds deaths compared patients diabetes mellitusWith one mmHg increase systolic blood pressure, log odds deaths change factor \\(0.00086\\), adjusting variables.increase one year age, log odds deaths change factor \\(0.024\\), adjusting variables.","code":"\nfatal_mv1 <- \n  glm(status ~ gcs + stroke_type + sex + dm + sbp + age, \n      data = fatal, \n      family = binomial(link = 'logit'))\n\nsummary(fatal_mv1)\n#> \n#> Call:\n#> glm(formula = status ~ gcs + stroke_type + sex + dm + sbp + age, \n#>     family = binomial(link = \"logit\"), data = fatal)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3715  -0.4687  -0.3280  -0.1921   2.5150  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error z value\n#> (Intercept)             -0.1588269  1.6174965  -0.098\n#> gcs                     -0.3284640  0.0557574  -5.891\n#> stroke_typeHaemorrhagic  1.2662764  0.4365882   2.900\n#> sexfemale                0.4302901  0.4362742   0.986\n#> dmyes                    0.4736670  0.4362309   1.086\n#> sbp                      0.0008612  0.0060619   0.142\n#> age                      0.0242321  0.0154010   1.573\n#>                              Pr(>|z|)    \n#> (Intercept)                   0.92178    \n#> gcs                     0.00000000384 ***\n#> stroke_typeHaemorrhagic       0.00373 ** \n#> sexfemale                     0.32399    \n#> dmyes                         0.27756    \n#> sbp                           0.88703    \n#> age                           0.11562    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 250.83  on 225  degrees of freedom\n#> Residual deviance: 159.34  on 219  degrees of freedom\n#> AIC: 173.34\n#> \n#> Number of Fisher Scoring iterations: 5\nlog_odds <- tidy(fatal_mv1, \n                 conf.int = TRUE)\nlog_odds\n#> # A tibble: 7 Ã— 7\n#>   term         estimate std.error statistic p.value conf.low\n#>   <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n#> 1 (Intercept)  -1.59e-1   1.62      -0.0982 9.22e-1 -3.38   \n#> 2 gcs          -3.28e-1   0.0558    -5.89   3.84e-9 -0.444  \n#> 3 stroke_typeâ€¦  1.27e+0   0.437      2.90   3.73e-3  0.411  \n#> 4 sexfemale     4.30e-1   0.436      0.986  3.24e-1 -0.420  \n#> 5 dmyes         4.74e-1   0.436      1.09   2.78e-1 -0.368  \n#> 6 sbp           8.61e-4   0.00606    0.142  8.87e-1 -0.0110 \n#> 7 age           2.42e-2   0.0154     1.57   1.16e-1 -0.00520\n#> # â„¹ 1 more variable: conf.high <dbl>"},{"path":"log-reg.html","id":"convert-the-log-odds-to-odds-ratio","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.12 Convert the log odds to odds ratio","text":"lay person, difficult interpret log odds. easier interpret using odds ratio. , can use argument exponentiate = TRUE tidy() function. However, also know odds ratio can easily calculate \\(\\exp^{\\beta_i}\\)","code":"\nodds_ratio <- tidy(fatal_mv1,\n                   exponentiate = TRUE,  \n                   conf.int = TRUE)\nodds_ratio\n#> # A tibble: 7 Ã— 7\n#>   term         estimate std.error statistic p.value conf.low\n#>   <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n#> 1 (Intercept)     0.853   1.62      -0.0982 9.22e-1   0.0341\n#> 2 gcs             0.720   0.0558    -5.89   3.84e-9   0.641 \n#> 3 stroke_typeâ€¦    3.55    0.437      2.90   3.73e-3   1.51  \n#> 4 sexfemale       1.54    0.436      0.986  3.24e-1   0.657 \n#> 5 dmyes           1.61    0.436      1.09   2.78e-1   0.692 \n#> 6 sbp             1.00    0.00606    0.142  8.87e-1   0.989 \n#> 7 age             1.02    0.0154     1.57   1.16e-1   0.995 \n#> # â„¹ 1 more variable: conf.high <dbl>"},{"path":"log-reg.html","id":"making-inference","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.13 Making inference","text":"Let us combine results log odds odds ratio rename table properly.model, means :gcs increases 1 unit (stroke type adjusted), log odds death changes factor \\(-0.32\\) odds death changes factor \\(0.72\\) (odds death reduces \\(28\\%\\)). \\(95\\%CI\\) \\(21\\%,36\\%\\), adjusting covariates.patients HS \\(3.55\\%\\) times higher odds stroke deaths - \\(95\\%CI : 17\\%, 85\\%\\) - compared patients HS, adjusting independent variables.female patients \\(53\\%\\) higher odds death compared female patients (\\(p = 0.154\\)), adjusting covariatespatients diabetes mellitus \\(60.6\\%\\) higher odds deaths compared patients diabetes mellitus though p value \\(5\\%\\) (\\(p = 0.642\\%\\))one mmHg increase systolic blood pressure, odds death change factor \\(1.00086\\), adjusting variables. p value also larger \\(5\\%\\).increase one year age, odds deaths increase factor \\(1.025\\), adjusting variables. However, p value \\(0.115\\)","code":"\ntab_logistic <- bind_cols(log_odds, odds_ratio) \n#> New names:\n#> â€¢ `term` -> `term...1`\n#> â€¢ `estimate` -> `estimate...2`\n#> â€¢ `std.error` -> `std.error...3`\n#> â€¢ `statistic` -> `statistic...4`\n#> â€¢ `p.value` -> `p.value...5`\n#> â€¢ `conf.low` -> `conf.low...6`\n#> â€¢ `conf.high` -> `conf.high...7`\n#> â€¢ `term` -> `term...8`\n#> â€¢ `estimate` -> `estimate...9`\n#> â€¢ `std.error` -> `std.error...10`\n#> â€¢ `statistic` -> `statistic...11`\n#> â€¢ `p.value` -> `p.value...12`\n#> â€¢ `conf.low` -> `conf.low...13`\n#> â€¢ `conf.high` -> `conf.high...14`\ntab_logistic %>% \n  select(term...1, estimate...2, std.error...3, \n         estimate...9, conf.low...13, conf.high...14 ,p.value...5) %>%\n  rename(covariate = term...1, \n         log_odds = estimate...2,\n         SE = std.error...3,\n         odds_ratio = estimate...9,\n         lower_OR = conf.low...13, \n         upper_OR = conf.high...14,\n         p.val = p.value...5) \n#> # A tibble: 7 Ã— 7\n#>   covariate    log_odds      SE odds_ratio lower_OR upper_OR\n#>   <chr>           <dbl>   <dbl>      <dbl>    <dbl>    <dbl>\n#> 1 (Intercept)  -1.59e-1 1.62         0.853   0.0341   20.3  \n#> 2 gcs          -3.28e-1 0.0558       0.720   0.641     0.799\n#> 3 stroke_typeâ€¦  1.27e+0 0.437        3.55    1.51      8.45 \n#> 4 sexfemale     4.30e-1 0.436        1.54    0.657     3.69 \n#> 5 dmyes         4.74e-1 0.436        1.61    0.692     3.87 \n#> 6 sbp           8.61e-4 0.00606      1.00    0.989     1.01 \n#> 7 age           2.42e-2 0.0154       1.02    0.995     1.06 \n#> # â„¹ 1 more variable: p.val <dbl>"},{"path":"log-reg.html","id":"model-comparison","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.14 Model comparison","text":"advisable assess important variables based p-values Wald statistics. better way use likelihood ratio compare models assess importance variables.example, statistical difference model 1 (fatal_mv) model 2 (fatal_glm_1) set level significance \\(5\\%\\)?models different statistically (\\(5\\%\\) level). Hence, prefer keep model fatal_mv1.Now letâ€™s economical, just keep gcs, stroke type age model. letâ€™s name model fatal_mv2And perform model comparison againThe p-value threshold \\(5\\%\\), can reject null hypothesis say models statistically different. obeying Occamâ€™s razor principle, choose simpler model model fatal_mv2 exploration.","code":"\nanova( fatal_glm_1, fatal_mv1, test = 'Chisq')\n#> Analysis of Deviance Table\n#> \n#> Model 1: status ~ gcs\n#> Model 2: status ~ gcs + stroke_type + sex + dm + sbp + age\n#>   Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n#> 1       224     170.92                       \n#> 2       219     159.34  5   11.582  0.04098 *\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nfatal_mv2 <- \n  glm(status ~ gcs + stroke_type + age, \n      data = fatal,\n      family = binomial(link = 'logit'))\nanova( fatal_mv1, \n       fatal_mv2, test = 'Chisq')\n#> Analysis of Deviance Table\n#> \n#> Model 1: status ~ gcs + stroke_type + sex + dm + sbp + age\n#> Model 2: status ~ gcs + stroke_type + age\n#>   Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n#> 1       219     159.34                     \n#> 2       222     161.51 -3  -2.1743    0.537"},{"path":"log-reg.html","id":"adding-an-interaction-term","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.15 Adding an interaction term","text":"Interaction effects occur effect one variable depends value another variable. Interaction effects common regression analysis, ANOVA, designed experiments.Interaction involves two risk factors (effect one disease outcome). effect one risk factor within strata defined , interaction. effect one risk factor different within strata defined , interaction (biological)(Statistical) interaction can measured based ways risks calculated (modeling). presence interaction based measurements called statistical interaction, inherently may reflect true biological interaction.Letâ€™s add interaction stroke type diabetes:\\[\\hat{g}(x)  = \\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(age)+ \\hat\\beta_4(gcs \\times stroke_type)\\]decide interaction term stay model, suggest consider biological statistical significance. think interaction justifies reasons, preferred keep interaction term model. example, model:coefficient interaction term stroke type gcs significant %5%$ level.getting advice stroke experts, believe effect gcs stroke death largely different different stroke typeAnd reasons, decided keep interaction gcs stroke type model.","code":"\nfatal_mv2_ia <- \n  glm(status ~ gcs + stroke_type + stroke_type:gcs + age, \n      data = fatal, \n      family = binomial(link = 'logit'))\ntidy(fatal_mv2_ia)\n#> # A tibble: 5 Ã— 5\n#>   term                  estimate std.error statistic p.value\n#>   <chr>                    <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)             0.508     1.37       0.371 7.10e-1\n#> 2 gcs                    -0.320     0.0800    -4.01  6.19e-5\n#> 3 stroke_typeHaemorrhaâ€¦   1.61      1.30       1.24  2.17e-1\n#> 4 age                     0.0236    0.0147     1.60  1.09e-1\n#> 5 gcs:stroke_typeHaemoâ€¦  -0.0347    0.111     -0.312 7.55e-1"},{"path":"log-reg.html","id":"prediction-from-binary-logistic-regression","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.16 Prediction from binary logistic regression","text":"can use broom::augment() function calculate thelog oddsprobabilityresidualshat valuesCooks distancestandardized residuals","code":""},{"path":"log-reg.html","id":"predict-the-log-odds","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.16.1 Predict the log odds","text":"obtain .fitted column (representing estimated log odds death) patient, can run:slice() gives snapshot data. case, choose first 10 patients.","code":"\nlog_odds_mv2 <- augment(fatal_mv2)\nlog_odds_mv2 %>%\n  slice(1:10)\n#> # A tibble: 10 Ã— 10\n#>    status   gcs stroke_type    age .fitted .resid .std.resid\n#>    <fct>  <dbl> <fct>        <dbl>   <dbl>  <dbl>      <dbl>\n#>  1 alive     13 Ischaemic Sâ€¦    50   -2.49 -0.398     -0.400\n#>  2 alive     15 Ischaemic Sâ€¦    58   -2.98 -0.314     -0.315\n#>  3 alive     15 Ischaemic Sâ€¦    64   -2.84 -0.337     -0.338\n#>  4 alive     15 Ischaemic Sâ€¦    50   -3.17 -0.287     -0.288\n#>  5 alive     15 Ischaemic Sâ€¦    65   -2.82 -0.341     -0.342\n#>  6 alive     15 Ischaemic Sâ€¦    78   -2.51 -0.395     -0.397\n#>  7 dead      13 Ischaemic Sâ€¦    66   -2.12  2.11       2.12 \n#>  8 alive     15 Ischaemic Sâ€¦    72   -2.65 -0.369     -0.370\n#>  9 alive     15 Ischaemic Sâ€¦    61   -2.91 -0.325     -0.326\n#> 10 dead      10 Ischaemic Sâ€¦    64   -1.15  1.69       1.70 \n#> # â„¹ 3 more variables: .hat <dbl>, .sigma <dbl>,\n#> #   .cooksd <dbl>"},{"path":"log-reg.html","id":"predict-the-probabilities","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.16.2 Predict the probabilities","text":"obtain .fitted column (representing estimated probabilities death) patient, can run:","code":"\nprob_mv2 <- \n  augment(fatal_mv2, \n          type.predict = \"response\")\nprob_mv2 %>%\n  slice(1:10)\n#> # A tibble: 10 Ã— 10\n#>    status   gcs stroke_type    age .fitted .resid .std.resid\n#>    <fct>  <dbl> <fct>        <dbl>   <dbl>  <dbl>      <dbl>\n#>  1 alive     13 Ischaemic Sâ€¦    50  0.0763 -0.398     -0.400\n#>  2 alive     15 Ischaemic Sâ€¦    58  0.0482 -0.314     -0.315\n#>  3 alive     15 Ischaemic Sâ€¦    64  0.0551 -0.337     -0.338\n#>  4 alive     15 Ischaemic Sâ€¦    50  0.0403 -0.287     -0.288\n#>  5 alive     15 Ischaemic Sâ€¦    65  0.0564 -0.341     -0.342\n#>  6 alive     15 Ischaemic Sâ€¦    78  0.0750 -0.395     -0.397\n#>  7 dead      13 Ischaemic Sâ€¦    66  0.107   2.11       2.12 \n#>  8 alive     15 Ischaemic Sâ€¦    72  0.0658 -0.369     -0.370\n#>  9 alive     15 Ischaemic Sâ€¦    61  0.0516 -0.325     -0.326\n#> 10 dead      10 Ischaemic Sâ€¦    64  0.241   1.69       1.70 \n#> # â„¹ 3 more variables: .hat <dbl>, .sigma <dbl>,\n#> #   .cooksd <dbl>"},{"path":"log-reg.html","id":"model-fitness","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.17 Model fitness","text":"assess overall model fitness checking thethe area curvethe Hosmer-Lemeshow testthe modidied Hosmer-Lemeshow testthe Oseo Rojek testThe p-values bigger 0.05 indicates significant difference observed data predicted data (model). supports good fit model.\narea curve \\(87.2\\%\\). values 80 considered good discriminating effect.Hosmer Lemeshow, modified Hosmer Lemeshow Oseo Rojek \\(5\\%\\) values supportive good fit model.","code":"\nfit_m <- gof(fatal_mv2, \n             g = 8)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\nfit_m$gof\n#>          test  stat       val df      pVal\n#> 1:         HL chiSq  4.622183  6 0.5930997\n#> 2:        mHL     F  1.071882  7 0.3844230\n#> 3:       OsRo     Z -0.501724 NA 0.6158617\n#> 4: SstPgeq0.5     Z  1.348843 NA 0.1773873\n#> 5:   SstPl0.5     Z  1.516578 NA 0.1293733\n#> 6:    SstBoth chiSq  4.119387  2 0.1274931\n#> 7: SllPgeq0.5 chiSq  1.579811  1 0.2087879\n#> 8:   SllPl0.5 chiSq  2.311910  1 0.1283862\n#> 9:    SllBoth chiSq  2.341198  2 0.3101811"},{"path":"log-reg.html","id":"presentation","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.18 Presentation","text":"gtsummary package useful function tbld_regression() produce formatted table suitable publication.table adjusted log odds ratio:table adjusted odds ratio:","code":"\ntbl_regression(fatal_mv2)\ntbl_regression(fatal_mv2, exponentiate = TRUE)"},{"path":"log-reg.html","id":"resources","chapter":"15 ğŸ’» Binary Logistic Regression","heading":"15.19 Resources","text":"","code":""},{"path":"log-reg-ex.html","id":"log-reg-ex","chapter":"16 ğŸ’» Binary LogReg exer","heading":"16 ğŸ’» Binary LogReg exer","text":"collection exercises concerning linear regression stuff! actually (noised chaging data) samples exam, dig ğŸ¦«! done class lecture, others left ---home exer.class stuck either ask teacher assemble group work ! Collaboration key ğŸ‘¯â€â™‚ï¸!","code":""},{"path":"log-reg-ex.html","id":"mixed-exercises","chapter":"16 ğŸ’» Binary LogReg exer","heading":"16.1 mixed Exercises ğŸ‘¨â€ğŸ’»","text":"Exercise 16.1  Load MASS package combine Pima.tr Pima.tr2 data.frame called train save Pima.te test. Change coding variable interest (type) 0 (non-diabetic) 1 (diabetic). Check take note missing values.Using glm() train data fit logistic model type age bmi. Print coefficients p-value.linear regression just specify first model formula, data end link. link actually logistic (logit).Answer Question 16.1:Missing values?Estimate Logistic regression ) formula b) data c) family = binomial :Exercise 16.2  Take considerationd data â€œtwo truths lieâ€ experiment described beginning lesson available two-truths---lie-2022-cleaned.csv data directory website:Read dataset directly R web, storing data.frame called two_truthshttps://ditraglia.com/data/Run logistic regression predicts guessed_right based certainty. coef stat significant?may see estimated coefficient certainty negative! means average predict wrong guesses studentsâ€™ subjective certainty guesses increases.Answer Question 16.2:needs read data:perform log reg:Exercise 16.3  BreastCancer data frame 699 observations 11 variables, one character variable, 9 ordered nominal, 1 target class. mlbench package. objective identify number benign malignant classes. Samples arrive periodically Dr.Â Wolberg reports clinical cases. database therefore reflects chronological grouping data. grouping information appears immediately , removed data . variable except first converted 11 primitive numerical attributes values ranging 0 10. 16 missing attribute values. See cited details.commento prima dellâ€™esercizioAnswer Question 16.3:first tou load data.take glance itin end fit model:","code":"library(MASS)\nlibrary(ggplot2)\n\ntrain <- rbind(Pima.tr, Pima.tr2)\ntest  <- Pima.te\n\ntrain$type <- as.integer(train$type) - 1L\ntest$type <- as.integer(test$type) - 1Lsapply(train, function(x) sum(is.na(x)))lg1 <- glm(type ~ age + bmi, data = train, family = binomial)\nsummary(lg1)$coefficients[, c(1, 4)]library(tidyverse)\ndata_url <- 'https://ditraglia.com/data/two-truths-and-a-lie-2022-cleaned.csv'\ntwo_truths <- read_csv(data_url)two_truths_reg <- glm(guessed_right ~ certainty, family = binomial(link = 'logit'),\n                      data = two_truths)\nsummary(two_truths_reg)data(BreastCancer, package=\"mlbench\")\nbc <- BreastCancer[complete.cases(BreastCancer), ]str(bc)glm(Class ~ Cell.shape, family=\"binomial\", data = bc)"},{"path":"pca.html","id":"pca","chapter":"17 ğŸ’» PCA","heading":"17 ğŸ’» PCA","text":"","code":""},{"path":"pca.html","id":"principal-components-analysis","chapter":"17 ğŸ’» PCA","heading":"17.1 Principal Components Analysis","text":"use following packages â€˜FactoMineRâ€™, â€˜factoextraâ€™, â€˜ISRL2â€™","code":""},{"path":"pca.html","id":"pca-using-factominer-factoextra","chapter":"17 ğŸ’» PCA","heading":"17.1.1 PCA using â€˜FactoMineRâ€™, â€˜factoextraâ€™","text":"","code":"#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa"},{"path":"pca.html","id":"exercise-1-read-the-data","chapter":"17 ğŸ’» PCA","heading":"17.1.2 Exercise 1 : read the data","text":"Covariance matrixEigen-analysis correlation matrix","code":"#>    Var1 Var2\n#> i1    2    2\n#> i2    1   -1\n#> i3   -1    1\n#> i4   -2   -2\nmean(X[,1]) # mean of Var1\n#> [1] 0\nmean(X[,2]) # mean of Var2\n#> [1] 0\n\n##variance and inertia \nS=var(X)*(3/4)  # the constant (n-1)/n is have the variance-covariance matrix used in the lecture\nS\n#>      Var1 Var2\n#> Var1  2.5  1.5\n#> Var2  1.5  2.5\n#inertia\nInertia=sum(diag(S))\nInertia\n#> [1] 5\n## eigen-analysis\neigen(S) # gives the eigen-values and eigen-vectors\n#> eigen() decomposition\n#> $values\n#> [1] 4 1\n#> \n#> $vectors\n#>           [,1]       [,2]\n#> [1,] 0.7071068 -0.7071068\n#> [2,] 0.7071068  0.7071068\nR=cor(X)\neigenan=eigen(R) ##eigen analysis of R\neigenan\n#> eigen() decomposition\n#> $values\n#> [1] 1.6 0.4\n#> \n#> $vectors\n#>           [,1]       [,2]\n#> [1,] 0.7071068 -0.7071068\n#> [2,] 0.7071068  0.7071068\nsum(eigenan$values)\n#> [1] 2\n#Inertia is p=2\n\n#normalized the data\nZ=scale(X)\nvar(Z) ## is teh correlation matrix \n#>      Var1 Var2\n#> Var1  1.0  0.6\n#> Var2  0.6  1.0"},{"path":"pca.html","id":"pca-function","chapter":"17 ğŸ’» PCA","heading":"17.1.3 PCA function","text":"PCA covariance matrix (using centered data). PCA correlation matrix (normed PCA), use scale.unit = TRUE (default option).Correlation two variables \\(X_1\\), \\(X_2\\)\\[\\rho=\\frac{cov(X_1,X_2)}{\\sigma_{X_1}\\sigma_{X_2}}\\]\n\\(cov(X_1,X_2)\\) covariance, \\(\\sigma_{X_1}=\\sqrt{Var(X_1)}\\) standard deviation \\(X_1\\).Eigen-analysis covariance matrix (â€˜scale.unit=FALSEâ€™)","code":"#> **Results for the Principal Component Analysis (PCA)**\n#> The analysis was performed on 4 individuals, described by 2 variables\n#> *The results are available in the following objects:\n#> \n#>    name               description                          \n#> 1  \"$eig\"             \"eigenvalues\"                        \n#> 2  \"$var\"             \"results for the variables\"          \n#> 3  \"$var$coord\"       \"coord. for the variables\"           \n#> 4  \"$var$cor\"         \"correlations variables - dimensions\"\n#> 5  \"$var$cos2\"        \"cos2 for the variables\"             \n#> 6  \"$var$contrib\"     \"contributions of the variables\"     \n#> 7  \"$ind\"             \"results for the individuals\"        \n#> 8  \"$ind$coord\"       \"coord. for the individuals\"         \n#> 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n#> 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n#> 11 \"$call\"            \"summary statistics\"                 \n#> 12 \"$call$centre\"     \"mean of the variables\"              \n#> 13 \"$call$ecart.type\" \"standard error of the variables\"    \n#> 14 \"$call$row.w\"      \"weights for the individuals\"        \n#> 15 \"$call$col.w\"      \"weights for the variables\""},{"path":"pca.html","id":"eigen-values","chapter":"17 ğŸ’» PCA","heading":"17.1.4 Eigen-values","text":"\\(p=2=min(n-1,p)= min(3,2)\\) eigen-values, 4 1, \\(Inertia=4+1=4\\) sum variances variables.","code":"#>        eigenvalue percentage of variance\n#> comp 1          4                     80\n#> comp 2          1                     20\n#>        cumulative percentage of variance\n#> comp 1                                80\n#> comp 2                               100"},{"path":"pca.html","id":"the-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.5 The variables","text":"","code":"#> $coord\n#>         Dim.1      Dim.2\n#> Var1 1.414214 -0.7071068\n#> Var2 1.414214  0.7071068\n#> \n#> $cor\n#>          Dim.1      Dim.2\n#> Var1 0.8944272 -0.4472136\n#> Var2 0.8944272  0.4472136\n#> \n#> $cos2\n#>      Dim.1 Dim.2\n#> Var1   0.8   0.2\n#> Var2   0.8   0.2\n#> \n#> $contrib\n#>      Dim.1 Dim.2\n#> Var1    50    50\n#> Var2    50    50"},{"path":"pca.html","id":"the-inviduals","chapter":"17 ğŸ’» PCA","heading":"17.1.6 The inviduals","text":"","code":"#> $coord\n#>                         Dim.1                       Dim.2\n#> i1  2.82842712474619073503845 -0.000000000000000006357668\n#> i2  0.00000000000000001271534 -1.414213562373095367519227\n#> i3 -0.00000000000000040992080  1.414213562373094923430017\n#> i4 -2.82842712474619029094924  0.000000000000000204960400\n#> \n#> $cos2\n#>                                          Dim.1\n#> i1 1.00000000000000044408920985006261616945267\n#> i2 0.00000000000000000000000000000000008083988\n#> i3 0.00000000000000000000000000000008401753104\n#> i4 1.00000000000000022204460492503130808472633\n#>                                           Dim.2\n#> i1 0.000000000000000000000000000000000005052492\n#> i2 1.000000000000000444089209850062616169452667\n#> i3 0.999999999999999777955395074968691915273666\n#> i4 0.000000000000000000000000000000005251095690\n#> \n#> $contrib\n#>                                         Dim.1\n#> i1 50.000000000000021316282072803005576133728\n#> i2  0.000000000000000000000000000000001010498\n#> i3  0.000000000000000000000000000001050219138\n#> i4 50.000000000000014210854715202003717422485\n#>                                         Dim.2\n#> i1  0.000000000000000000000000000000001010498\n#> i2 50.000000000000021316282072803005576133728\n#> i3 49.999999999999985789145284797996282577515\n#> i4  0.000000000000000000000000000001050219138\n#> \n#> $dist\n#>       i1       i2       i3       i4 \n#> 2.828427 1.414214 1.414214 2.828427"},{"path":"pca.html","id":"another-example","chapter":"17 ğŸ’» PCA","heading":"17.1.7 Another example","text":"","code":"\nA=matrix(c(9,12,10,15,9,10,5,10,8,11,13,14,11,13,8,3,15,10),nrow=6, byrow=TRUE)\nA\n#>      [,1] [,2] [,3]\n#> [1,]    9   12   10\n#> [2,]   15    9   10\n#> [3,]    5   10    8\n#> [4,]   11   13   14\n#> [5,]   11   13    8\n#> [6,]    3   15   10\nNframe=as.data.frame(A)\nm1=c(\"Alex\", \"Bea\", \"Claudio\",\"Damien\", \"Emilie\", \"Fran\")\nm2=c(\"Biostatistics\", \"Economics\", \"English\")\nrow.names(A)=m1\ncolnames(A)=m2\nhead(A)\n#>         Biostatistics Economics English\n#> Alex                9        12      10\n#> Bea                15         9      10\n#> Claudio             5        10       8\n#> Damien             11        13      14\n#> Emilie             11        13       8\n#> Fran                3        15      10"},{"path":"pca.html","id":"pca-on-the-correlation-matrix","chapter":"17 ğŸ’» PCA","heading":"17.1.8 PCA on the correlation matrix","text":"","code":"#> **Results for the Principal Component Analysis (PCA)**\n#> The analysis was performed on 6 individuals, described by 3 variables\n#> *The results are available in the following objects:\n#> \n#>    name               description                          \n#> 1  \"$eig\"             \"eigenvalues\"                        \n#> 2  \"$var\"             \"results for the variables\"          \n#> 3  \"$var$coord\"       \"coord. for the variables\"           \n#> 4  \"$var$cor\"         \"correlations variables - dimensions\"\n#> 5  \"$var$cos2\"        \"cos2 for the variables\"             \n#> 6  \"$var$contrib\"     \"contributions of the variables\"     \n#> 7  \"$ind\"             \"results for the individuals\"        \n#> 8  \"$ind$coord\"       \"coord. for the individuals\"         \n#> 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n#> 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n#> 11 \"$call\"            \"summary statistics\"                 \n#> 12 \"$call$centre\"     \"mean of the variables\"              \n#> 13 \"$call$ecart.type\" \"standard error of the variables\"    \n#> 14 \"$call$row.w\"      \"weights for the individuals\"        \n#> 15 \"$call$col.w\"      \"weights for the variables\""},{"path":"pca.html","id":"eigen-values-1","chapter":"17 ğŸ’» PCA","heading":"17.1.9 Eigen-values","text":"Kaiser rule suggests \\(q=2\\) components eigen-value mean 1 (89% explained variance). rule Thumb gives \\(q=2\\) first 2 dimensions explain 89% variance/inertia.","code":"#>        eigenvalue percentage of variance\n#> comp 1  1.5000000               50.00000\n#> comp 2  1.1830127               39.43376\n#> comp 3  0.3169873               10.56624\n#>        cumulative percentage of variance\n#> comp 1                          50.00000\n#> comp 2                          89.43376\n#> comp 3                         100.00000\n#>       eigenvalue variance.percent\n#> Dim.1  1.5000000         50.00000\n#> Dim.2  1.1830127         39.43376\n#> Dim.3  0.3169873         10.56624\n#>       cumulative.variance.percent\n#> Dim.1                    50.00000\n#> Dim.2                    89.43376\n#> Dim.3                   100.00000"},{"path":"pca.html","id":"variables","chapter":"17 ğŸ’» PCA","heading":"17.1.10 Variables","text":"","code":"#> Principal Component Analysis Results for variables\n#>  ===================================================\n#>   Name      \n#> 1 \"$coord\"  \n#> 2 \"$cor\"    \n#> 3 \"$cos2\"   \n#> 4 \"$contrib\"\n#>   Description                                    \n#> 1 \"Coordinates for the variables\"                \n#> 2 \"Correlations between variables and dimensions\"\n#> 3 \"Cos2 for the variables\"                       \n#> 4 \"contributions of the variables\""},{"path":"pca.html","id":"correlations-of-variables-and-componentsdimensions","chapter":"17 ğŸ’» PCA","heading":"17.1.11 Correlations of variables and components/dimensions","text":"first axis correlated Biostatistics (+0.86) Economics (-0.86). second axis correlated English (0.96).two components (dimensions) correlated least one variable. \\(q=2\\) may considered reduce dimension (\\(p=3\\)).","code":"#>                                  Dim.1     Dim.2      Dim.3\n#> Biostatistics -0.866025403784438041477 0.3535534  0.3535534\n#> Economics      0.866025403784439151700 0.3535534  0.3535534\n#> English        0.000000000000001047394 0.9659258 -0.2588190"},{"path":"pca.html","id":"coordinates-of-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.12 Coordinates of variables","text":"","code":"#>                                  Dim.1     Dim.2      Dim.3\n#> Biostatistics -0.866025403784438041477 0.3535534  0.3535534\n#> Economics      0.866025403784439040678 0.3535534  0.3535534\n#> English        0.000000000000001047394 0.9659258 -0.2588190"},{"path":"pca.html","id":"quality-of-representation-of-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.13 Quality of representation of variables","text":"Biostatistics Economics well represented first dimension (75%), English well represented second axis (93%). first plane (Dim.1 Dim.2) Biostatistics Economics well represented (75%+12.5%=87.5%), English\nrepresented first plane (93%+0%=93%)","code":"#>                                                Dim.1\n#> Biostatistics 0.749999999999999000799277837359113619\n#> Economics     0.750000000000000888178419700125232339\n#> English       0.000000000000000000000000000001097035\n#>                   Dim.2     Dim.3\n#> Biostatistics 0.1250000 0.1250000\n#> Economics     0.1250000 0.1250000\n#> English       0.9330127 0.0669873"},{"path":"pca.html","id":"contributions-of-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.14 Contributions of variables","text":"Biostatistics Economics contribute construction first dimension (50%), English contribute highy construction second axis (78.8%). first plane (Dim.1 Dim.2) contribution Biostatistics Economics 50%+10.5%=65.5%, English 78.8%.Description first dimension","code":"#>                                                Dim.1\n#> Biostatistics 49.99999999999993605115378159098327160\n#> Economics     50.00000000000005684341886080801486969\n#> English        0.00000000000000000000000000007313567\n#>                  Dim.2    Dim.3\n#> Biostatistics 10.56624 39.43376\n#> Economics     10.56624 39.43376\n#> English       78.86751 21.13249#> \n#> Link between the variable and the continuous variables (R-square)\n#> =================================================================================\n#>               correlation    p.value\n#> Economics       0.8660254 0.02572142\n#> Biostatistics  -0.8660254 0.02572142"},{"path":"pca.html","id":"contributions-of-first-two-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.15 Contributions of first two variables","text":"","code":"#>               Dim.1    Dim.2    Dim.3\n#> Biostatistics    50 10.56624 39.43376\n#> Economics        50 10.56624 39.43376"},{"path":"pca.html","id":"representation-of-variables","chapter":"17 ğŸ’» PCA","heading":"17.1.16 Representation of variables","text":"","code":""},{"path":"pca.html","id":"with-the-quality-of-representation","chapter":"17 ğŸ’» PCA","heading":"17.1.17 With the quality of representation","text":"","code":""},{"path":"pca.html","id":"with-the-quality-of-representation-1","chapter":"17 ğŸ’» PCA","heading":"17.1.18 With the quality of representation","text":"","code":""},{"path":"pca.html","id":"variables-with-quality-of-representation-larger-than-0.6","chapter":"17 ğŸ’» PCA","heading":"17.1.19 Variables with quality of representation larger than 0.6","text":"","code":""},{"path":"pca.html","id":"with-contribution","chapter":"17 ğŸ’» PCA","heading":"17.1.20 With contribution","text":"","code":""},{"path":"pca.html","id":"variables-and-individus-with-largest-quality-of-representation","chapter":"17 ğŸ’» PCA","heading":"17.1.21 Variables and individus with largest quality of representation","text":"","code":""},{"path":"pca.html","id":"the-individuals","chapter":"17 ğŸ’» PCA","heading":"17.1.22 The individuals","text":"","code":"#> Principal Component Analysis Results for individuals\n#>  ===================================================\n#>   Name       Description                       \n#> 1 \"$coord\"   \"Coordinates for the individuals\" \n#> 2 \"$cos2\"    \"Cos2 for the individuals\"        \n#> 3 \"$contrib\" \"contributions of the individuals\""},{"path":"pca.html","id":"individuals-coordinates","chapter":"17 ğŸ’» PCA","heading":"17.1.23 Individuals: coordinates","text":"","code":"#>                             Dim.1                    Dim.2\n#> Alex     0.0000000000000002653034 -0.000000000000000941663\n#> Bea     -2.1213203435596423851450  0.000000000000001237395\n#> Claudio -0.0000000000000022171779 -1.538189001320852344890\n#> Damien   0.0000000000000026937746  2.101205251626097503248\n#> Emilie   0.0000000000000001895371 -0.563016250305248155961\n#> Fran     2.1213203435596428292342 -0.000000000000002975084\n#>                              Dim.3\n#> Alex    -0.00000000000000007994654\n#> Bea      0.00000000000000015526499\n#> Claudio -0.79622521701812609684623\n#> Damien  -0.29143865656241157990891\n#> Emilie   1.08766387358053817635550\n#> Fran     0.00000000000000002468473\n#>                             Dim.1                    Dim.2\n#> Alex     0.0000000000000002653034 -0.000000000000000941663\n#> Bea     -2.1213203435596423851450  0.000000000000001237395\n#> Claudio -0.0000000000000022171779 -1.538189001320852344890\n#> Damien   0.0000000000000026937746  2.101205251626097503248\n#> Emilie   0.0000000000000001895371 -0.563016250305248155961\n#> Fran     2.1213203435596428292342 -0.000000000000002975084\n#>                              Dim.3\n#> Alex    -0.00000000000000007994654\n#> Bea      0.00000000000000015526499\n#> Claudio -0.79622521701812609684623\n#> Damien  -0.29143865656241157990891\n#> Emilie   1.08766387358053817635550\n#> Fran     0.00000000000000002468473"},{"path":"pca.html","id":"quality-of-representation","chapter":"17 ğŸ’» PCA","heading":"17.1.24 Quality of representation","text":"","code":"#>                                            Dim.1\n#> Alex    0.07137976857538211317155685264879139140\n#> Bea     1.00000000000000022204460492503130808473\n#> Claudio 0.00000000000000000000000000000163862588\n#> Damien  0.00000000000000000000000000000161253810\n#> Emilie  0.00000000000000000000000000000002394954\n#> Fran    0.99999999999999977795539507496869191527\n#>                                           Dim.2\n#> Alex    0.8992501752497785716400358069222420454\n#> Bea     0.0000000000000000000000000000003402549\n#> Claudio 0.7886751345948129765517364830884616822\n#> Damien  0.9811252243246878501636842884181533009\n#> Emilie  0.2113248654051877173376539076343760826\n#> Fran    0.0000000000000000000000000000019669168\n#>                                             Dim.3\n#> Alex    0.006481699860810073883510273873298501712\n#> Bea     0.000000000000000000000000000000005357159\n#> Claudio 0.211324865405187134470565979427192360163\n#> Damien  0.018874775675311854933324795524640649091\n#> Emilie  0.788675134594813309618643870635423809290\n#> Fran    0.000000000000000000000000000000000135408"},{"path":"pca.html","id":"contributions","chapter":"17 ğŸ’» PCA","heading":"17.1.25 Contributions","text":"","code":"#>                                            Dim.1\n#> Alex     0.0000000000000000000000000000007820654\n#> Bea     50.0000000000000000000000000000000000000\n#> Claudio  0.0000000000000000000000000000546208628\n#> Damien   0.0000000000000000000000000000806269052\n#> Emilie   0.0000000000000000000000000000003991590\n#> Fran    50.0000000000000213162820728030055761337\n#>                                          Dim.2\n#> Alex     0.00000000000000000000000000001249253\n#> Bea      0.00000000000000000000000000002157130\n#> Claudio 33.33333333333337833437326480634510517\n#> Damien  62.20084679281458051036679535172879696\n#> Emilie   4.46581987385206335972043234505690634\n#> Fran     0.00000000000000000000000000012469753\n#>                                             Dim.3\n#> Alex     0.00000000000000000000000000000033605182\n#> Bea      0.00000000000000000000000000000126751744\n#> Claudio 33.33333333333333570180911920033395290375\n#> Damien   4.46581987385203582618942164117470383644\n#> Emilie  62.20084679281465867006772896274924278259\n#> Fran     0.00000000000000000000000000000003203788"},{"path":"pca.html","id":"contributions-on-the-first-two-axes","chapter":"17 ğŸ’» PCA","heading":"17.1.26 Contributions on the first two axes","text":"","code":""},{"path":"pca.html","id":"representation-of-individuals-with-respect-to-quality-of-projection","chapter":"17 ğŸ’» PCA","heading":"17.1.27 Representation of individuals with respect to quality of projection","text":"","code":""},{"path":"pca.html","id":"save-the-figures-in-pdf","chapter":"17 ğŸ’» PCA","heading":"17.1.28 Save the figures in pdf","text":"","code":""},{"path":"pca.html","id":"save-in-png","chapter":"17 ğŸ’» PCA","heading":"17.1.29 Save in png","text":"","code":""},{"path":"pca.html","id":"another-saving","chapter":"17 ğŸ’» PCA","heading":"17.1.30 Another saving","text":"","code":"#> file saved to PCA.pdf\n#> file saved to PCA.pdf\n#> [1] \"PCA%03d.png\"\n#> file saved to PCA%03d.png"},{"path":"pca.html","id":"export-the-results-in-txt-fimes","chapter":"17 ğŸ’» PCA","heading":"17.1.31 Export the results in txt fimes","text":"","code":""},{"path":"pca.html","id":"pca-with-prcomp-function-of-the-base-r-package","chapter":"17 ğŸ’» PCA","heading":"17.2 PCA with prcomp function of the base R package","text":"perform PCA USArrests, rows data set contain 50 states, alphabetical order.?USArrests give details dataframe\nView(USArrests) see whole dataframeThe columns data set contain four variables:-Murder: Murder arrests (per 100,000)\n-Assault: Assault arrests (per 100,000)\n-UrbanPop: Percent urban population\n-Rape: Rape arrests (per 100,000)Notice variables vastly different means (variables columns), apply() function.\nâ€˜apply(USArrests, 2, mean)â€™ permits calculate means column, option â€˜1â€™ calculation rowThere average three times many rapes murders, eight times many assaults rapes. Let us also examine variances variablesThe variables different variances:\nUrbanPop variable measuring percentage population state living urban area, comparable number number rapes\nstate per 100,000 individuals.\nscale variables performing PCA, principal components observed driven Assault variable, since largest mean variance.\n, important standardize variables mean zero standard deviation one performing PCA.option scale = TRUE, scaling \nvariables standard deviation one.center scale components correspond means standard deviations variables used scaling prior implementing PCA.rotation matrix provides principal component loadings;\ncolumn pr.$rotation contains corresponding\nprincipal component loading vector.see four distinct principal components. \nexpected general \\(\\min(n-1,p)\\) informative\nprincipal components data set \\(n\\) observations \\(p\\)\nvariables.Using pr.$x \\(50 \\times 4\\) matrix x principal component score vectors. , \\(k\\)th column \\(k\\)th principal component score vector.can plot first two principal components follows:scale = 0 argument biplot() ensures arrows scaled represent loadings; values scale give slightly different biplots different interpretations.principal components unique sign change, can reproduce figure making small changes:standard deviation (square root corresponding eigen-value) principal component follows:variance explained principal component (corresponding eigen-value) obtained squaring :Compute proportion variance explained principal component followsWe see first principal component explains \\(62.0 \\%\\) variance data, next principal component explains \\(24.7 \\%\\) variance.\nPlot PVE (Proportion Variance Explained) explained component, cumulative PVE, follows:function cumsum() computes cumulative sum elements numeric vector.","code":"#>  [1] \"Alabama\"        \"Alaska\"         \"Arizona\"       \n#>  [4] \"Arkansas\"       \"California\"     \"Colorado\"      \n#>  [7] \"Connecticut\"    \"Delaware\"       \"Florida\"       \n#> [10] \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n#> [13] \"Illinois\"       \"Indiana\"        \"Iowa\"          \n#> [16] \"Kansas\"         \"Kentucky\"       \"Louisiana\"     \n#> [19] \"Maine\"          \"Maryland\"       \"Massachusetts\" \n#> [22] \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n#> [25] \"Missouri\"       \"Montana\"        \"Nebraska\"      \n#> [28] \"Nevada\"         \"New Hampshire\"  \"New Jersey\"    \n#> [31] \"New Mexico\"     \"New York\"       \"North Carolina\"\n#> [34] \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n#> [37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"  \n#> [40] \"South Carolina\" \"South Dakota\"   \"Tennessee\"     \n#> [43] \"Texas\"          \"Utah\"           \"Vermont\"       \n#> [46] \"Virginia\"       \"Washington\"     \"West Virginia\" \n#> [49] \"Wisconsin\"      \"Wyoming\"#> [1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"#>   Murder  Assault UrbanPop     Rape \n#>    7.788  170.760   65.540   21.232#>     Murder    Assault   UrbanPop       Rape \n#>   18.97047 6945.16571  209.51878   87.72916#> [1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"#>   Murder  Assault UrbanPop     Rape \n#>    7.788  170.760   65.540   21.232\n#>    Murder   Assault  UrbanPop      Rape \n#>  4.355510 83.337661 14.474763  9.366385#>                 PC1        PC2        PC3         PC4\n#> Murder   -0.5358995  0.4181809 -0.3412327  0.64922780\n#> Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748\n#> UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\n#> Rape     -0.5434321 -0.1673186  0.8177779  0.08902432#> [1] 50  4\n#>                   PC1        PC2         PC3          PC4\n#> Alabama    -0.9756604  1.1220012 -0.43980366  0.154696581\n#> Alaska     -1.9305379  1.0624269  2.01950027 -0.434175454\n#> Arizona    -1.7454429 -0.7384595  0.05423025 -0.826264240\n#> Arkansas    0.1399989  1.1085423  0.11342217 -0.180973554\n#> California -2.4986128 -1.5274267  0.59254100 -0.338559240\n#> Colorado   -1.4993407 -0.9776297  1.08400162  0.001450164#> [1] 1.5748783 0.9948694 0.5971291 0.4164494#> [1] 2.4802416 0.9897652 0.3565632 0.1734301#> [1] 0.62006039 0.24744129 0.08914080 0.04335752"},{"path":"pca.html","id":"exercise-1-nutritional-and-marketing-information-on-us-cereals","chapter":"17 ğŸ’» PCA","heading":"17.3 Exercise 1: Nutritional and Marketing Information on US Cereals","text":"Consider UScereal data (65 rows 11 columns, package â€˜MASSâ€™) 1993 ASA Statistical Graphics Exposition taken mandatory F&DA food label. data normalized portion one American cup.","code":"#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select#> Error in PCA(UScereal): \n#> The following variables are not quantitative:  mfr\n#> The following variables are not quantitative:  vitamins"},{"path":"pca.html","id":"exercise-2","chapter":"17 ğŸ’» PCA","heading":"17.4 Exercise 2","text":"Consider NCI cancer cell line microarray data, consists \\(6{,}830\\) gene expression measurements \\(64\\) cancer cell lines.","code":"#> \n#> Attaching package: 'ISLR2'\n#> The following object is masked from 'package:MASS':\n#> \n#>     Boston"},{"path":"pca.html","id":"exercise-3-wine-quality-analysis","chapter":"17 ğŸ’» PCA","heading":"17.4.1 Exercise 3: Wine Quality Analysis","text":"Consider wine dataset available gclus package, contains chemical analyses wines grown region Italy derived three different cultivars. dataset 13 variables 170 observations.","code":"\nlibrary(gclus)\n#> Loading required package: cluster\ndata(wine)"},{"path":"pca.html","id":"tasks","chapter":"17 ğŸ’» PCA","heading":"17.4.1.1 Tasks:","text":"() Perform PCA wine dataset. Remember standardize variables necessary, might different scales.\n\n# Example code snippet\nlibrary(FactoMineR)\nres.pca.wine <- PCA(wine, scale.unit = TRUE, graph = FALSE)() Perform PCA wine dataset. Remember standardize variables necessary, might different scales.(b) Interpret PCA results. Focus understanding chemical properties contribute variance dataset wines cluster cultivar.(b) Interpret PCA results. Focus understanding chemical properties contribute variance dataset wines cluster cultivar.","code":"\n# Example code snippet\nlibrary(FactoMineR)\nres.pca.wine <- PCA(wine, scale.unit = TRUE, graph = FALSE)"},{"path":"pca.html","id":"exercise-4-boston-housing-data-analysis","chapter":"17 ğŸ’» PCA","heading":"17.4.2 Exercise 4: Boston Housing Data Analysis","text":"Consider Boston dataset MASS package, contains information collected U.S Census Service concerning housing area Boston Mass. 506 rows 14 columns.","code":"\nlibrary(MASS)\ndata(Boston)"},{"path":"pca.html","id":"tasks-1","chapter":"17 ğŸ’» PCA","heading":"17.4.2.1 Tasks:","text":"() Conduct PCA Boston housing dataset. performing PCA, assess variables suitable analysis preprocess data accordingly.\n\n# Example code snippet\nlibrary(FactoMineR)\nres.pca.boston <- PCA(Boston, scale.unit = TRUE, graph = FALSE)() Conduct PCA Boston housing dataset. performing PCA, assess variables suitable analysis preprocess data accordingly.(b) Interpret results PCA. Look patterns might indicate relationships different aspects housing data, crime rates, property tax, median value owner-occupied homes.(b) Interpret results PCA. Look patterns might indicate relationships different aspects housing data, crime rates, property tax, median value owner-occupied homes.","code":"\n# Example code snippet\nlibrary(FactoMineR)\nres.pca.boston <- PCA(Boston, scale.unit = TRUE, graph = FALSE)"},{"path":"pca.html","id":"notes-for-solving","chapter":"17 ğŸ’» PCA","heading":"17.4.3 Notes for Solving:","text":"Data Preprocessing: performing PCA, â€™s crucial preprocess data. may include handling missing values, standardizing data, selecting relevant variables.PCA Interpretation: interpreting results, focus eigenvalues, proportion variance explained principal components, loadings variables principal components.Visualization: Use plots like scree plots, biplots, individual component plots aid interpretation, use packages !Contextual Understanding: dataset context. Understanding domain can significantly help interpreting results meaningfully.","code":""},{"path":"poi-reg-ex.html","id":"poi-reg-ex","chapter":"18 ğŸ’» Poisson regr exer","heading":"18 ğŸ’» Poisson regr exer","text":"brief collection exercises concerning Poisson regression stuff! actually (noised changing data) samples exam, dig ! done class lecture, others left ---home exercise.","code":""},{"path":"poi-reg-ex.html","id":"what-about-poisson-regression","chapter":"18 ğŸ’» Poisson regr exer","heading":"18.1 What about Poisson regression","text":"Poisson Regression involves regression models response variable form counts fractional numbers. example, count number births number wins football match series. Also values response variables follow Poisson distribution.general mathematical equation Poisson regression :$$log(y) = _0 + _1x_1 + _2x_2 â€¦ _Nx_N$$basic syntax glm() function Poisson regression :","code":"\n\nglm(formula, data, family)"},{"path":"poi-reg-ex.html","id":"walkthrough","chapter":"18 ğŸ’» Poisson regr exer","heading":"18.2 Walkthrough ğŸš¶","text":"-built data set warpbreaks describes effect wool type (B) tension (low, medium high) number warp breaks per loom. Letâ€™s consider breaks response variable count number breaks. wool type tension taken predictor variables.summary look p-value last column less 0.05 consider impact predictor variable response variable. seen wooltype B tension type M H impact count breaks.","code":"\npoisson_regression <-glm(formula = breaks ~ wool+tension, data = warpbreaks,family = \"poisson\")\n\nsummary(poisson_regression)"},{"path":"poi-reg-ex.html","id":"mixed-exercises-1","chapter":"18 ğŸ’» Poisson regr exer","heading":"18.3 mixed Exercises ğŸ‘¨â€ğŸ’»","text":"class stuck either ask teacher assemble group work ! Collaboration key ğŸ‘¯â€â™‚ï¸!Letâ€™s go ğŸ¬Exercise 18.1  Sweden motor insurance companies apply identical risk arguments classify customers, thus portfolios claims statistics can combined. Committee asked look problem analyzing real influence claims risk arguments compare structure actual tariff.\ndataset motorins contained package faraway. Fit poisson regression Claims dependent others independent variablesAs linear regression logistic regression just specify first model formula, data end link. link actually poisson. patter look familiar . happening models far fall umbrella Generalized Linear Models, .e.Â GLM share common worflow.Answer Question 18.1:Exercise 18.2  Within library faraway, gala dataset records counts numbers species tortoise found 30 Galapagos Islands. relationship number plant species several geographic variables interest.check teh distribution variablw Speciesfit poisson model Species given others.statistically significant \\(\\alpha = 0.05\\)Answer Question 18.2:need load library data.check distribution Speciesin end fit model:looking pvalues see : Endemics, Area Nearest significant ones.Exercise 18.3  fishing data set stats4nr package contains data number fish caught visitors state park. includes following variables:livebait: whether group used live bait (0/1),camper: whether group brought camper visit (0/1),persons: number people group,child: number children group, andcount: number fish caughdo:plot histogram, mean variance count, normally distributed?fit poisson regression count person, child camper, coefficients significant?Answer Question 18.3:install.packages(â€œstats4nrâ€)\ndata(â€œfishingâ€)check variable count:fit model:Exercise 18.4  hypothesize forests become older, canopies become dominated species, limits light availability species growing understory. Data examine hypothesis acquired Hubachek Wilderness Research Center, experimental forest located northern Minnesota (Gill et al.Â 2019). Data collected 36 forest inventory plots least one tree larger 12.7 cm diameter. number species (num_spp), number standing dead trees (num_dead), average height trees plot (ht_m, meters) variables interest.Run code create data set sp_ht use analysisDoes count variable interest num_spp display characteristics? Comment usefulness using Poisson distribution variable?Create histogram num_spp variable comment distribution?Fit two count regression models predicting number species based average height. One model assume Poisson negative model distribution. Based AIC values, model prefer ?Answer Question 18.4:","code":"library(faraway)\ndata(motorins)\n\npoisson_model <- glm(Claims ~ Payment, family = poisson, data = motorins)\n\nsummary(poisson_model)library(faraway)\ndata(\"gala\")hist(gala$Species)poisson_regression_2 = glm(Species~., data = gala, family = \"poisson\")\nsummary(poisson_regression_2)hist(fishing$count)\nmean(fishing$count)\nvar(fishing$count)fish_model = glm(count ~ persons + child + camper, data = fishing, family = \"poisson\")\nsummary(fish_model)sp_ht <- tibble(\n  num_spp = c(4, 3, 2, 4, 2, 4, 2, 3, 2, 2, \n              5, 1, 7, 4, 4, 7, 4, 6, 4, 5, \n              3, 5, 5, 3, 7, 2, 5, 5, 3, 5, \n              3, 5, 2, 4, 5, 4),\n  num_dead= c(1, 1, 0, 0, 1, 0, 1, 1, 0, 0, \n              2, 0, 0, 3, 0, 0, 0, 0, 0, 0, \n              0, 0, 0, 0, 1, 0, 1, 0, 0, 0, \n              0, 2, 0, 0, 0, 0),\n  ht_m = c(17.6, 19.8, 26.0, 17.4, 19.8, 20.7, \n           14.7, 17.6, 11.6, 19.4, 12.7, 16.4, \n           12.6, 17.9, 23.2, 14.5, 18.6, 15.0, \n           11.5, 15.1, 13.8, 9.9, 20.2, 7.3, \n           6.2, 11.4, 16.5, 8.7, 9.8, 6.9, \n           18.0, 10.0, 12.8, 13.7, 12.8, 16.7)\n  )"},{"path":"hier-clust-ex.html","id":"hier-clust-ex","chapter":"19 ğŸ’» Hierarchical Clustering","heading":"19 ğŸ’» Hierarchical Clustering","text":"Hierarchical clustering machine learning technique used group data points clusters based similarity. works creating hierarchical tree-like structure clusters, cluster divided smaller clusters data points grouped together. technique useful finding patterns data can used variety applications, customer segmentation, image recognition, anomaly detection.stuck either ask TA team work! Collaboration always key ğŸ‘¯â€â™‚ï¸!Remember also want visualize clusters may think use fviz_cluster() lib factoextra. just simply define observation belogs group taking advantage cutree function. minimal example","code":"\nlibrary(cluster)  # clustering algorithms\nlibrary(factoextra) #\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\ndf <- iris[, 1:4]\ndf <- na.omit(df)\ndf <- scale(df)\n \n# Dissimilarity matrix\nd <- dist(df, method = \"euclidean\")\n\n# Hierarchical clustering using Complete Linkage\nhc1 <- hclust(d, method = \"complete\" )\n\n# Cut tree into 3 groups\nsub_grps <- cutree(hc1, k = 3)\n \n# Visualize the result in a scatter plot\nfviz_cluster(list(data = df, cluster = sub_grps))"},{"path":"hier-clust-ex.html","id":"exercises-4","chapter":"19 ğŸ’» Hierarchical Clustering","heading":"19.1 Exercises ğŸ‘¨â€ğŸ’»","text":"Exercise 19.1  Consider USArrests data os ny default installed R. now perform hierarchical clustering states.compute distance matrix data USArrests.Using hierarchical clustering though funtion hclust complete linkage Euclidean distance, cluster states.Cut dendrogram height results 4 distinct clusters. states belong clusters?â€™s also possible draw dendrogram border around clusters 4. argument border used specify border colors rectangles:Visualise results scatterplotLetâ€™s say interested clustering data USArrests just Murder Rape. results?Remember use hclust.Answer Question 19.1:Exercise 19.2  suppose measure small sample penguins ğŸ§. Measurament stored following dataframe:Fit hierarchical clustering visualize dendogram scatterplotAnswer Question 19.2:first visualise clusters dendogramthen scatter:Exercise 19.3  coleman data set robustbase library lists summary statistics 20 different schools northeast US.six variables measured school include demographic information (percent white-collar fathers) characteristics school (staff salaries per pupil).Perform hierarchical cluster data using explanatory variables.cluster individuals 9 groups many individuals fall largest cluster ?Answer Question 19.3:first visualise clusters dendogramthen scatter:","code":"(dist(USArrests)\n\nset.seed(28)\nhc_complete = hclust(dist(USArrests),method=\"complete\")\nplot(hc_complete)\n\nsub_groups = cutree(hc_complete, k = 4)\n\nrect.hclust(hc_complete, k = 4, border = 2:5)\n\nlibrary(factoextra)\nfviz_cluster(list(data = dist(USArrests), cluster = sub_groups))\n\nlibrary(dplyr)\nus_arrests_simplified = select(USArrests, Murder, Rape)\nhc_complete_simple = hclust(dist(us_arrests_simplified), method=\"complete\")\nplot(hc_complete_simple)\nrect.hclust(hc_complete_simple, k = 4, border = 2:5)penguins_small <- data.frame(\n    depth = c(2.5, 2.7, 3.2, 3.5, 3.6),\n    length = c(5.5, 6.0, 4.5, 5.0, 4.7)\n)penguin_cluster <- hclust(dist(penguins_small), method = \"complete\")\nplot(penguin_cluster)library(factoextra)\npenguins_subgroups = cutree(penguin_cluster, k = 4)\nfviz_cluster(list(data = dist(penguins_small), cluster = penguins_subgroups))penguin_cluster <- hclust(dist(penguins_small), method = \"complete\")\nplot(penguin_cluster)library(factoextra)\npenguins_subgroups = cutree(penguin_cluster, k = 4)\nfviz_cluster(list(data = dist(penguin_cluster), cluster = penguins_subgroups))"},{"path":"k-mean-ex.html","id":"k-mean-ex","chapter":"20 ğŸ’» K-Means Clustering","heading":"20 ğŸ’» K-Means Clustering","text":"K-means clustering unsupervised machine learning algorithm used group data points clusters. one popular clustering algorithms used variety applications, image segmentation, data compression, market segmentation. algorithm works assigning data point closest cluster center, iteratively updating cluster centers minimize distance data points.stuck either ask TA team work! Collaboration always key ğŸ‘¯â€â™‚ï¸! may notice functioning pretty similar hierarchical clsutering, spend much time !Remember also want visualize clusters may think use fviz_cluster() lib factoextra. can also use ggplot2 package calling autoplot function kmean regression. going tp use dataset USArrests. scaling data beacause donâ€™t want clustering algorithm depend arbitrary variable unit, start scaling/standardizing data using R function. /done also hierarchical cluster. Please note also want consistently reproduce results across need set seed specific number set.seed(28)Lets also see clusters change wrt number \\(k\\)s","code":"\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nscale_USArrests <- scale(USArrests)\nk2 <- kmeans(scale_USArrests, centers = 2, nstart = 25)\nfviz_cluster(k2, data = scale_USArrests)\nset.seed(28)\nk3 <- kmeans(scale_USArrests, centers = 3, nstart = 25)\nk4 <- kmeans(scale_USArrests, centers = 4, nstart = 25)\nk5 <- kmeans(scale_USArrests, centers = 5, nstart = 25)\n\n# plots to compare\np1 <- fviz_cluster(k2, geom = \"point\", data = scale_USArrests) + ggtitle(\"k = 2\")\np2 <- fviz_cluster(k3, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 3\")\np3 <- fviz_cluster(k4, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 4\")\np4 <- fviz_cluster(k5, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 5\")\n\nlibrary(gridExtra)\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\ngrid.arrange(p1, p2, p3, p4, nrow = 2)"},{"path":"k-mean-ex.html","id":"exercises-5","chapter":"20 ğŸ’» K-Means Clustering","heading":"20.1 Exercises ğŸ‘¨â€ğŸ’»","text":"Exercise 20.1  Letâ€™s look example R using Chatterjee-Price Attitude Data library(datasets) package. dataset survey clerical employees large financial organization. data aggregated questionnaires approximately 35 employees 30 (randomly selected) departments. numbers give percent proportion favourable responses seven questions department. details, see ?attitude. take subset attitude dataset consider two variables K-Means clustering exercise .e.Â privileges learning.visualize privileges learning scatterplot. see cluster?apply algorithm kmeans data \\(k = 2\\) visualize results function fviz_clusternow letâ€™s try \\(k = 3\\), visual inspection , think better?Remember use kmeans base::R.Answer Question 20.1:first select variables data:execute k-means algo:letâ€™s try k = 3","code":"library(dplyr)\nselected_data = select(attitude, privileges, learning)\nplot(selected_data,  pch =20, cex =2)\n  set.seed(28)\nkm1 = kmeans(selected_data, k = 2, nstart=100)\nfviz_cluster(km1, geom = \"point\", data = selected_data)km2 = kmeans(selected_data, k =  3, nstart=100)\nfviz_cluster(km2, geom = \"point\", data = selected_data)"},{"path":"k-mean-ex.html","id":"bonus-elbow-method","chapter":"20 ğŸ’» K-Means Clustering","heading":"20.2 bonus: elbow method âœ¨","text":"main goal cluster partitioning algorithms, k-means clustering, minimize total intra-cluster variation, also known total within-cluster variation total within-cluster sum square.$$minimize(_{k = 1}^{k}W(C_k))\n$$\n\\(C_k\\) \\(k^{th}\\) cluster \\(W(C_k)\\) within-cluster variation. total within-cluster sum square (wss) measures compactness clustering want small possible. Thus, can use following algorithm define optimal clusters:Compute clustering algorithm (e.g., k-means clustering) different values k. instance, varying k 1 10 clustersFor k, calculate total within-cluster sum square (wss)Plot curve wss according number clusters k.location bend (knee) plot generally considered indicator appropriate number clusters.","code":""},{"path":"k-mean-ex.html","id":"code-implementation","chapter":"20 ğŸ’» K-Means Clustering","heading":"20.2.1 code implementation","text":"Data beginning","code":"\n\nfviz_nbclust(scale_USArrests, kmeans, method = \"wss\")"},{"path":"reg-tree-ex.html","id":"reg-tree-ex","chapter":"21 ğŸ’» Regression Trees","heading":"21 ğŸ’» Regression Trees","text":"Regression trees type machine learning algorithm can used predict value target variable based values variables. algorithm works splitting data smaller smaller subsets, subset containing data points similar values target variable. , algorithm can build tree-like structure can used make predictions target variable.advantages using regression trees include ability handle non-linear relationships variables, ability handle large datasets, ability provide interpretable results. main disadvantage tend overfit data, meaning may generalize well unseen data.stuck either ask TA team work! Collaboration always key ğŸ‘¯â€â™‚ï¸! may notice functioning pretty similar hierarchical clsutering, spend much time !","code":""},{"path":"reg-tree-ex.html","id":"train-and-test-splitting","chapter":"21 ğŸ’» Regression Trees","heading":"21.0.1 train and test splitting","text":"methods generally need train test split data. number ways , cover 2: one base::r (without importing package), importing `rsamples``Lets also see clusters change wrt number \\(k\\)s","code":"\nlibrary(rpart)\nlibrary(rpart.plot)  # plotting regression trees\nlibrary(rsample)\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nset.seed(28)\n\nscale_USArrests = scale(USArrests)\n\nk2 <- kmeans(scale_USArrests, centers = 2, nstart = 25)\nk3 <- kmeans(scale_USArrests, centers = 3, nstart = 25)\nk4 <- kmeans(scale_USArrests, centers = 4, nstart = 25)\nk5 <- kmeans(scale_USArrests, centers = 5, nstart = 25)\n\n# plots to compare\np1 <- fviz_cluster(k2, geom = \"point\", data = scale_USArrests) + ggtitle(\"k = 2\")\np2 <- fviz_cluster(k3, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 3\")\np3 <- fviz_cluster(k4, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 4\")\np4 <- fviz_cluster(k5, geom = \"point\",  data = scale_USArrests) + ggtitle(\"k = 5\")\n\nlibrary(gridExtra)\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\ngrid.arrange(p1, p2, p3, p4, nrow = 2)"},{"path":"reg-tree-ex.html","id":"exercises-6","chapter":"21 ğŸ’» Regression Trees","heading":"21.1 Exercises ğŸ‘¨â€ğŸ’»","text":"Exercise 20.1  Letâ€™s look example R using Chatterjee-Price Attitude Data library(datasets) package. dataset survey clerical employees large financial organization. data aggregated questionnaires approximately 35 employees 30 (randomly selected) departments. numbers give percent proportion favourable responses seven questions department. details, see ?attitude. take subset attitude dataset consider two variables K-Means clustering exercise .e.Â privileges learning.visualize privileges learning scatterplot. see cluster?apply algorithm kmeans data \\(k = 2\\) visualize results function fviz_clusternow letâ€™s try \\(k = 3\\), visual inspection , think better?Remember use kmeans base::R.Answer Question 20.1:first select variables data:execute k-means algo:letâ€™s try k = 3","code":"library(dplyr)\nselected_data = select(attitude, privileges, learning)\nplot(selected_data,  pch =20, cex =2)\n  set.seed(28)\nkm1 = kmeans(selected_data, k = 2, nstart=100)\nfviz_cluster(km1, geom = \"point\", data = selected_data)km2 = kmeans(selected_data, k =  3, nstart=100)\nfviz_cluster(km2, geom = \"point\", data = selected_data)"},{"path":"reg-tree-ex.html","id":"bonus-elbow-method-1","chapter":"21 ğŸ’» Regression Trees","heading":"21.2 bonus: elbow method âœ¨","text":"main goal cluster partitioning algorithms, k-means clustering, minimize total intra-cluster variation, also known total within-cluster variation total within-cluster sum square.$$minimize(_{k = 1}^{k}W(C_k))\n$$\n\\(C_k\\) \\(k^{th}\\) cluster \\(W(C_k)\\) within-cluster variation. total within-cluster sum square (wss) measures compactness clustering want small possible. Thus, can use following algorithm define optimal clusters:Compute clustering algorithm (e.g., k-means clustering) different values k. instance, varying k 1 10 clustersFor k, calculate total within-cluster sum square (wss)Plot curve wss according number clusters k.location bend (knee) plot generally considered indicator appropriate number clusters.","code":""},{"path":"reg-tree-ex.html","id":"code-implementation-1","chapter":"21 ğŸ’» Regression Trees","heading":"21.2.1 code implementation","text":"Data beginning","code":"\n\nfviz_nbclust(scale_USArrests, kmeans, method = \"wss\")"},{"path":"course-performance.html","id":"course-performance","chapter":"22 âš¡ course performance","heading":"22 âš¡ course performance","text":"place can grasp previous colleagues performed past years course. Hopefully inspire better challenge grade expectation.\nData extracted Blackboard carefully anonymized securely stored preserve privacy.","code":""},{"path":"course-performance.html","id":"students","chapter":"22 âš¡ course performance","heading":"22.1 ğŸ“ 20-21 students","text":"","code":""},{"path":"course-performance.html","id":"students-1","chapter":"22 âš¡ course performance","heading":"22.2 ğŸ“ 21-22 students","text":"â€¦ soon analysed â€¦","code":""},{"path":"course-performance.html","id":"students-2","chapter":"22 âš¡ course performance","heading":"22.3 ğŸ“ 22-23 students","text":"â€¦ soon analysed â€¦","code":""},{"path":"how-to-ask-for-help.html","id":"how-to-ask-for-help","chapter":"23 âœ‹ How to ask for help!","heading":"23 âœ‹ How to ask for help!","text":"Number one rule stuck go google directly copy paste error.tutorial taken heavy inspiration wonderful reprex intro curated th {Tidyverse} team. looking walk reprex package resource might fit .space, one can hear scream.Ease use acceptance essential design criteria tidyverseâ€™s packages. â€™re shaking head frustration, â€™s can assist us.","code":""},{"path":"how-to-ask-for-help.html","id":"make-a-reprex","chapter":"23 âœ‹ How to ask for help!","heading":"23.1 Make a reprex","text":"â€™re stuck, first step make reprex, repeatable/reproducible example. reprexâ€™s purpose package faulty code way others may execute feel misery. , perhaps, able offer remedy put suffering.two parts creating reprex:begin, must make code replicable. implies must capture everything, including library() calls creation essential objects. reprex package simplest approach ensure â€™ve done .Second, keep simple possible. Remove anything isnâ€™t directly linked situation. Typically, entails generating lot smaller simpler R object one â€™re dealing real life, even utilizing built-data.sounds like lot work! can , great payoff:Creating good reprex shows root problem 80% time. â€™s remarkable frequently providing self-contained short example helps answer issue.Creating good reprex shows root problem 80% time. â€™s remarkable frequently providing self-contained short example helps answer issue.20% time, caught core problem form others can play . significantly increases chances receiving assistance!20% time, caught core problem form others can play . significantly increases chances receiving assistance!","code":""},{"path":"how-to-ask-for-help.html","id":"reprex-pkg","chapter":"23 âœ‹ How to ask for help!","heading":"23.2 Reprex package","text":"constructing reprex hand, â€™s possible overlook anything prevents code running someone elseâ€™s computer. Use reprex package avoid issue. may implemented part tidyverse separately. .want make reprex, need load reprex package.Write bit code copy clipboard:R Console, type reprex(). â€™ll see preview rendered reprex RStudio.â€™s now clipboard, ready paste , say, GitHub issue. can get reprex addins menu RStudio, making even easier point code choose output type.either case, may ultimately experiment capabilities like adding session information structuring output commented R script. Reprex even uploads figures can simply ask ggplot2 queries. people load reprex upon startup always available.Running reprex() returns error code self-contained. may feel like harsh love, may get narrative straight private way. reprex format also strongly pushes choose smallest dataset required demonstrate problem. Creating successful reprex taught ability, instant feedback provided reprex emphasizes .","code":"\n\n# pick ONE:\n# \n# reprex is one of the (many) packages installed when you install tidyverse\ninstall.packages(\"tidyverse\")\n\n# install reprex by itself\ninstall.packages(\"reprex\")\nlibrary(reprex)\n(y <- 1:4)\nmean(y)\n(y <- 1:4)\n#> [1] 1 2 3 4\nmean(y)\n#> [1] 2.5"},{"path":"how-to-ask-for-help.html","id":"but-where-to-ask","chapter":"23 âœ‹ How to ask for help!","heading":"23.3 But Where to ask?","text":"post reprex suitable topic now â€™ve created one can readily inflict others. several possibilities:community.rstudio.com: friendly inviting location ask questions tidyverse (can also ask questions shiny RStudio !)community.rstudio.com: friendly inviting location ask questions tidyverse (can also ask questions shiny RStudio !)Stack Overflow â€™ve definitely heard Stack Overflow googling: â€™s popular source solutions coding-related problems. might frightening ask question Stack Overflow, â€™ve taken time develop reprex, â€™re far likely get relevant response. Make sure tag question R tidyverse seen relevant people.Stack Overflow â€™ve definitely heard Stack Overflow googling: â€™s popular source solutions coding-related problems. might frightening ask question Stack Overflow, â€™ve taken time develop reprex, â€™re far likely get relevant response. Make sure tag question R tidyverse seen relevant people.Twitter. â€™s difficult share reprex just Twitter 140 characters rarely adequate images donâ€™t let people experiment code. However, Twitter excellent platform sharing link reprex hosted elsewhere. #rstats twitter group highly pleasant active, fantastic community part . Make sure include hashtags #rstats #tidyverse tweet.Twitter. â€™s difficult share reprex just Twitter 140 characters rarely adequate images donâ€™t let people experiment code. However, Twitter excellent platform sharing link reprex hosted elsewhere. #rstats twitter group highly pleasant active, fantastic community part . Make sure include hashtags #rstats #tidyverse tweet.think â€™ve found bug, please follow instructions contributing tidyverse.think â€™ve found bug, please follow instructions contributing tidyverse.","code":""},{"path":"frequently-asked-questions.html","id":"frequently-asked-questions","chapter":"24 â“ Frequently Asked Questions","heading":"24 â“ Frequently Asked Questions","text":"number frequent troubles students get frequently stucked . collection relative answer. generally suggest look online answer just coping pasting error message console Google. respected resource look trouble Stackovverflow.","code":""},{"path":"frequently-asked-questions.html","id":"q-na","chapter":"24 â“ Frequently Asked Questions","heading":"24.1 Q nâ€™A","text":"canâ€™t really install  via install.packages(\"<package>\"), gives weird stuff cant undestrand.check syntax? spell correctly package name, capital letters matters know ?\nstill problem? try install source, cases packages source code lies GitHub. Look package author name : library(devtools) (still already installed package devtools run install.packages(\"devtools) execute library(devtools)). point run install_github(\"<github package author username>/<package name>\"). mentioned give fro granted: PLEASE SURE INTERNET CONNECTION ACCESS otherwise going dowload anything.file local inside R project execute read.csv(\"<path file>\") says R cant find files, â€™s ?likely problem Working Directory ( check Working Directory means 4.3). Load library() (still already installed package , execute install.packages(\"\") library(devtools)). point run () verify file lies directory output.lectures recorded?\n> Generally Yes, lectures recorded made available enrolled students. might happen sometimes teacher forget start registration, notice please raise hand say !lectures recorded?\n> Generally Yes, lectures recorded made available enrolled students. might happen sometimes teacher forget start registration, notice please raise hand say !videos made available publicly?\n> canâ€™t , sorry internal policyWill videos made available publicly?\n> canâ€™t , sorry internal policyIs attendance mandatory?\n> wonâ€™t taking attendance expect see often class. love talking students understand , make sure get class, get feedback improve materials. class relatively small probably get know well.\ntime conflict canâ€™t attend online lectures, please send us email let us know!attendance mandatory?\n> wonâ€™t taking attendance expect see often class. love talking students understand , make sure get class, get feedback improve materials. class relatively small probably get know well.\ntime conflict canâ€™t attend online lectures, please send us email let us know!format class?\n> lectures, laboratories, discussion. Occasionally depending fast lectures might industry experts giving us tutorials themed speeches.format class?\n> lectures, laboratories, discussion. Occasionally depending fast lectures might industry experts giving us tutorials themed speeches.need know R course?\n> Since R gold standard statistics people, expect tutorials R even though fluency isnâ€™t required, make life much easier course.need know R course?\n> Since R gold standard statistics people, expect tutorials R even though fluency isnâ€™t required, make life much easier course.(group) assignments?\n> still discussing , evemntually make clear one month maxAre (group) assignments?\n> still discussing , evemntually make clear one month maxI question class. best way reach course staff?\n> please email teaching assistant, Dr.Â Niccolo Salvini related R laboratories, case drop email Prof.Â Giuseppe ArbiaI question class. best way reach course staff?\n> please email teaching assistant, Dr.Â Niccolo Salvini related R laboratories, case drop email Prof.Â Giuseppe ArbiaI canâ€™t install package OneTwoSamples, ?\n> something incopatible R version built time current R version. cunfuses donâ€™t bother going use . Instead going use base R hyp testing stats::t.test() make use infer, tidy statistical inferece.canâ€™t install package OneTwoSamples, ?\n> something incopatible R version built time current R version. cunfuses donâ€™t bother going use . Instead going use base R hyp testing stats::t.test() make use infer, tidy statistical inferece.","code":""},{"path":"StatisticsAndParameters.html","id":"StatisticsAndParameters","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25 Symbols, formulas, statistics and parameters","text":"","code":""},{"path":"StatisticsAndParameters.html","id":"symbols-and-standard-errors","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25.1 Symbols and standard errors","text":"\nTable 25.1: sample statistics used estimate population parameters. Empty table cells means studied textbook. dashes means formula given textbook.\n","code":""},{"path":"StatisticsAndParameters.html","id":"confidence-intervals","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25.2 Confidence intervals","text":"Almost confidence intervals form\\[\n    \\text{statistic} \\pm ( \\text{multiplier} \\times \\text{s.e.}(\\text{statistic})).\n\\]Notes:multiplier approximately 2 approximate 95% CI (based 68â€“95â€“99.7 rule).\\(\\text{multiplier} \\times \\text{s.e.}(\\text{statistic})\\) called margin error.Confidence intervals odds ratios slightly different, formula apply odds ratios.\nreason, standard error ORs given.","code":""},{"path":"StatisticsAndParameters.html","id":"hypothesis-testing-1","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25.3 Hypothesis testing","text":"many hypothesis tests, test statistic \\(t\\)-score, form:\\[\n  t = \\frac{\\text{statistic} - \\text{parameter}}{\\text{s.e.}(\\text{statistic})}.\n\\]Notes:Since \\(t\\)-scores little like \\(z\\)-scores, 68â€“95â€“99.7 rule can used approximate \\(P\\)-values.Tests involving odds ratios use \\(t\\)-scores, formula apply tests involving odds ratios.tests involving odds ratios, test statistic\n\\(\\chi^2\\) score \\(t\\)-score.\nreason, standard error ORs given.\\(\\chi^2\\) statistic approximately like \\(z\\)-score value (\\(\\text{df}\\) â€˜degrees freedomâ€™ given software output):\\[\n   \\sqrt{\\frac{\\chi^2}{\\text{df}}}.\n\\]","code":""},{"path":"StatisticsAndParameters.html","id":"other-formulas","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25.4 Other formulas","text":"estimate sample size needed estimating proportion: \\(\\displaystyle n = \\frac{1}{(\\text{Margin error})^2}\\).estimate sample size needed estimating mean: \\(\\displaystyle n = \\left( \\frac{2\\times s}{\\text{Margin error}}\\right)^2\\).calculate \\(z\\)-scores: \\(\\displaystyle z = \\frac{x - \\mu}{\\sigma}\\) , generally, \\(\\displaystyle z = \\frac{\\text{specific value variable} - \\text{mean variable}}{\\text{measure variable's variation}}\\).unstandardizing formula: \\(x = \\mu + (z\\times \\sigma)\\).Notes:sample size calculations, always round sample size found formulas.","code":""},{"path":"StatisticsAndParameters.html","id":"other-symbols-used","chapter":"25 Symbols, formulas, statistics and parameters","heading":"25.5 Other symbols used","text":"\nTable 25.2: symbols used\n","code":""},{"path":"appendixdatasets.html","id":"appendixdatasets","chapter":"26 Datasets","heading":"26 Datasets","text":"following data sets available download. TBDâ€¦","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
