[{"path":"index.html","id":"about","chapter":"1 🫶 About","heading":"1 🫶 About","text":" official course website Statistics & Big data2 2025 - 2026 laboratories. website augments lecture topics provides exercises home class assignments. Additional theory wrt slides textbook part exam, indeed growth hopefully future quicksilver resource recover R proficiency lethargy.","code":""},{"path":"index.html","id":"logistics","chapter":"1 🫶 About","heading":"1.1 🔧 Logistics","text":"Lectures:\nMondays: 14:00 - 17:00 CET\nTuesdays: 10:00 - 13:00 CET\nMondays: 14:00 - 17:00 CETTuesdays: 10:00 - 13:00 CETLocation: Campus Gemelli, Room 20 (optionally remote)Office hours:\nDr. Niccolò Salvini: Available via email questions /lectures\nProf. Sophie Dabo-Niang: Available intensive session (week November 17th)\nDr. Niccolò Salvini: Available via email questions /lecturesProf. Sophie Dabo-Niang: Available intensive session (week November 17th)’s shared drive (slides notebooks extra class)","code":""},{"path":"index.html","id":"team","chapter":"1 🫶 About","heading":"1.2 👥 Team","text":"","code":""},{"path":"index.html","id":"labs-content","chapter":"1 🫶 About","heading":"1.3 🗒 labs’ contents","text":"","code":""},{"path":"index.html","id":"part-1-the-foundations-dr.-niccolò-salvini","chapter":"1 🫶 About","heading":"1.3.1 Part 1: The Foundations (Dr. Niccolò Salvini)","text":"Introduction R ecosystem\nInstall R RStudio\nR tricks research professional life\nData wrangling R\nInstall R RStudioR tricks research professional lifeData wrangling RHypothesis Testing Fundamentals\nAlternative hypothesis testing\ncalculate p-values\nHypothesis testing null hypothesis\nHypothesis testing averages\nAlternative hypothesis testingHow calculate p-valuesHypothesis testing null hypothesisHypothesis testing averagesAnalysis Variance (ANOVA)\nTesting 2 means\nTesting 2 meansChi-Square Tests\nTesting 2 proportions\nTesting 2 proportionsLinear Regression Analysis\nSimple linear regression\nMultiple linear regression\nNonlinear regression\nRegression dummy variables\nSimple linear regressionMultiple linear regressionNonlinear regressionRegression dummy variablesLogistic Regression\nIntroduction logistic regression\nIntroduction logistic regression","code":""},{"path":"index.html","id":"part-2-advanced-modeling-prof.-sophie-dabo-niang","chapter":"1 🫶 About","heading":"1.3.2 Part 2: Advanced Modeling (Prof. Sophie Dabo-Niang)","text":"Factor AnalysisCluster AnalysisDiscrimination & ClassificationBinomial & Multinomial Logistic RegressionKernel MethodsGeneral Additive ModelsOther Supervised Models","code":""},{"path":"index.html","id":"exam","chapter":"1 🫶 About","heading":"1.4 Exam 📝","text":"exam going open closed questions theory practice (coding part). asked provide results sometimes code leading results. can also asked directly provide code solve exercise. exam going take place labs classroom, means going laptop exam. generally don’t provide assignment neither group works. Indeed provide intermediate exams want try .going 2 intermediate sessions exams half whole content course. means:\n- first intermediate: happen typically November Part 1 content (Dr. Salvini’s part)\n- second intermediate: happen January/February Part 2 content (Prof. Dabo-Niang’s part)can take first intermediate take second exam date within winter session, meaning take part 1 Nov part 2 either Jan Feb. can reject intermediates, means take first part, try second perform well, need take full. Grades may undergo review process official particularly low. happened quite often, happen every time.","code":""},{"path":"index.html","id":"suggested-reading","chapter":"1 🫶 About","heading":"1.5 📚 Suggested reading list","text":"going split resources expected level audience:","code":""},{"path":"index.html","id":"minimal-or-0-knowledge-of-r","chapter":"1 🫶 About","heading":"1.5.1 Minimal or 0 knowledge of R","text":"Everitt, B., Hothorn, T. (2011) Introduction Applied Multivariate Analysis R, Springer-VerlagJames, G, Witten, D, Hastie, T Tibshirani, R, (2015) Introduction Statistical Learning, Applications RT. Timbers, T. Campbell, M. Lee Data Science: First Introduction, Jul 2022 online versionWickham, H., Grolemund G. (2018) R Data Science, O’Reilly. Freely available -line https://r4ds..co.nz/index.htmlR non-programmers, Daniel Dauber 2022, free book","code":""},{"path":"index.html","id":"advanced-knowledge-of-r-to-become-a-top-g","chapter":"1 🫶 About","heading":"1.5.2 Advanced knowledge of R to become a top G","text":"Reproducible Medical Research R,Peter D.R. Higgins, MD, PhD, MSc, 2022, free bookFundamentals Wrangling Healthcare Data R, J. Kyle Armstrong 2022, free book AdvancedWickham H. (2015). Advanced r. CRC Press free book","code":""},{"path":"index.html","id":"honorcode","chapter":"1 🫶 About","heading":"1.6 📜 Honor Code","text":"Permissive strict. unsure, please ask course staff!OKAY Pleeease, using ChatGPT derivatives daily basis. understand, ’s awesome enforcing rule home. Don’t exam.OK search, ask public systems ’re studying. Cite resources reference.\nE.g. read paper, cite . ask Quora, include link.OKAY ask someone assignments/projects , monitoring freelancing websites, plethora bots job daily.OK discuss questions classmates. Disclose discussion partners.OKAY blindly copy solutions classmates.OK use existing solutions part projects/assignments. Clarify contributions.OKAY pretend someone’s solution .OK publish final project course (encourage need love help !)OKAY post assignment solutions online.","code":""},{"path":"index.html","id":"qr-code-time","chapter":"1 🫶 About","heading":"1.7 QR code time!","text":"","code":""},{"path":"index.html","id":"intro-colophon","chapter":"1 🫶 About","heading":"1.8 Colophon","text":"book authored using bookdown inside RStudio bs4 theme\nwebsite hosted Netlify, automatically updated Netlify CI.\ncomplete source available GitHub.version book built :","code":"\nlibrary(devtools)\n#> Caricamento del pacchetto richiesto: usethis\nlibrary(roxygen2)\nlibrary(testthat)\n#> \n#> Caricamento pacchetto: 'testthat'\n#> Il seguente oggetto `e mascherato da 'package:devtools':\n#> \n#>     test_file\n#> Il seguente oggetto `e mascherato da 'package:dplyr':\n#> \n#>     matches\ndevtools::session_info()\n#> - Session info -------------------------------------------\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS 15.6.1\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  C\n#>  ctype    C\n#>  tz       Europe/London\n#>  date     2025-10-16\n#>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown)\n#> \n#> - Packages -----------------------------------------------\n#>  package      * version    date (UTC) lib source\n#>  bookdown       0.29       2022-09-12 [1] CRAN (R 4.2.0)\n#>  brio           1.1.3      2021-11-30 [1] CRAN (R 4.2.0)\n#>  bslib          0.5.1      2023-08-11 [1] CRAN (R 4.2.0)\n#>  cachem         1.0.8      2023-05-01 [1] CRAN (R 4.2.0)\n#>  callr          3.7.3      2022-11-02 [1] CRAN (R 4.2.0)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.2.3)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.2.0)\n#>  devtools     * 2.4.5      2022-10-11 [1] CRAN (R 4.2.0)\n#>  dichromat      2.0-0.1    2022-05-02 [1] CRAN (R 4.2.0)\n#>  digest         0.6.33     2023-07-07 [1] CRAN (R 4.2.0)\n#>  downlit        0.4.2      2022-07-05 [1] CRAN (R 4.2.0)\n#>  dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.2.3)\n#>  ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.2.0)\n#>  evaluate       1.0.3      2025-01-10 [1] CRAN (R 4.2.0)\n#>  fansi          1.0.4      2023-01-22 [1] CRAN (R 4.2.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.2.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.2.0)\n#>  fs             1.6.3      2023-07-20 [1] CRAN (R 4.2.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.2.0)\n#>  glue         * 1.6.2      2022-02-24 [1] CRAN (R 4.2.0)\n#>  htmltools      0.5.6.1    2023-10-06 [1] CRAN (R 4.2.0)\n#>  htmlwidgets    1.6.2      2023-03-17 [1] CRAN (R 4.2.0)\n#>  httpuv         1.6.6      2022-09-08 [1] CRAN (R 4.2.0)\n#>  httr           1.4.6      2023-05-08 [1] CRAN (R 4.2.0)\n#>  jquerylib      0.1.4      2021-04-26 [1] CRAN (R 4.2.0)\n#>  jsonlite       1.8.7      2023-06-29 [1] CRAN (R 4.2.0)\n#>  kableExtra   * 1.3.4.9000 2023-06-01 [1] Github (kupietz/kableExtra@3bf9b21)\n#>  knitr        * 1.44       2023-09-11 [1] CRAN (R 4.2.0)\n#>  later          1.3.0      2021-08-18 [1] CRAN (R 4.2.0)\n#>  lifecycle      1.0.3      2022-10-07 [1] CRAN (R 4.2.0)\n#>  lubridate    * 1.9.2      2023-02-10 [1] CRAN (R 4.2.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.2.0)\n#>  memoise        2.0.1      2021-11-26 [1] CRAN (R 4.2.0)\n#>  mime           0.12       2021-09-28 [1] CRAN (R 4.2.0)\n#>  miniUI         0.1.1.1    2018-05-18 [1] CRAN (R 4.2.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.2.0)\n#>  pkgbuild       1.4.2      2023-06-26 [1] CRAN (R 4.2.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.2.0)\n#>  pkgload        1.4.0      2024-06-28 [1] CRAN (R 4.2.0)\n#>  prettyunits    1.1.1      2020-01-24 [1] CRAN (R 4.2.0)\n#>  processx       3.8.4      2024-03-16 [1] CRAN (R 4.2.3)\n#>  profvis        0.3.8      2023-05-02 [1] CRAN (R 4.2.0)\n#>  promises       1.2.0.1    2021-02-11 [1] CRAN (R 4.2.0)\n#>  ps             1.7.5      2023-04-18 [1] CRAN (R 4.2.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.2.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.2.0)\n#>  RColorBrewer   1.1-3      2022-04-03 [1] CRAN (R 4.2.0)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.2.3)\n#>  remotes        2.4.2      2021-11-30 [1] CRAN (R 4.2.0)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.2.3)\n#>  rmarkdown      2.25       2023-09-18 [1] CRAN (R 4.2.0)\n#>  roxygen2     * 7.3.1      2024-01-22 [1] CRAN (R 4.2.3)\n#>  rstudioapi     0.14       2022-08-22 [1] CRAN (R 4.2.0)\n#>  rvest          1.0.3      2022-08-19 [1] CRAN (R 4.2.0)\n#>  sass           0.4.6      2023-05-03 [1] CRAN (R 4.2.0)\n#>  scales         1.4.0      2025-04-24 [1] CRAN (R 4.2.0)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.2.0)\n#>  shiny          1.7.2      2022-07-19 [1] CRAN (R 4.2.0)\n#>  stringi        1.7.12     2023-01-11 [1] CRAN (R 4.2.0)\n#>  stringr        1.5.0      2022-12-02 [1] CRAN (R 4.2.0)\n#>  svglite        2.1.1      2023-01-10 [1] CRAN (R 4.2.0)\n#>  systemfonts    1.0.4      2022-02-11 [1] CRAN (R 4.2.0)\n#>  testthat     * 3.2.1.1    2024-04-14 [1] CRAN (R 4.2.3)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.2.0)\n#>  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.2.0)\n#>  timechange     0.2.0      2023-01-11 [1] CRAN (R 4.2.0)\n#>  urlchecker     1.0.1      2021-11-30 [1] CRAN (R 4.2.0)\n#>  usethis      * 2.1.6      2022-05-25 [1] CRAN (R 4.2.0)\n#>  utf8           1.2.3      2023-01-31 [1] CRAN (R 4.2.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.2.3)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.2.0)\n#>  webexercises * 1.0.0      2021-09-15 [1] CRAN (R 4.2.0)\n#>  webshot        0.5.4      2022-09-26 [1] CRAN (R 4.2.0)\n#>  withr          3.0.2      2024-10-28 [1] CRAN (R 4.2.0)\n#>  xfun           0.40       2023-08-09 [1] CRAN (R 4.2.0)\n#>  xml2           1.3.4      2023-04-27 [1] CRAN (R 4.2.0)\n#>  xtable         1.8-4      2019-04-21 [1] CRAN (R 4.2.0)\n#>  yaml           2.3.7      2023-01-23 [1] CRAN (R 4.2.0)\n#> \n#>  [1] /Users/niccolo/Library/R/arm64/4.2/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n#> \n#> ----------------------------------------------------------"},{"path":"prereq.html","id":"prereq","chapter":"2 ✨Prerequisites","heading":"2 ✨Prerequisites","text":"","code":""},{"path":"prereq.html","id":"setting-up-r-and-rstudio","chapter":"2 ✨Prerequisites","heading":"2.1 Setting up R and RStudio","text":"get started R, need acquire copy. appendix show download R well RStudio, software application makes R easier use. ’ll go downloading R opening first R session. Use menu right hand side page select OS follow correct installation.R RStudio free easy download. feel comfortable interacting videos instead reading please visit  interactive tutorials  guide full R set !","code":""},{"path":"prereq.html","id":"how-to-download-and-install-r","chapter":"2 ✨Prerequisites","heading":"2.2 How to Download and Install R","text":"R maintained international team developers make language available web page Comprehensive R Archive Network .e. CRAN. top web page provides three links downloading R. Follow link describes operating system: Windows, Mac, Linux.","code":""},{"path":"prereq.html","id":"r-in-windows","chapter":"2 ✨Prerequisites","heading":"2.2.1 R in Windows","text":"install R Windows, click “Download R Windows” link. click “base” link. Next, click first link top new page. link say something like “Download R 3.0.3 Windows,” except 3.0.3 replaced current version R. link downloads installer program, installs --date version R Windows. Run program step installation wizard appears. wizard install R program files folders place shortcut Start menu. Note ’ll need appropriate administration privileges install new software machine. (detailed steps)","code":""},{"path":"prereq.html","id":"r-in-mac","chapter":"2 ✨Prerequisites","heading":"2.2.2 R in Mac","text":"also setup, feel free reach mail address something messed .Go www.r-project.org\nFigure 2.1: R mirrors website\nClick CRAN says Download.Click CRAN says Download.Choose server country (work, downloads perform quicker choose country one close ).Choose server country (work, downloads perform quicker choose country one close ).\nFigure 2.2: CRAN mirrors\nSelect operating system computer, example Download R macOS.\nFigure 2.3: OS choices available\nSelect version want install (recommend latest version)\nFigure 2.4: R versions available\nOpen downloaded file follow installation instructions. recommend leaving suggested settings .Binaries Versus SourceR can installed precompiled binaries built source operating system. Windows Mac machines, installing R binaries extremely easy. binary comes preloaded installer. Although can build R source platforms, process much complicated won’t provide much benefit users. Linux systems, opposite true. Precompiled binaries can found systems, much common build R source files installing Linux. download pages CRAN’s website provide information building R source Windows, Mac, Linux platforms.","code":""},{"path":"prereq.html","id":"r-in-linux","chapter":"2 ✨Prerequisites","heading":"2.2.3 R in Linux","text":"R comes preinstalled many Linux systems, ’ll want newest version R date. CRAN website provides files build R source Debian, Redhat, SUSE, Ubuntu systems link “Download R Linux.” Click link follow directory trail version Linux wish install . exact installation procedure vary depending Linux system use. CRAN guides process grouping set source files documentation README files explain install system.32-bit Versus 64-bitR comes 32-bit 64-bit versions. use? cases, won’t matter. versions use 32-bit integers, means compute numbers numerical precision. difference occurs way version manages memory. 64-bit R uses 64-bit memory pointers, 32-bit R uses 32-bit memory pointers. means 64-bit R larger memory space use (search ).\nrule thumb, 32-bit builds R faster 64-bit builds, though always. hand, 64-bit builds can handle larger files data sets fewer memory management problems. either version, maximum allowable vector size tops around 2 billion elements. operating system doesn’t support 64-bit programs, RAM less 4 GB, 32-bit R . Windows Mac installers automatically install versions system supports 64-bit R.","code":""},{"path":"prereq.html","id":"using-r","chapter":"2 ✨Prerequisites","heading":"2.3 Using R","text":"R isn’t program can open start using, like Microsoft Word Internet Explorer. Instead, R computer language, like C, C++, UNIX. use R writing commands R language asking computer interpret . old days, people ran R code UNIX terminal window—hackers movie 1980s. Now almost everyone uses R application called RStudio, recommend , .R UNIXYou can still run R UNIX BASH window (prompt Powershell) typing command:\nR\nopens R interpreter. can work close interpreter running q() finished.","code":""},{"path":"prereq.html","id":"using-rstudio","chapter":"2 ✨Prerequisites","heading":"2.4 Using RStudio","text":"R just ‘beating heart’ R programming, particular user interface. may heard saying stuff like: “R engine car, indeed RStudio car body, ’s true, just don’t need engine don’t car body. say: want buttons click actually ‘see’ , better way RStudio.\nRStudio integrated development environment (IDE) primary tool interact R. software need fun parts , course, follow along examples book.\nmay ask Posit, fair question. Back days Posit, company behind RStudio, actually named RStudio (product). 2023 rebranded Posit also include languages like Python,Howeveeeer install RStudio perform following steps:Go https://posit.co/\nFigure 2.5: Posit.co main page\nGo DOWNLOAD RSTUDIO upper right corner (download R still haven’t).Go DOWNLOAD RSTUDIO upper right corner (download R still haven’t).Select DOWNLOAD RSTUDIO, just left DOWNLOAD RSTUDIO SERVER.Select DOWNLOAD RSTUDIO, just left DOWNLOAD RSTUDIO SERVER.\nFigure 2.6: Choose RStudio version\npage, scroll select Download (download column) corresponding OS (mind different versions OS, say macOS 11.2 macOS 8.3 need different RStudio download installations).\nFigure 2.7: Choose RStudio version\nOpen downloaded file follow installation instructions. , keep default settings much possible.Congratulations, set learn R. now need start RStudio R. course, curious type, nothing shall stop try R without RStudio.","code":""},{"path":"prereq.html","id":"when-you-first-start-rstudio","chapter":"2 ✨Prerequisites","heading":"2.5 When you first start RStudio","text":"start programming away, might want make tweaks settings right away better experience (humble opinion). open Rstudio settings click onRStudio > Preferences press ⌘ + , Mac.RStudio > Preferences press ⌘ + , Mac.RStudio > Tools > Global Options press Ctrl + , work Windows computer.RStudio > Tools > Global Options press Ctrl + , work Windows computer.recommend least make following changes set success right beginning:Already first tab, .e. General > Basic, make one significant changes. Deactivate every option starts Restore. ensure every time start RStudio, begin clean slate. first sight, might sound counter-intuitive restart everything left , essential make projects easily reproducible. Furthermore, work together others, restoring personal settings also ensures programming works across different computers. Therefore, recommend following unticked:\nRestore recently opened project startup,\nRestore previsouly open source documents startup,\nRestore .Rdata workspace startup\nAlready first tab, .e. General > Basic, make one significant changes. Deactivate every option starts Restore. ensure every time start RStudio, begin clean slate. first sight, might sound counter-intuitive restart everything left , essential make projects easily reproducible. Furthermore, work together others, restoring personal settings also ensures programming works across different computers. Therefore, recommend following unticked:Restore recently opened project startup,Restore recently opened project startup,Restore previsouly open source documents startup,Restore previsouly open source documents startup,Restore .Rdata workspace startupRestore .Rdata workspace startup\nFigure 2.8: get RStudio preferences\ntab Workspace, select Never setting Save workspace .RData exit. One might think wise keep intermediary results stored one R session another. However, often found fixing issues due lazy method, code became less reliable , therefore, reproducible. experience, find avoids many headaches.tab Workspace, select Never setting Save workspace .RData exit. One might think wise keep intermediary results stored one R session another. However, often found fixing issues due lazy method, code became less reliable , therefore, reproducible. experience, find avoids many headaches.Code > Editing tab, make sure least first five options ticked, especially Auto-indent code paste. setting save time trying format coding appropriately, making easier read. Indentation primary way making code look readable less like series characters appear almost random.Code > Editing tab, make sure least first five options ticked, especially Auto-indent code paste. setting save time trying format coding appropriately, making easier read. Indentation primary way making code look readable less like series characters appear almost random.\nFigure 2.9: Pimp RStudio IDE\nDisplay tab, might want first three options selected. particular, Highlight selected line helpful , complicated code, helpful see cursor .\nFigure 2.10: Edit RStudio display preferences\ncourse, wish customise workspace , can . visually impactful way alter default appearance RStudio select Appearance pick completely different colour theme. Feel free browse various options see prefer. right wrong . Just make .\nFigure 2.11: get instantly nerd\n","code":""},{"path":"prereq.html","id":"updating-r-and-rstudio","chapter":"2 ✨Prerequisites","heading":"2.6 Updating R and RStudio: Living at the pulse of innovation","text":"strictly something helps become better programmer, advice might come handy avoid turning frustrated programmer. update software, need update R RStudio separately . R RStudio work closely , still constitute separate pieces software. Thus, essential keep mind updating RStudio automatically update R. can become problematic specific tools installed via RStudio (like fancy learning algorithm) might compatible earlier versions R. Also, additional R packages (see Chapter 3) developed developers separate pieces require updating , independently R RStudio.know thinking: already sounds complicated cumbersome. However, rest assured, take look can easily update packages RStudio. Thus, need remember : R needs updated separately everything else.","code":""},{"path":"r-packages.html","id":"r-packages","chapter":"3 📦 R Packages","heading":"3 📦 R Packages","text":"Many R’s useful functions come preloaded start R, reside packages can installed top R. R packages similar libraries C, C++, Javascript, packages Python, gems Ruby. R package bundles together useful functions, help files, data sets. can use functions within R code load package live . Usually contents R package related single type task, package helps solve. R packages let take advantage R’s useful features: large community package writers (many active data scientists) prewritten routines handling many common (exotic) data-science tasks.Base R\nmay hear R users () refer “base R.” base R? just collection R functions gets loaded every time start R. functions provide basics language, don’t load package can use .","code":""},{"path":"r-packages.html","id":"installing-packages","chapter":"3 📦 R Packages","heading":"3.1 Installing Packages","text":"use R package, must first install computer load current R session. easiest way install R package install.packages R function. Open R type following command line:search specified package collection packages hosted CRAN site. R finds package, download libraries folder computer. R can access package future R sessions without reinstalling . Anyone can write R package disseminate like; however, almost R packages published CRAN website. CRAN tests R package publishing . doesn’t eliminate every bug inside package, mean can trust package CRAN run current version R OS.can install multiple packages linking names R’s concatenate function, c. example, install ggplot2, reshape2, dplyr packages, run:first time installing package, R prompt choose online mirror install . Mirrors listed location. downloads quickest select mirror close . want download new package, try Austria mirror first. main CRAN repository, new packages can sometimes take couple days make around mirrors.","code":"\ninstall.packages(\"<package name>\")\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"carData\", \"spdep\"))"},{"path":"r-packages.html","id":"loading-packages","chapter":"3 📦 R Packages","heading":"3.2 Loading Packages","text":"Installing package doesn’t immediately place functions fingertips. just places computer. use R package, next load R session command:Notice quotation marks disappeared. can use like, quotation marks optional library command. (true install.packages command).library make package’s functions, data sets, help files available close current R session. next time begin R session, ’ll reload package library want use , won’t reinstall . install package . , copy package live R library. see packages currently R library, run:library() also shows path actual R library, folder contains R packages. may notice many packages don’t remember installing. R automatically downloads set useful packages first install R.Install packages (almost) anywhereThe devtools R package makes easy install packages locations CRAN website. devtools provides functions like install_github, install_gitorious, install_bitbucket, install_url. work similar install.packages, search new locations R packages. install_github especially useful many R developers provide development versions packages GitHub. development version package contain sneak peek new functions patches may stable bug free CRAN version.R make bother installing loading packages? can imagine R every package came preloaded, large slow program. May 6, 2014, CRAN website hosts 5,511 packages. simpler install load packages want use want use . keeps copy R fast fewer functions help pages search one time. arrangement benefits well. example, possible update copy R package without updating entire copy R.’s best way learn R packages?difficult use R package don’t know exists. go CRAN website click Packages link see list available packages, ’ll wade thousands . Moreover, many R packages things.know package best? R-packages mailing list place start. sends announcements new packages maintains archive old announcements. Blogs aggregate posts R can also provide valuable leads. recommend R-bloggers. RStudio maintains list useful R packages Getting Started section http://support.rstudio.com. Finally, CRAN groups together useful—respected—packages subject area. excellent place learn packages designed area work.","code":"\nlibrary(\"<package name>\")\nlibrary()"},{"path":"r-packages.html","id":"updating-r-and-its-packages","chapter":"3 📦 R Packages","heading":"3.3 Updating R and Its Packages","text":"R Core Development Team continuously hones R language catching bugs, improving performance, updating R work new technologies. result, new versions R released several times year. easiest way stay current R periodically check CRAN website. website updated new release makes release available download. ’ll install new release. process first installed R.Don’t worry ’re interested staying --date R Core’s doings. R changes slightly releases, ’re likely notice differences. However, updating current version R good place start ever encounter bug can’t explain.RStudio also constantly improves product. can acquire newest updates just downloading RStudio.","code":""},{"path":"r-packages.html","id":"r-packages-1","chapter":"3 📦 R Packages","heading":"3.3.1 R Packages","text":"Package authors occasionally release new versions packages add functions, fix bugs, improve performance. update.packages command checks whether current version package installs current version . syntax update.packages follows install.packages. already ggplot2, reshape2, dplyr computer, ’d good idea check updates use :start new R session updating packages. package loaded update , ’ll close R session open new one begin using updated version package.","code":"\nupdate.packages(c(\"ggplot2\", \"dplyr\", \"carData\", \"spdep\"))"},{"path":"nice-warm-up.html","id":"nice-warm-up","chapter":"4 🔥 Nice warm-up","heading":"4 🔥 Nice warm-up","text":"Now going cover basic operations computer science concepts R. Hopefully get really cool starter pack function might reuse throughout R journey.","code":""},{"path":"nice-warm-up.html","id":"starting-your-fresh-new-r-project","chapter":"4 🔥 Nice warm-up","heading":"4.1 Starting your fresh new R project","text":"Every fresh attempt likely pique interest pique emotions. . uncover answers research questions, become knowledgeable consequence. However, likely dislike certain aspects data analysis. Two examples spring mind:Keeping track files generated projectA Keeping track files generated projectB Data manipulationB Data manipulationWhile go deeper detail data manipulation later chapter, ’d like share ideas work helped stay organized , result, less frustrated. following applicable small large research projects, making extremely useful regardless circumstance size project.","code":""},{"path":"nice-warm-up.html","id":"creating-an-r-project","chapter":"4 🔥 Nice warm-up","heading":"4.2 Creating an R Project file","text":"working project, likely create many different files various purposes, especially R Scripts (File > New File > R Script). careful, file stored system’s default location, might want . RStudio allows manage entire project intuitively conveniently R Project files. Using R Project files comes couple perks, example:files create saved location. data, coding, exported charts, reports, one location, don’t maintain files manually. RStudio sets root directory folder project stored.files create saved location. data, coding, exported charts, reports, one location, don’t maintain files manually. RStudio sets root directory folder project stored.wish share project, may sharing entire folder, others can rapidly replicate study assist issue resolution. due fact file paths relative rather absolute.wish share project, may sharing entire folder, others can rapidly replicate study assist issue resolution. due fact file paths relative rather absolute.may utilize GitHub readily backups -called ‘version control’ tools, allows trace changes code time. (btw really crucial work envirnoments, like know dedicated tutorial website git+GitHub+RStudio workflow set slides explain concepts). requirement course can skip . However let clarify : nice--skill whenever collaborating someone. happen job, writing thesis, name .may utilize GitHub readily backups -called ‘version control’ tools, allows trace changes code time. (btw really crucial work envirnoments, like know dedicated tutorial website git+GitHub+RStudio workflow set slides explain concepts). requirement course can skip . However let clarify : nice--skill whenever collaborating someone. happen job, writing thesis, name .time , significant reason make R Project files ease file organization ability readily share co-investigators, supervisor, students.create R Project, need perform following steps:Select File > New Project… menu bar.\nFigure 4.1: Get R project\nSelect New Directory popup window.\nFigure 4.2: New Project Wizard pop menu\nNext, select New Project.\nFigure 4.3: full set project can initialize RStudio IDE\nPick meaningful name project folder, .e. Directory Name. Ensure project folder created right place. can change subdirectory clicking Browse…. Ideally subdirectory place usually store research projects.\nFigure 4.4: RProject specifications\noption Create git repository. relevant already GitHub account wish use version control. now, can happily ignore use GitHub.option Create git repository. relevant already GitHub account wish use version control. now, can happily ignore use GitHub.Lastly, tick Open new session. open R Project new RStudio window.Lastly, tick Open new session. open R Project new RStudio window.\nFigure 4.5: Choose directory name new project\nhappy choices, can click Create Project. open new R Session, can start working project.\nFigure 4.6: new RStudio Session pop just like magic!\nlook carefully, can see RStudio now ‘branded’ project name. top window, see project name, files pane shows root directory files , even console shows top file path project. set manually, recommend , least easy swift work R Projects","code":""},{"path":"nice-warm-up.html","id":"workdir","chapter":"4 🔥 Nice warm-up","heading":"4.3 Working Directory with here","text":"bootstrap RProject way, RStudio going take care many headaches fresher sophmore developer beginning. matter fact time double click RStudio project file (one finishes .RProj) RStudio link directory computer specified creation project, previous case “tidy_tuesday_2021_08_03”. called Working Directory.\ninteresting place R look files attempt load , R save files save . location working directory vary different computers.\nbase (rather vintage) way look working directory.\nunderstrand directory R using working directory, run:However since live 2022 going use convenient package .e. exactly thing prettier intuitively.() going look .RProj file Working Directory exactly placed.","code":"\ngetwd()\n## \"/Users/niccolo/Desktop/r_projects/sbd_22-23\"\ninstall.packages(\"here\")\nlibrary(here)\nhere()\n## \"/Users/niccolo/Desktop/r_projects/sbd_22-23\""},{"path":"nice-warm-up.html","id":"creating-an-r-script","chapter":"4 🔥 Nice warm-up","heading":"4.4 Creating an R Script","text":"Code may easily grow lengthy complicated. result, writing console inconvenient. alternative, may write code R Script. R Script document recognized RStudio R programming code. Non-R Script files, .txt,.rtf, .md, can also opened RStudio, code typed immediately recognized.open create new R script, appear Source pane. window sometimes referred ‘script editor’. R script begins empty file. Good coding etiquette requires us put comment # first line describe file . ’s ‘TidyTuesday’ R Project sample.\nFigure 4.7: Open R Script write \nexamples tutorial made copied pasted R script. However, need install R packages certain code. Let’s give shot following code. plot produced code displays car company provides fuel-efficient vehicles. code copied pasted R script. ’s simple script generates plot, copy paste file, execute .’re probably wondering happened plot. Copying code execute R script. However, required order develop plot. pressed Return ↵, just add new line. Instead, choose code wish run hit Ctrl+Return ↵ (PC) Cmd+Return ↵ (Mac). may also use Run command top source window, keyboard shortcut far convenient. Furthermore, rapidly remember shortcut need utilize frequently. everything order, see following:can see, Honda automobiles appear travel furthest quantity fuel (gallon) vehicles. result, ’re seeking cheap automobiles, now know look .’s worth noting R script editor includes handy features developing code. ’ve undoubtedly noticed part code ’ve pasted blue others green. distinct significance, colors aid making code understandable. default settings, green represents value ““, often represents characters. Syntax highlighting refers automatic coloring programming code.","code":"\n\nlibrary(tidyverse)\n\nmpg %>% \n  ggplot(aes(x = reorder(manufacturer, desc(hwy), FUN = median),\n                   y = hwy,\n                   fill = manufacturer)) +\n  geom_boxplot() +\n  coord_flip() +\n  theme_minimal() +\n  xlab(\"Manufacturer\") +\n  ylab(\"Highway miles per gallon\")"},{"path":"nice-warm-up.html","id":"using-r-markdown","chapter":"4 🔥 Nice warm-up","heading":"4.5 Using R Markdown","text":"lot say R Markdown, ’ll just mention exists highlight one feature persuade use instead plain R scripts: appear Word documents (almost).R Markdown files, name implies, mix R scripts ‘Markdown.’ ‘Markdown’ method composing formatting text documents without use software Microsoft Word. instead write everything plain text. plain text may translated variety document forms, including HTML webpages, PDF files, Word documents. recommend checking R Markdown Cheatsheet learn works. Click File > New File > R Markdown create R Markdown file.R Markdown file inverse R script. default, R script treats everything code, can use language describe code commenting #. ’ve seen previous code examples. R Markdown file, hand, treats everything text requires us declare code. may accomplish injecting ‘code chunks.’ result, using comments # R Markdown files less necessary may write . Another advantage R Markdown files results analysis shown immediately underneath code chunk rather terminal. also sometimes called notebooks since can display code text together, Python equivalent someway exposed Python scripting Jupyter","code":""},{"path":"syllabus.html","id":"syllabus","chapter":"5 🗒️ Syllabus","heading":"5 🗒️ Syllabus","text":"lecture slides, notes, tutorials, assignments posted  drive , feel free jump . please anticipate questions address class, instead drop mail sure something.reasons trouble accessing G Drive, still please contact teaching assistant. One common issue students complain may need authorization access, may forgotten switch open share option. see , knock shoulder!schedule subject change according pace class may schedule lab feel really ready intermediate exam.updated edition course Dr. Niccolò Salvini (Part 1) Prof. Sophie Dabo-Niang (Part 2).","code":""},{"path":"syllabus.html","id":"course-materials","chapter":"5 🗒️ Syllabus","heading":"5.1 Course Materials","text":"course materials available slides/ folder:","code":""},{"path":"syllabus.html","id":"part-1-the-foundations-dr.-niccolò-salvini-1","chapter":"5 🗒️ Syllabus","heading":"5.1.1 Part 1: The Foundations (Dr. Niccolò Salvini)","text":"Slides Dr. Niccolò Salvini:\n- Hypothesis Testing Null Hypothesis\n- Hypothesis Testing Alternative Hypothesis\n- Calculate P-values\n- Hypothesis Testing Averages\n- Exercises Hypothesis Testing\n- Multiple Linear Regression\n- Nonlinear Regression\n- Introduction Linear Regression\n- Introduction Logistic RegressionSlides Prof. Vincenzo Nardelli:\n- Linear Regression","code":""},{"path":"syllabus.html","id":"part-2-advanced-modeling-prof.-sophie-dabo-niang-1","chapter":"5 🗒️ Syllabus","heading":"5.1.2 Part 2: Advanced Modeling (Prof. Sophie Dabo-Niang)","text":"Materials provided intensive session (week November 17th)","code":""},{"path":"syllabus.html","id":"course-structure","chapter":"5 🗒️ Syllabus","heading":"5.2 Course Structure","text":"Part 1: Foundations (Dr. Niccolò Salvini)\n- 5 hours integrated labs regular lectures\n- Mondays: 14:00 - 17:00 CET\n- Tuesdays: 10:00 - 13:00 CETPart 2: Advanced Modeling (Prof. Sophie Dabo-Niang)\n- 5 hours intensive session week November 17thto ready:","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"hypothesis-testing-fundamentals","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6 🧪 Hypothesis Testing Fundamentals","text":"chapter introduces fundamental concepts hypothesis testing, covering alternative hypothesis testing, p-value calculation, hypothesis testing null hypothesis.","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"learning-objectives","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.1 Learning Objectives","text":"end chapter, able :Understand concept hypothesis testingFormulate null alternative hypothesesCalculate interpret p-valuesPerform hypothesis tests averagesMake statistical decisions based test results","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"introduction-to-hypothesis-testing","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.2 Introduction to Hypothesis Testing","text":"Hypothesis testing statistical method used make decisions population parameters based sample data. involves:Formulating hypotheses: Stating null hypothesis (H₀) alternative hypothesis (H₁)Collecting data: Gathering sample data relevant hypothesisCalculating test statistics: Computing appropriate test statisticsMaking decisions: Comparing test statistics critical values p-values","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"null-and-alternative-hypotheses","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.3 Null and Alternative Hypotheses","text":"","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"null-hypothesis-h₀","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.3.1 Null Hypothesis (H₀)","text":"null hypothesis represents status quo claim want test. typically states effect, difference, relationship.Examples:\n- H₀: μ = 50 (population mean equals 50)\n- H₀: μ₁ = μ₂ (two population means equal)\n- H₀: ρ = 0 (correlation variables)","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"alternative-hypothesis-h₁","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.3.2 Alternative Hypothesis (H₁)","text":"alternative hypothesis represents want prove claim ’re testing . can :One-tailed: H₁: μ > 50 H₁: μ < 50Two-tailed: H₁: μ ≠ 50","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"p-values-and-statistical-significance","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.4 P-values and Statistical Significance","text":"","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"what-is-a-p-value","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.4.1 What is a P-value?","text":"p-value probability observing test statistic extreme , extreme , one calculated sample data, assuming null hypothesis true.","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"interpreting-p-values","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.4.2 Interpreting P-values","text":"p < 0.05: Strong evidence H₀ (reject H₀)p < 0.01: strong evidence H₀ (reject H₀)p > 0.05: Weak evidence H₀ (fail reject H₀)","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"common-misconceptions","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.4.3 Common Misconceptions","text":"P-value probability H₀ trueP-value probability H₁ trueP-value probability making Type error","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"hypothesis-testing-on-averages","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.5 Hypothesis Testing on Averages","text":"","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"one-sample-t-test","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.5.1 One-Sample t-test","text":"Used test whether population mean differs specified value.Assumptions:\n- Data normally distributed (large sample size)\n- Observations independent\n- Random samplingTest Statistic::\n- x̄ = sample mean\n- μ₀ = hypothesized population mean\n- s = sample standard deviation\n- n = sample size","code":"t = (x̄ - μ₀) / (s/√n)"},{"path":"hypothesis-testing-fundamentals.html","id":"two-sample-t-test","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.5.2 Two-Sample t-test","text":"Used compare means two groups.Types:\n- Independent samples: Two separate groups\n- Paired samples: subjects measured twice","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"practical-example","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.6 Practical Example","text":"Let’s work practical example using R:","code":"\n# Load required packages\nlibrary(tidyverse)\n\n# Example: Testing if a new teaching method improves test scores\n# H₀: μ_new = μ_old (no difference in means)\n# H₁: μ_new > μ_old (new method is better)\n\n# Sample data\nold_method <- c(65, 70, 68, 72, 69, 71, 67, 73, 70, 68)\nnew_method <- c(72, 75, 78, 74, 76, 79, 73, 77, 75, 74)\n\n# Perform two-sample t-test\nt_test_result <- t.test(new_method, old_method, alternative = \"greater\")\nprint(t_test_result)\n\n# Extract p-value\np_value <- t_test_result$p.value\ncat(\"P-value:\", p_value, \"\\n\")\n\n# Make decision\nif (p_value < 0.05) {\n  cat(\"Reject H₀: New method significantly improves scores\\n\")\n} else {\n  cat(\"Fail to reject H₀: No significant improvement\\n\")\n}"},{"path":"hypothesis-testing-fundamentals.html","id":"type-i-and-type-ii-errors","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.7 Type I and Type II Errors","text":"","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"type-i-error-α","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.7.1 Type I Error (α)","text":"Definition: Rejecting H₀ ’s actually trueProbability: α (significance level, typically 0.05)Consequence: False positive","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"type-ii-error-β","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.7.2 Type II Error (β)","text":"Definition: Failing reject H₀ ’s actually falseProbability: βConsequence: False negative","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"power-1---β","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.7.3 Power (1 - β)","text":"Definition: Probability correctly rejecting H₀ ’s falseGoal: Maximize power controlling Type error","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"best-practices","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.8 Best Practices","text":"State hypotheses clearly collecting dataChoose appropriate significance level (usually α = 0.05)Check assumptions performing testsReport effect sizes along p-valuesAvoid p-hacking (don’t change hypotheses seeing results)Consider multiple comparisons testing many hypotheses","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"summary","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.9 Summary","text":"Hypothesis testing powerful statistical tool making data-driven decisions. Key points remember:Always formulate clear null alternative hypothesesUnderstand p-values represent don’t representConsider statistical practical significanceBe aware Type Type II errorsFollow best practices ensure valid results","code":""},{"path":"hypothesis-testing-fundamentals.html","id":"further-on","chapter":"6 🧪 Hypothesis Testing Fundamentals","heading":"6.10 Further-on","text":"Slides: 02_hypt_testing_null_hypo.pdf, 03_hypt_testing_alternative_hypo.pdf, 04_how_to_calculate_pvalues.pdf, hypt_testing_on_avg.pdf, 05_hypt_testing_exeR.pdfAdditional resources available course drive","code":""},{"path":"linear-regression-analysis.html","id":"linear-regression-analysis","chapter":"7 📈 Linear Regression Analysis","heading":"7 📈 Linear Regression Analysis","text":"chapter covers linear regression analysis, including simple linear regression, multiple linear regression, nonlinear regression techniques.","code":""},{"path":"linear-regression-analysis.html","id":"learning-objectives-1","chapter":"7 📈 Linear Regression Analysis","heading":"7.1 Learning Objectives","text":"end chapter, able :Understand principles linear regressionPerform simple multiple linear regressionInterpret regression coefficients statisticsAssess model fit assumptionsHandle nonlinear relationshipsUse R regression analysis","code":""},{"path":"linear-regression-analysis.html","id":"introduction-to-linear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.2 Introduction to Linear Regression","text":"Linear regression statistical method used model relationship dependent variable (Y) one independent variables (X). assumes linear relationship variables.","code":""},{"path":"linear-regression-analysis.html","id":"simple-linear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.2.1 Simple Linear Regression","text":"Simple linear regression models relationship two variables:Model: Y = β₀ + β₁X + εWhere:\n- Y = dependent variable (response)\n- X = independent variable (predictor)\n- β₀ = intercept\n- β₁ = slope\n- ε = error term","code":""},{"path":"linear-regression-analysis.html","id":"multiple-linear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.2.2 Multiple Linear Regression","text":"Multiple linear regression extends simple regression include multiple predictors:Model: Y = β₀ + β₁X₁ + β₂X₂ + … + βₖXₖ + ε","code":""},{"path":"linear-regression-analysis.html","id":"assumptions-of-linear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.3 Assumptions of Linear Regression","text":"Linearity: relationship X Y linearIndependence: Observations independentHomoscedasticity: Constant variance errorsNormality: Errors normally distributedNo multicollinearity: Independent variables highly correlated","code":""},{"path":"linear-regression-analysis.html","id":"model-evaluation-metrics","chapter":"7 📈 Linear Regression Analysis","heading":"7.4 Model Evaluation Metrics","text":"","code":""},{"path":"linear-regression-analysis.html","id":"r-squared-r²","chapter":"7 📈 Linear Regression Analysis","heading":"7.4.1 R-squared (R²)","text":"Definition: Proportion variance Y explained XRange: 0 1Interpretation: Higher values indicate better fit","code":""},{"path":"linear-regression-analysis.html","id":"adjusted-r-squared","chapter":"7 📈 Linear Regression Analysis","heading":"7.4.2 Adjusted R-squared","text":"Definition: R² adjusted number predictorsUse: Compare models different numbers predictorsFormula: 1 - (1-R²)(n-1)/(n-k-1)","code":""},{"path":"linear-regression-analysis.html","id":"root-mean-square-error-rmse","chapter":"7 📈 Linear Regression Analysis","heading":"7.4.3 Root Mean Square Error (RMSE)","text":"Definition: Standard deviation residualsInterpretation: Lower values indicate better fitUnits: dependent variable","code":""},{"path":"linear-regression-analysis.html","id":"practical-example-simple-linear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.5 Practical Example: Simple Linear Regression","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Create sample data\nset.seed(123)\nn <- 100\nx <- rnorm(n, mean = 50, sd = 10)\ny <- 2 + 0.5 * x + rnorm(n, mean = 0, sd = 5)\n\n# Create data frame\ndata <- data.frame(x = x, y = y)\n\n# Fit simple linear regression\nmodel <- lm(y ~ x, data = data)\n\n# View model summary\nsummary(model)\n\n# Extract key statistics\nmodel_summary <- summary(model)\nr_squared <- model_summary$r.squared\nadj_r_squared <- model_summary$adj.r.squared\np_value <- model_summary$coefficients[2, 4]\n\ncat(\"R-squared:\", round(r_squared, 3), \"\\n\")\ncat(\"Adjusted R-squared:\", round(adj_r_squared, 3), \"\\n\")\ncat(\"P-value:\", round(p_value, 4), \"\\n\")\n\n# Create visualization\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Simple Linear Regression\",\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  theme_minimal()"},{"path":"linear-regression-analysis.html","id":"multiple-linear-regression-1","chapter":"7 📈 Linear Regression Analysis","heading":"7.6 Multiple Linear Regression","text":"","code":""},{"path":"linear-regression-analysis.html","id":"example-with-multiple-predictors","chapter":"7 📈 Linear Regression Analysis","heading":"7.6.1 Example with Multiple Predictors","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(car)  # for VIF calculation\n\n# Create sample data with multiple predictors\nset.seed(123)\nn <- 100\nx1 <- rnorm(n, mean = 50, sd = 10)\nx2 <- rnorm(n, mean = 30, sd = 8)\nx3 <- rnorm(n, mean = 20, sd = 5)\ny <- 10 + 0.3 * x1 + 0.2 * x2 - 0.1 * x3 + rnorm(n, mean = 0, sd = 3)\n\n# Create data frame\ndata <- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y)\n\n# Fit multiple linear regression\nmodel <- lm(y ~ x1 + x2 + x3, data = data)\n\n# View model summary\nsummary(model)\n\n# Check for multicollinearity using VIF\nvif_values <- vif(model)\nprint(\"Variance Inflation Factors:\")\nprint(vif_values)\n\n# Interpretation guidelines:\n# VIF < 5: No multicollinearity concern\n# VIF 5-10: Moderate multicollinearity\n# VIF > 10: High multicollinearity"},{"path":"linear-regression-analysis.html","id":"nonlinear-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.7 Nonlinear Regression","text":"relationship variables linear, can use nonlinear regression techniques.","code":""},{"path":"linear-regression-analysis.html","id":"polynomial-regression","chapter":"7 📈 Linear Regression Analysis","heading":"7.7.1 Polynomial Regression","text":"","code":"\n# Create nonlinear data\nset.seed(123)\nx <- seq(0, 10, length.out = 50)\ny <- 2 + 0.5 * x + 0.1 * x^2 + rnorm(50, mean = 0, sd = 1)\n\n# Create data frame\ndata <- data.frame(x = x, y = y)\n\n# Fit polynomial regression (quadratic)\nmodel_poly <- lm(y ~ x + I(x^2), data = data)\n\n# View model summary\nsummary(model_poly)\n\n# Create visualization\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = TRUE) +\n  labs(\n    title = \"Polynomial Regression (Quadratic)\",\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  theme_minimal()"},{"path":"linear-regression-analysis.html","id":"logarithmic-transformation","chapter":"7 📈 Linear Regression Analysis","heading":"7.7.2 Logarithmic Transformation","text":"","code":"\n# Create exponential data\nset.seed(123)\nx <- seq(1, 10, length.out = 50)\ny <- exp(0.5 + 0.3 * x + rnorm(50, mean = 0, sd = 0.1))\n\n# Create data frame\ndata <- data.frame(x = x, y = y)\n\n# Fit log-transformed model\nmodel_log <- lm(log(y) ~ x, data = data)\n\n# View model summary\nsummary(model_log)\n\n# Create visualization\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_y_log10() +\n  labs(\n    title = \"Log-transformed Regression\",\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y) - Log Scale\"\n  ) +\n  theme_minimal()"},{"path":"linear-regression-analysis.html","id":"model-diagnostics","chapter":"7 📈 Linear Regression Analysis","heading":"7.8 Model Diagnostics","text":"","code":""},{"path":"linear-regression-analysis.html","id":"residual-analysis","chapter":"7 📈 Linear Regression Analysis","heading":"7.8.1 Residual Analysis","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Fit model\nmodel <- lm(y ~ x1 + x2 + x3, data = data)\n\n# Get residuals and fitted values\nmodel_data <- augment(model)\n\n# Residual plots\n# 1. Residuals vs Fitted Values\nggplot(model_data, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\n\n# 2. Q-Q Plot for normality\nggplot(model_data, aes(sample = .resid)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot of Residuals\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n# 3. Scale-Location Plot\nggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Scale-Location Plot\",\n    x = \"Fitted Values\",\n    y = \"√|Standardized Residuals|\"\n  ) +\n  theme_minimal()"},{"path":"linear-regression-analysis.html","id":"best-practices-1","chapter":"7 📈 Linear Regression Analysis","heading":"7.9 Best Practices","text":"Check assumptions interpreting resultsUse appropriate transformations nonlinear relationshipsAvoid overfitting including many predictorsConsider interaction terms theoretically justifiedReport confidence intervals coefficientsValidate models using cross-validation possible","code":""},{"path":"linear-regression-analysis.html","id":"common-pitfalls","chapter":"7 📈 Linear Regression Analysis","heading":"7.10 Common Pitfalls","text":"Correlation vs Causation: Regression doesn’t imply causationExtrapolation: cautious predicting outside data rangeOutliers: Check influential observationsMissing data: Handle missing values appropriatelyModel selection: Use appropriate criteria model comparison","code":""},{"path":"linear-regression-analysis.html","id":"summary-1","chapter":"7 📈 Linear Regression Analysis","heading":"7.11 Summary","text":"Linear regression fundamental statistical technique modeling relationships variables. Key points:Understand assumptions check themUse appropriate metrics evaluate model fitConsider nonlinear relationships necessaryPerform thorough diagnosticsInterpret results carefully avoid common pitfalls","code":""},{"path":"linear-regression-analysis.html","id":"further-on-1","chapter":"7 📈 Linear Regression Analysis","heading":"7.12 Further-on","text":"Slides: linear_regression.pdf, mlt_lin_reg.pdf, nonlinear_regression.pdfAdditional resources available course drive","code":""},{"path":"advanced-statistical-methods.html","id":"advanced-statistical-methods","chapter":"8 🔬 Advanced Statistical Methods","heading":"8 🔬 Advanced Statistical Methods","text":"chapter covers advanced statistical methods including regression dummy variables, logistic regression, factor analysis, cluster analysis.","code":""},{"path":"advanced-statistical-methods.html","id":"learning-objectives-2","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.1 Learning Objectives","text":"end chapter, able :Understand implement regression dummy variablesPerform logistic regression binary outcomesConduct factor analysis reduce dimensionalityApply cluster analysis techniquesInterpret results advanced statistical methodsUse R advanced statistical analysis","code":""},{"path":"advanced-statistical-methods.html","id":"regression-with-dummy-variables","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.2 Regression with Dummy Variables","text":"Dummy variables (also called indicator variables) binary variables (0/1) used represent categorical data regression models.","code":""},{"path":"advanced-statistical-methods.html","id":"creating-dummy-variables","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.2.1 Creating Dummy Variables","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(fastDummies)\n\n# Create sample data with categorical variable\nset.seed(123)\nn <- 100\neducation <- sample(c(\"High School\", \"Bachelor\", \"Master\", \"PhD\"), n, replace = TRUE)\nexperience <- rnorm(n, mean = 5, sd = 2)\nsalary <- 30000 + 5000 * (education == \"Bachelor\") + \n          8000 * (education == \"Master\") + \n          12000 * (education == \"PhD\") + \n          2000 * experience + rnorm(n, mean = 0, sd = 3000)\n\n# Create data frame\ndata <- data.frame(education = education, experience = experience, salary = salary)\n\n# Create dummy variables\ndata_dummy <- dummy_cols(data, select_columns = \"education\", remove_first_dummy = TRUE)\n\n# View the data\nhead(data_dummy)\n\n# Fit regression with dummy variables\nmodel <- lm(salary ~ experience + education_Bachelor + education_Master + education_PhD, \n            data = data_dummy)\n\n# View model summary\nsummary(model)\n\n# Alternative: R automatically creates dummy variables\nmodel_auto <- lm(salary ~ experience + education, data = data)\nsummary(model_auto)"},{"path":"advanced-statistical-methods.html","id":"interpreting-dummy-variable-coefficients","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.2.2 Interpreting Dummy Variable Coefficients","text":"Reference category: category included model (usually first alphabetically)Coefficients: Represent difference reference categoryExample: “High School” reference, coefficient “Bachelor” represents additional salary Bachelor’s degree holders","code":""},{"path":"advanced-statistical-methods.html","id":"logistic-regression","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.3 Logistic Regression","text":"Logistic regression used dependent variable binary (0/1, Yes/, Success/Failure).","code":""},{"path":"advanced-statistical-methods.html","id":"binary-logistic-regression","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.3.1 Binary Logistic Regression","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Create sample data for logistic regression\nset.seed(123)\nn <- 200\nage <- rnorm(n, mean = 35, sd = 10)\nincome <- rnorm(n, mean = 50000, sd = 15000)\neducation_years <- rnorm(n, mean = 16, sd = 3)\n\n# Create binary outcome (loan approval)\nlog_odds <- -2 + 0.05 * age + 0.0001 * income + 0.2 * education_years\nprob <- exp(log_odds) / (1 + exp(log_odds))\nloan_approved <- rbinom(n, 1, prob)\n\n# Create data frame\ndata <- data.frame(age = age, income = income, education_years = education_years, \n                   loan_approved = loan_approved)\n\n# Fit logistic regression\nmodel <- glm(loan_approved ~ age + income + education_years, \n             data = data, family = binomial())\n\n# View model summary\nsummary(model)\n\n# Extract coefficients and odds ratios\ncoef_summary <- tidy(model, exponentiate = TRUE)\nprint(coef_summary)\n\n# Predict probabilities\ndata$predicted_prob <- predict(model, type = \"response\")\n\n# Create visualization\nggplot(data, aes(x = income, y = loan_approved)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE) +\n  labs(\n    title = \"Logistic Regression: Loan Approval vs Income\",\n    x = \"Income\",\n    y = \"Loan Approved (0/1)\"\n  ) +\n  theme_minimal()"},{"path":"advanced-statistical-methods.html","id":"interpreting-logistic-regression-results","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.3.2 Interpreting Logistic Regression Results","text":"Coefficients: Represent change log-oddsOdds Ratios: e^(coefficient) represents multiplicative change oddsProbabilities: Use logistic function convert log-odds probabilities","code":""},{"path":"advanced-statistical-methods.html","id":"factor-analysis","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.4 Factor Analysis","text":"Factor analysis used identify underlying latent factors explain correlations among observed variables.","code":""},{"path":"advanced-statistical-methods.html","id":"exploratory-factor-analysis-efa","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.4.1 Exploratory Factor Analysis (EFA)","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(GPArotation)\n\n# Create sample data with underlying factors\nset.seed(123)\nn <- 300\n\n# Factor 1: Math ability\nmath1 <- rnorm(n, mean = 0, sd = 1)\nmath2 <- 0.8 * math1 + rnorm(n, mean = 0, sd = 0.6)\nmath3 <- 0.7 * math1 + rnorm(n, mean = 0, sd = 0.7)\n\n# Factor 2: Verbal ability\nverbal1 <- rnorm(n, mean = 0, sd = 1)\nverbal2 <- 0.9 * verbal1 + rnorm(n, mean = 0, sd = 0.4)\nverbal3 <- 0.8 * verbal1 + rnorm(n, mean = 0, sd = 0.6)\n\n# Create data frame\ndata <- data.frame(\n  math_test1 = math1,\n  math_test2 = math2,\n  math_test3 = math3,\n  verbal_test1 = verbal1,\n  verbal_test2 = verbal2,\n  verbal_test3 = verbal3\n)\n\n# Perform factor analysis\n# First, check if data is suitable for factor analysis\ncortest.bartlett(data)\n\n# Determine number of factors\nfa.parallel(data, fa = \"fa\", n.iter = 100)\n\n# Perform factor analysis\nfa_result <- fa(data, nfactors = 2, rotate = \"varimax\")\nprint(fa_result)\n\n# Plot factor loadings\nfa.diagram(fa_result)\n\n# Extract factor scores\nfactor_scores <- factor.scores(data, fa_result)\ndata$factor1 <- factor_scores$scores[, 1]\ndata$factor2 <- factor_scores$scores[, 2]\n\n# Visualize factor scores\nggplot(data, aes(x = factor1, y = factor2)) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Factor Scores\",\n    x = \"Factor 1 (Math Ability)\",\n    y = \"Factor 2 (Verbal Ability)\"\n  ) +\n  theme_minimal()"},{"path":"advanced-statistical-methods.html","id":"cluster-analysis","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.5 Cluster Analysis","text":"Cluster analysis groups similar observations together based characteristics.","code":""},{"path":"advanced-statistical-methods.html","id":"k-means-clustering","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.5.1 K-Means Clustering","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(cluster)\nlibrary(factoextra)\n\n# Create sample data with clusters\nset.seed(123)\nn <- 200\n\n# Generate three clusters\ncluster1 <- data.frame(\n  x = rnorm(n/3, mean = 2, sd = 0.5),\n  y = rnorm(n/3, mean = 2, sd = 0.5),\n  cluster = 1\n)\n\ncluster2 <- data.frame(\n  x = rnorm(n/3, mean = 6, sd = 0.5),\n  y = rnorm(n/3, mean = 2, sd = 0.5),\n  cluster = 2\n)\n\ncluster3 <- data.frame(\n  x = rnorm(n/3, mean = 4, sd = 0.5),\n  y = rnorm(n/3, mean = 6, sd = 0.5),\n  cluster = 3\n)\n\n# Combine clusters\ndata <- rbind(cluster1, cluster2, cluster3)\ndata$cluster <- as.factor(data$cluster)\n\n# Perform K-means clustering\nkmeans_result <- kmeans(data[, 1:2], centers = 3, nstart = 25)\ndata$kmeans_cluster <- as.factor(kmeans_result$cluster)\n\n# Visualize clusters\nggplot(data, aes(x = x, y = y, color = cluster)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"True Clusters\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme_minimal()\n\nggplot(data, aes(x = x, y = y, color = kmeans_cluster)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"K-means Clusters\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme_minimal()\n\n# Determine optimal number of clusters\nfviz_nbclust(data[, 1:2], kmeans, method = \"wss\")\nfviz_nbclust(data[, 1:2], kmeans, method = \"silhouette\")"},{"path":"advanced-statistical-methods.html","id":"hierarchical-clustering","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.5.2 Hierarchical Clustering","text":"","code":"\n# Perform hierarchical clustering\ndist_matrix <- dist(data[, 1:2])\nhclust_result <- hclust(dist_matrix, method = \"ward.D2\")\n\n# Plot dendrogram\nplot(hclust_result, main = \"Hierarchical Clustering Dendrogram\")\n\n# Cut tree to get clusters\nhclust_clusters <- cutree(hclust_result, k = 3)\ndata$hclust_cluster <- as.factor(hclust_clusters)\n\n# Visualize hierarchical clusters\nggplot(data, aes(x = x, y = y, color = hclust_cluster)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Hierarchical Clusters\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme_minimal()"},{"path":"advanced-statistical-methods.html","id":"model-selection-and-validation","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.6 Model Selection and Validation","text":"","code":""},{"path":"advanced-statistical-methods.html","id":"cross-validation-for-logistic-regression","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.6.1 Cross-Validation for Logistic Regression","text":"","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(caret)\n\n# Create sample data\nset.seed(123)\nn <- 300\nx1 <- rnorm(n, mean = 0, sd = 1)\nx2 <- rnorm(n, mean = 0, sd = 1)\nx3 <- rnorm(n, mean = 0, sd = 1)\n\n# Create binary outcome\nlog_odds <- -1 + 0.5 * x1 + 0.3 * x2 - 0.2 * x3\nprob <- exp(log_odds) / (1 + exp(log_odds))\ny <- rbinom(n, 1, prob)\n\n# Create data frame\ndata <- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y)\n\n# Set up cross-validation\nctrl <- trainControl(method = \"cv\", number = 10, classProbs = TRUE)\n\n# Train logistic regression model\nmodel <- train(as.factor(y) ~ x1 + x2 + x3, \n               data = data, \n               method = \"glm\", \n               family = \"binomial\",\n               trControl = ctrl)\n\n# View results\nprint(model)\nprint(model$results)"},{"path":"advanced-statistical-methods.html","id":"best-practices-2","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.7 Best Practices","text":"Check assumptions methodUse appropriate sample sizes reliable resultsValidate models using cross-validationInterpret results context research questionConsider multiple methods appropriateDocument decisions rationale","code":""},{"path":"advanced-statistical-methods.html","id":"common-pitfalls-1","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.8 Common Pitfalls","text":"Overfitting: Including many variablesMulticollinearity: Highly correlated predictorsSample size: Insufficient data reliable resultsAssumptions: checking method-specific assumptionsInterpretation: Misunderstanding coefficients results","code":""},{"path":"advanced-statistical-methods.html","id":"summary-2","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.9 Summary","text":"Advanced statistical methods provide powerful tools analyzing complex data. Key points:Choose appropriate methods based research questionsCheck assumptions validate modelsInterpret results carefully contextUse multiple methods appropriateDocument decisions rationale","code":""},{"path":"advanced-statistical-methods.html","id":"references","chapter":"8 🔬 Advanced Statistical Methods","heading":"8.10 References","text":"Slides: Available course driveAdditional resources examples provided classR documentation specific packages used","code":""},{"path":"advanced-modeling-techniques.html","id":"advanced-modeling-techniques","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9 🚀 Advanced Modeling Techniques","text":"chapter covers advanced modeling techniques taught Prof. Sophie Dabo-Niang intensive session. methods extend beyond basic statistical analysis include sophisticated machine learning modeling approaches.","code":""},{"path":"advanced-modeling-techniques.html","id":"learning-objectives-3","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.1 Learning Objectives","text":"end chapter, able :Understand apply factor analysis techniquesPerform cluster analysis data segmentationImplement discrimination classification methodsUse binomial multinomial logistic regressionApply kernel methods non-linear relationshipsWork general additive modelsExplore supervised learning models","code":""},{"path":"advanced-modeling-techniques.html","id":"course-structure-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.2 Course Structure","text":"part course consists 5 hours intensive sessions held week November 17th. sessions designed provide hands-experience advanced modeling techniques build upon foundations covered Part 1.","code":""},{"path":"advanced-modeling-techniques.html","id":"factor-analysis-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.3 Factor Analysis","text":"Factor analysis statistical method used identify underlying latent factors explain correlations among observed variables.","code":""},{"path":"advanced-modeling-techniques.html","id":"key-concepts","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.3.1 Key Concepts","text":"Exploratory Factor Analysis (EFA): Discovering underlying structureConfirmatory Factor Analysis (CFA): Testing hypothesized structuresFactor Loadings: Relationships variables factorsEigenvalues: Amount variance explained factor","code":""},{"path":"advanced-modeling-techniques.html","id":"applications","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.3.2 Applications","text":"Psychometric testingMarket researchSocial science researchData reduction","code":""},{"path":"advanced-modeling-techniques.html","id":"cluster-analysis-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.4 Cluster Analysis","text":"Cluster analysis groups similar observations together based characteristics, without prior knowledge group membership.","code":""},{"path":"advanced-modeling-techniques.html","id":"methods-covered","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.4.1 Methods Covered","text":"K-means clustering: Partitioning data k clustersHierarchical clustering: Building clusters tree-like structureDensity-based clustering: Finding clusters arbitrary shapeModel-based clustering: Using statistical models","code":""},{"path":"advanced-modeling-techniques.html","id":"applications-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.4.2 Applications","text":"Customer segmentationMarket researchImage segmentationGene expression analysis","code":""},{"path":"advanced-modeling-techniques.html","id":"discrimination-classification","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.5 Discrimination & Classification","text":"methods aim classify observations predefined categories based characteristics.","code":""},{"path":"advanced-modeling-techniques.html","id":"techniques","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.5.1 Techniques","text":"Linear Discriminant Analysis (LDA): Linear boundaries classesQuadratic Discriminant Analysis (QDA): Quadratic boundariesNaive Bayes: Probabilistic classificationSupport Vector Machines (SVM): Finding optimal separating hyperplanes","code":""},{"path":"advanced-modeling-techniques.html","id":"logistic-regression-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.6 Logistic Regression","text":"Logistic regression models probability categorical outcomes.","code":""},{"path":"advanced-modeling-techniques.html","id":"types-covered","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.6.1 Types Covered","text":"Binomial Logistic Regression: Binary outcomes (0/1, Yes/)Multinomial Logistic Regression: Multiple categoriesOrdinal Logistic Regression: Ordered categories","code":""},{"path":"advanced-modeling-techniques.html","id":"key-concepts-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.6.2 Key Concepts","text":"Odds Odds Ratios: Interpreting coefficientsMaximum Likelihood Estimation: Parameter estimationModel Diagnostics: Assessing model fitModel Selection: Choosing appropriate predictors","code":""},{"path":"advanced-modeling-techniques.html","id":"kernel-methods","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.7 Kernel Methods","text":"Kernel methods extend linear algorithms handle non-linear relationships mapping data higher-dimensional spaces.","code":""},{"path":"advanced-modeling-techniques.html","id":"applications-2","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.7.1 Applications","text":"Kernel SVM: Non-linear classificationKernel PCA: Non-linear dimensionality reductionKernel Ridge Regression: Non-linear regression","code":""},{"path":"advanced-modeling-techniques.html","id":"general-additive-models-gams","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.8 General Additive Models (GAMs)","text":"GAMs extend linear models allowing non-linear relationships predictors response variable.","code":""},{"path":"advanced-modeling-techniques.html","id":"features","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.8.1 Features","text":"Smooth functions: Flexible non-linear relationshipsAdditive structure: Sum smooth functionsInterpretability: Maintains model interpretabilityFlexibility: Handles various data types","code":""},{"path":"advanced-modeling-techniques.html","id":"other-supervised-models","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.9 Other Supervised Models","text":"Additional supervised learning techniques classification regression.","code":""},{"path":"advanced-modeling-techniques.html","id":"methods-covered-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.9.1 Methods Covered","text":"Random Forest: Ensemble decision treesGradient Boosting: Sequential ensemble methodNeural Networks: Multi-layer perceptronsEnsemble Methods: Combining multiple models","code":""},{"path":"advanced-modeling-techniques.html","id":"practical-implementation","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.10 Practical Implementation","text":"methods implemented using R appropriate packages:","code":"\n# Load required packages for advanced modeling\nlibrary(factoextra)      # Factor analysis\nlibrary(cluster)         # Cluster analysis\nlibrary(MASS)           # LDA, QDA\nlibrary(e1071)          # SVM\nlibrary(mgcv)           # GAMs\nlibrary(randomForest)   # Random Forest\nlibrary(gbm)            # Gradient Boosting\nlibrary(nnet)           # Neural Networks\nlibrary(caret)          # Model training and validation"},{"path":"advanced-modeling-techniques.html","id":"assessment-and-evaluation","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.11 Assessment and Evaluation","text":"","code":""},{"path":"advanced-modeling-techniques.html","id":"model-evaluation-metrics-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.11.1 Model Evaluation Metrics","text":"Classification: Accuracy, Precision, Recall, F1-scoreRegression: RMSE, MAE, R-squaredClustering: Silhouette score, Within-cluster sum squaresCross-validation: Ensuring model generalizability","code":""},{"path":"advanced-modeling-techniques.html","id":"best-practices-3","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.11.2 Best Practices","text":"Data Preprocessing: Handle missing values outliersFeature Selection: Choose relevant predictorsModel Validation: Use cross-validation techniquesHyperparameter Tuning: Optimize model parametersModel Comparison: Compare different approachesInterpretation: Understand communicate results","code":""},{"path":"advanced-modeling-techniques.html","id":"intensive-session-schedule","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.12 Intensive Session Schedule","text":"intensive session cover:Day 1: Factor Analysis Cluster Analysis\n- Morning: Theory concepts\n- Afternoon: Hands-implementationDay 2: Classification Logistic Regression\n- Morning: Discrimination methods\n- Afternoon: Logistic regression applicationsDay 3: Advanced Methods\n- Morning: Kernel methods GAMs\n- Afternoon: Ensemble methods model comparison","code":""},{"path":"advanced-modeling-techniques.html","id":"prerequisites","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.13 Prerequisites","text":"Students familiar :\n- Basic statistical concepts Part 1\n- R programming fundamentals\n- Linear regression concepts\n- Hypothesis testing","code":""},{"path":"advanced-modeling-techniques.html","id":"resources","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.14 Resources","text":"Course slides materials provided intensive sessionAdditional resources available course driveR documentation specific packagesPractice datasets hands-exercises","code":""},{"path":"advanced-modeling-techniques.html","id":"summary-3","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.15 Summary","text":"intensive session provides students advanced modeling techniques essential modern data analysis. focus practical implementation interpretation results, building upon statistical foundations established Part 1 course.","code":""},{"path":"advanced-modeling-techniques.html","id":"references-1","chapter":"9 🚀 Advanced Modeling Techniques","heading":"9.16 References","text":"Slides materials provided Prof. Sophie Dabo-NiangAdditional resources available course driveR documentation advanced modeling packages","code":""},{"path":"int-prep.html","id":"int-prep","chapter":"10 💻 Intermediate preparation","heading":"10 💻 Intermediate preparation","text":"Intermediate test topics include:\n- hypothesis tests means\n- anova\n- simple + multiple regression (adj-R2 + collinearity + dummy vars)\n- logistic regressionBelow find review core concepts bunch exercises way may resemble intermediate ones.","code":""},{"path":"int-prep.html","id":"core-concepts-of-t-test","chapter":"10 💻 Intermediate preparation","heading":"10.1 🪨 core concepts of T test","text":"recall Arbia’s slide 69 focusing test mean steps calculating statistical tests generally :Develop null HypothesisSpecify level Significance \\(\\alpha\\)Collect sample data compute test statistic.pvalues:\n4. Use value test statistic compute p-value\n5. Reject \\(H_0\\) p-value $ < $Notice Z-test really used since almost always know variance \\(\\sigma^2\\) distribution. instead Standard Normal distribution use Student’s T distribution, t-tests. ’s t test stat:\\[\nt = \\frac{(m - \\mu_0)}{S/\\sqrt{n}}\n\\]possible cases dealing :\nFigure 10.1: t.test vademecum\n","code":""},{"path":"int-prep.html","id":"t.test-function","chapter":"10 💻 Intermediate preparation","heading":"10.2 t.test() function","text":"t.test() function may run one two sample t-tests data vectors. function several parameters invoked follows:case, x numeric vector data values, y optional. y included, function one-sample t-test data x; included, function runs two-sample t-test x y.mu (.e. \\(\\mu\\)) argument returns number showing real value mean (difference means two sample test used) null hypothesis. test conducts two-sided t-test default; however, may perform alternative hypothesis changing alternative argument \"greater\" \"less\", depending whether alternative hypothesis mean larger smaller mu. Consider following:…performs one-sample t-test data contained x null hypothesis $ = 25$ alternative $ < 25 $paired argument indicate whether want paired t-test. default set FALSE can set TRUE desire perform paired t-test. paired test considering pre-post treatment test considering couples individuals.two-sample t-test, var.equal option specifies whether assume equal variances. default assumption unequal variance Welsh approximation degrees freedom; however, may change TRUE pool variance.Finally, conf.level parameter specifies degree confidence reported confidence interval \\(\\mu\\) one-sample \\(\\mu_1 - \\mu_2\\) two-sample cases. simple example:use lm() function R linear modeling, can create Markdown document explain usage various arguments. example create conceptual guide RMarkdown format:## Linear Modeling lm() R","code":"\nt.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, \n        paired = FALSE, var.equal = FALSE, conf.level = 0.95)\nt.test(x, alternative = \"less\", mu = 25)"},{"path":"int-prep.html","id":"introduction","chapter":"10 💻 Intermediate preparation","heading":"10.3 Introduction","text":"lm() function R used performing linear regression modeling. Linear regression statistical method models relationship dependent variable one independent variables. document, explore use lm() function various arguments linear modeling.","code":""},{"path":"int-prep.html","id":"basic-usage","chapter":"10 💻 Intermediate preparation","heading":"10.4 Basic Usage","text":"perform linear modeling lm(), need specify formula linear regression model. basic syntax follows:dependent_variable: variable want predict.independent_variable: variable(s) used make prediction.data: data frame containing variables.","code":"\nmodel <- lm(dependent_variable ~ independent_variable, data = your_data)"},{"path":"int-prep.html","id":"arguments","chapter":"10 💻 Intermediate preparation","heading":"10.5 Arguments","text":"","code":""},{"path":"int-prep.html","id":"formula","chapter":"10 💻 Intermediate preparation","heading":"10.5.1 Formula","text":"formula core lm() function specifies relationship dependent independent variables. follows Y ~ X pattern, Y dependent variable, X independent variable. can include multiple independent variables interactions using + *. example:","code":"\nmodel <- lm(y ~ x1 + x2 + x1*x2, data = your_data)"},{"path":"int-prep.html","id":"data","chapter":"10 💻 Intermediate preparation","heading":"10.5.2 Data","text":"data argument point data frame variables located. argument helps R locate variables specified formula.","code":""},{"path":"int-prep.html","id":"subset","chapter":"10 💻 Intermediate preparation","heading":"10.5.3 Subset","text":"can use subset argument specify subset data modeling. useful want focus specific portion dataset.","code":"\nmodel <- lm(y ~ x, data = your_data, subset = condition)"},{"path":"int-prep.html","id":"weight","chapter":"10 💻 Intermediate preparation","heading":"10.5.4 Weight","text":"weight argument allows assign different weights data point. can useful want give importance certain observations.","code":"\nmodel <- lm(y ~ x, data = your_data, weights = weight_variable)"},{"path":"int-prep.html","id":"na.action","chapter":"10 💻 Intermediate preparation","heading":"10.5.5 Na.action","text":"na.action argument controls missing values treated. default, na.action = na.fail, means model fail missing values data. can also use na.omit automatically remove rows missing values.","code":"\nmodel <- lm(y ~ x, data = your_data, na.action = na.omit)"},{"path":"int-prep.html","id":"other-arguments","chapter":"10 💻 Intermediate preparation","heading":"10.5.6 Other Arguments","text":"additional arguments controlling aspects modeling process, offset, method, control. can refer R documentation complete list available arguments descriptions.","code":""},{"path":"int-prep.html","id":"vif-variance-inflation-factor","chapter":"10 💻 Intermediate preparation","heading":"10.6 VIF (Variance Inflation Factor)","text":"Variance Inflation Factor (VIF) measure used detect multicollinearity linear regression model. Multicollinearity occurs independent variables model highly correlated, making challenging determine individual effect variable dependent variable. VIF function regclass package R can help us identify multicollinearity linear regression model.","code":""},{"path":"int-prep.html","id":"calculating-vif","chapter":"10 💻 Intermediate preparation","heading":"10.6.1 Calculating VIF","text":"calculate VIF independent variables model, can use VIF() function. First, make sure regclass package installed loaded:Next, can calculate VIF linear regression model follows:VIF() function takes linear regression model argument returns data frame VIF values independent variable. Higher VIF values indicate stronger multicollinearity, typically threshold 5 10 rule thumb.","code":"\ninstall.packages(\"regclass\")\nlibrary(regclass)\n# Fit a linear regression model\nmodel <- lm(formula  = y ~ x1 + x2 + x3, data = your_data)\n\n# Calculate VIF\nvif_results <- VIF(model)"},{"path":"int-prep.html","id":"interpreting-vif","chapter":"10 💻 Intermediate preparation","heading":"10.6.2 Interpreting VIF","text":"VIF close 1, suggests variable highly correlated independent variables, indicating significant multicollinearity.VIF close 1, suggests variable highly correlated independent variables, indicating significant multicollinearity.VIF greater 1 less chosen threshold (e.g., 5 10), suggests correlation necessarily problematic multicollinearity.VIF greater 1 less chosen threshold (e.g., 5 10), suggests correlation necessarily problematic multicollinearity.VIF significantly greater chosen threshold (e.g., 10), indicates high degree multicollinearity, may need consider removing combining variables address issue.VIF significantly greater chosen threshold (e.g., 10), indicates high degree multicollinearity, may need consider removing combining variables address issue.","code":""},{"path":"int-prep.html","id":"addressing-multicollinearity","chapter":"10 💻 Intermediate preparation","heading":"10.6.3 Addressing Multicollinearity","text":"detect problematic multicollinearity high VIF values, can take several steps address issue:Remove one correlated variables: two variables highly correlated, removing one can often resolve multicollinearity issue.Remove one correlated variables: two variables highly correlated, removing one can often resolve multicollinearity issue.Combine correlated variables: can create new variables combinations highly correlated variables, reducing multicollinearity.Combine correlated variables: can create new variables combinations highly correlated variables, reducing multicollinearity.Collect data: Sometimes, multicollinearity can alleviated collecting data, especially sample size small.Collect data: Sometimes, multicollinearity can alleviated collecting data, especially sample size small.Regularization techniques: Consider using regularization techniques like Ridge Lasso regression, can handle multicollinearity adding penalties coefficients correlated variables.Regularization techniques: Consider using regularization techniques like Ridge Lasso regression, can handle multicollinearity adding penalties coefficients correlated variables.VIF analysis crucial step assessing quality linear regression model ensuring independence independent variables.","code":""},{"path":"int-prep.html","id":"anova-analysis-of-variance","chapter":"10 💻 Intermediate preparation","heading":"10.7 ANOVA (Analysis of Variance)","text":"ANOVA, Analysis Variance, statistical technique used analyze differences among group means dataset. particularly useful want compare means two groups. aov() function R commonly used perform ANOVA.","code":""},{"path":"int-prep.html","id":"performing-anova-with-aov","chapter":"10 💻 Intermediate preparation","heading":"10.7.1 Performing ANOVA with aov()","text":"perform ANOVA using aov() function, need specify formula describes relationship dependent variable grouping factor (categorical variable). basic syntax follows:dependent_variable: continuous variable want analyze.grouping_factor: categorical variable defines groups.data: data frame containing variables.","code":"\nanova_model <- aov(dependent_variable ~ grouping_factor, data = your_data)"},{"path":"int-prep.html","id":"anova-tables","chapter":"10 💻 Intermediate preparation","heading":"10.7.2 ANOVA Tables","text":"’ve created ANOVA model, can obtain ANOVA table using summary() function applied aov object:table provides various statistics, including sum squares, degrees freedom, F-statistic, p-value, allow assess significance differences among group means.","code":"\nsummary(anova_model)"},{"path":"int-prep.html","id":"interpreting-anova","chapter":"10 💻 Intermediate preparation","heading":"10.7.3 Interpreting ANOVA","text":"F-statistic ANOVA table tests whether significant differences among group means. small p-value (< 0.05) suggests significant differences.F-statistic ANOVA table tests whether significant differences among group means. small p-value (< 0.05) suggests significant differences.ANOVA statistically significant, indicates significant differences among group means.ANOVA statistically significant, indicates significant differences among group means.","code":""},{"path":"int-prep.html","id":"assumptions-of-anova","chapter":"10 💻 Intermediate preparation","heading":"10.7.4 Assumptions of ANOVA","text":"ANOVA assumes variances groups equal (homogeneity variances) data normally distributed. Violations assumptions may lead inaccurate results. can check homogeneity variances using tests like Levene’s test Bartlett’s test assess normality data using normal probability plots statistical tests like Shapiro-Wilk test.","code":""},{"path":"int-prep.html","id":"logistic-regression-with-glm","chapter":"10 💻 Intermediate preparation","heading":"10.8 Logistic Regression with glm()","text":"Logistic regression statistical technique used modeling relationship binary dependent variable (0/1, Yes/, True/False) one independent variables. glm() function R commonly used perform logistic regression.","code":""},{"path":"int-prep.html","id":"performing-logistic-regression-with-glm","chapter":"10 💻 Intermediate preparation","heading":"10.8.1 Performing Logistic Regression with glm()","text":"perform logistic regression using glm() function, need specify formula describes relationship binary dependent variable independent variables. basic syntax follows:dependent_variable: binary dependent variable want model.independent_variable1, independent_variable2, etc.: independent variables influence probability binary outcome.family: Specify family argument binomial indicate logistic regression.data: data frame containing variables.","code":"\nlogistic_model <- glm(formula = dependent_variable ~ independent_variable1 + independent_variable2, family = \"binomial\", data = your_data)"},{"path":"int-prep.html","id":"model-summary","chapter":"10 💻 Intermediate preparation","heading":"10.8.2 Model Summary","text":"creating logistic regression model, can obtain summary model’s coefficients, standard errors, z-values, p-values using summary() function applied glm object:summary provides valuable information influence independent variables log-odds binary outcome.","code":"\nsummary(logistic_model)"},{"path":"int-prep.html","id":"interpreting-logistic-regression","chapter":"10 💻 Intermediate preparation","heading":"10.8.3 Interpreting Logistic Regression","text":"coefficients summary indicate direction strength relationship independent variables log-odds binary outcome.coefficients summary indicate direction strength relationship independent variables log-odds binary outcome.Positive coefficients suggest increase log-odds, negative coefficients suggest decrease.Positive coefficients suggest increase log-odds, negative coefficients suggest decrease.odds ratio (exp(coef)) can used interpret change odds binary outcome one-unit change independent variable.odds ratio (exp(coef)) can used interpret change odds binary outcome one-unit change independent variable.significant p-value (< 0.05) coefficient suggests independent variable significant effect binary outcome.significant p-value (< 0.05) coefficient suggests independent variable significant effect binary outcome.null hypothesis logistic regression relationship independent variable binary outcome.null hypothesis logistic regression relationship independent variable binary outcome.","code":""},{"path":"int-prep.html","id":"model-evaluation","chapter":"10 💻 Intermediate preparation","heading":"10.8.4 Model Evaluation","text":"evaluate performance logistic regression model, can assess accuracy, sensitivity, specificity, metrics using techniques like cross-validation ROC analysis. can also plot ROC curve calculate AUC (Area Curve) assess model’s predictive power.","code":""},{"path":"int-prep.html","id":"assumptions-of-logistic-regression","chapter":"10 💻 Intermediate preparation","heading":"10.8.5 Assumptions of Logistic Regression","text":"Logistic regression assumes log-odds binary outcome linear combination independent variables. important check violations assumption, can done residual analysis.","code":""},{"path":"int-prep.html","id":"poisson-regression-with-glm","chapter":"10 💻 Intermediate preparation","heading":"10.9 Poisson Regression with glm()","text":"Poisson regression statistical technique used model relationship count-dependent variable (typically non-negative integers) one independent variables. glm() function R commonly used perform Poisson regression.","code":""},{"path":"int-prep.html","id":"performing-poisson-regression-with-glm","chapter":"10 💻 Intermediate preparation","heading":"10.9.1 Performing Poisson Regression with glm()","text":"perform Poisson regression using glm() function, need specify formula describes relationship count-dependent variable independent variables. basic syntax follows:count_dependent_variable: count-dependent variable want model.independent_variable1, independent_variable2, etc.: independent variables influence count-dependent variable.family: Specify family argument poisson indicate Poisson regression.data: data frame containing variables.","code":"\npoisson_model <- glm(formula = count_dependent_variable ~ independent_variable1 + independent_variable2, family = \"poisson\", data = your_data)"},{"path":"int-prep.html","id":"model-summary-1","chapter":"10 💻 Intermediate preparation","heading":"10.9.2 Model Summary","text":"creating Poisson regression model, can obtain summary model’s coefficients, standard errors, z-values, p-values using summary() function applied glm object:summary provides information influence independent variables expected count dependent variable.","code":"\nsummary(poisson_model)"},{"path":"int-prep.html","id":"interpreting-poisson-regression","chapter":"10 💻 Intermediate preparation","heading":"10.9.3 Interpreting Poisson Regression","text":"coefficients summary indicate direction strength relationship independent variables expected count dependent variable.coefficients summary indicate direction strength relationship independent variables expected count dependent variable.Positive coefficients suggest increase expected count, negative coefficients suggest decrease.Positive coefficients suggest increase expected count, negative coefficients suggest decrease.exponential coefficients (exp(coef)) can used interpret multiplicative effect one-unit change independent variable expected count.exponential coefficients (exp(coef)) can used interpret multiplicative effect one-unit change independent variable expected count.significant p-value (< 0.05) coefficient suggests independent variable significant effect expected count.significant p-value (< 0.05) coefficient suggests independent variable significant effect expected count.null hypothesis Poisson regression relationship independent variable expected count.null hypothesis Poisson regression relationship independent variable expected count.","code":""},{"path":"int-prep.html","id":"class-exercises-do-it-in-groups","chapter":"10 💻 Intermediate preparation","heading":"10.10 class exercises, do it in groups 👯","text":"first guided,Exercise 10.1  state Highway Patrol periodically samples vehicle various location \nparticular roadway. sample vehicle speed used test hypothesis H0\nmean less equal 65The locations \\(H_0\\) rejected deemed best locations radar traps.\nlocation F, sample 64 vehicles shows mean speed 66.2 mph std\ndev 4.2 mph. Use \\(\\alpha  = 0.05\\) test hypothesis.Exercise 10.2  Let’s assume dataset midwest ggplot2 package: contains demographic information midwest counties 2000 US census.\nBesides variables interested percollege describes Percent college educated midwest.test midwest average less national average (.e. *35%) p-value < .02.Exercise 10.3  Download datafile ‘prawnGR.CSV’ Data link save data directory (Remember R projects working directory). Import data R assign variable appropriate name. data collected experiment investigate difference growth rate giant tiger prawn (Penaeus monodon) fed either artificial natural diet.quick look structure dataset.plot growth rate versus diet using appropriate plot.many observations diet treatment?want compare difference growth rate two diets using two sample t-test.Conduct two sample t-test using t.test() using argument var.equal = TRUE perform t-test assuming equal variances. null hypothesis want test? reject fail reject null hypothesis? value t statistic, degrees freedom p value? summarise summary statistics report?Exercise 10.4  new coach hired athletics school, effectiveness new type training evaluated comparing average times 10 centimeters. times seconds athlete’s competition repeated.ùWe two groups trained competitors, measurements taken athletes competition. determine improvement, deterioration, time averages remained essentially constant (.e., H0). Conduct test t student paired changes difference significant 95% confidence level?Exercise 10.5  Following exercise 4 Assume club management, based statistics, fires coach improved hires another promising coach. Following second training session, record athletes’ times:Exercise 10.6  Let’s assume genderweight datarium package, containing weight 40 individuals (20 women 20 men).mean weights male females?test statistically significant 95% confidence levelExercise 10.7  Let’s assume sample data respective belonging group:perform anova test aov() function testing significant differences group 1, 2 3.specify formula, x continous variable y group variable.\nsolve .Exercise 10.8  Assume dataset named PlantGrowth variables weight (dependent variable) group (categorical independent variable).Perform ANOVA analysis compare means weight among different group levels.Check p-value determine whether significant differences among group means.Exercise 10.9  recruit 90 people participate experiment randomly assign 30 people follow either program , program B, program C one month.plot boxplot weight_loss ~ program Hint: use boxplot() function specifying formula.fit 1 way anova test difference weight loss program.Exercise 10.10  Consider maximum size 4 fish 3 populations (n=12). want use model help us examine question whether mean maximum fish size differs among populations.visualize boxplotUsing ANOVA model test whether group means differ another.Exercise 10.11  Let’s consider 6 different insect sprays InsectSprays contained R. Let’s assume interested testing difference number insects found field spraying, use varibales count spray.Exercise 10.12  Let’s consider diet dataset link data set contains information 76 people undertook one three diets (referred diet , B C). background information age, gender, height. aim study see diet best losing weight.read data first dowload link, move data inside R project. run commands:using variable Diet, pre.weight weight6weeksread data kagglecompute mean weights groupcalculate anova Diet weight cutExercise 10.13  Use built-mtcars dataset variables mpg (miles per gallon) vs (engine type: 0 = V-shaped, 1 = straight).Fit logistic regression model predict vs (engine type) based mpg.Interpret coefficients logistic regression model.Exercise 10.14  Use built-mtcars dataset variables mpg (miles per gallon) gear (number forward gears).Fit Poisson regression model predict gear based mpg.Interpret coefficients Poisson regression model.","code":"before_training: =  c(12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3)\nafter_training = c( 12.7, 13.6, 12.0, 15.2, 16.8, 20.0, 12.0, 15.9, 16.0, 11.1)before training: 12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3\nafter training: 12.0, 12.2, 11.2, 13.0, 15.0, 15.8, 12.2, 13.4, 12.9, 11.0x<-c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16)\n\nz<-c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)#make this example reproducible\nset.seed(0)\n\n#create data frame\ndata <- data.frame(program = rep(c(\"A\", \"B\", \"C\"), each = 30),\n                   weight_loss = c(runif(30, 0, 3),\n                                   runif(30, 0, 5),\n                                   runif(30, 1, 7)))\n                                   size <- c(3,4,5,6,4,5,6,7,7,8,9,10)\npop <- c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\")diet = read.csv(\"< the dataset name>.csv\")"},{"path":"int-prep.html","id":"solutions","chapter":"10 💻 Intermediate preparation","heading":"10.11 solutions","text":"Answer Exercise 10.1:Sample random data Normal distribution given mean sd.\ndefine H0 H1. end run hte test.One Sample t-test testing difference x (mean = 65.85)\nmu = 65 suggests effect positive, statistically \nsignificant, small (difference = 0.85, 95% CI [-Inf, 66.81],\nt(63) = 1.47, p = 0.927Answer Exercise 10.7:ANOVA (formula: x ~ z) suggests main effect z statistically significant small\n(F(1, 13) = 0.05, p = 0.832; Eta2 = 3.57e-03, 95% CI [0.00, 1.00]). means group means different\none otherAnswer Exercise 10.12:Assuming ‘plant_growth’ datasetAnswer Exercise 10.13:Convert vs factor variableFit logistic regression modelInterpret coefficientsAnswer Exercise 10.14:Fit Poisson regression modelInterpret coefficients","code":"x <- rnorm(n = 64, mean = 66.2, sd = 4.2)\ntest<-t.test(x, mu = 65, alternative = \"less\")\n\n\nt = 1.469, df = 63, p-value = 0.9266\nalternative hypothesis: true mean is less than 65\n95 percent confidence interval:\n -Inf 66.80805\nsample estimates:\nmean of x \n65.84629 x <- c(12,23,12,13,14,21,23,24,30,21,12,13,14,15,16) \nz <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)\nanova <- aov( x ~ z ) \nsummary(anova)\n\n        Df Sum Sq Mean Sq F value Pr(>F)\nz            1    1.6    1.60   0.047  **0.832**\nResiduals   13  446.1   34.32# 1. Perform ANOVA\nanova_model <- aov(weight ~ group, data = plant_growth)\n\n# 2. Check p-value\nsummary(anova_model)data(mtcars)mtcars$vs <- as.factor(mtcars$vs)logistic_model <- glm(vs ~ mpg, family = binomial, data = mtcars)summary(logistic_model)data(mtcars)poisson_model <- glm(gear ~ mpg, family = poisson, data = mtcars)summary(poisson_model)"},{"path":"int-prep.html","id":"tips","chapter":"10 💻 Intermediate preparation","heading":"10.12 🍬 tips and tricks","text":"Yout might interested standardizing/fornalize say things statistical jargon, report .\nsimply pass test, wether ANOVA t.test object inside report report(). rememer function summary() using linear regression? exactly , ANOVA t tests.data longer fromat different syntax specify t test pretty much follows one linear models .e. lm().\nlet’s look .may something like:exaclty dataset arranged different format. used wider format iut might happen bump longer one. ? ’sa trick , let’s say want test mean statistically different 95% confidence level, instead supplying x y t.test() follow pretty much syntax linear models lm():conclude? conclude : Effect sizes labelled following Cohen’s (1988) recommendations.Welch Two Sample t-test testing difference var group (mean group = 31.00, mean group b = 52.50) suggests effect negative, statistically significant, medium (difference = -21.50, 95% CI [-78.99, 35.99], t(5.92) = -0.92, p = 0.394; Cohen’s d = -0.75, 95% CI [-2.39, 0.94])","code":"\n# install.packages(\"remotes\")\n# remotes::install_github(\"easystats/report\")\nlibrary(report)\n\nx <- rnorm(n = 64, mean = 66.2, sd = 4.2)\ntest<-t.test(x, mu = 65, alternative = \"less\")\nreport(test)\n#> Effect sizes were labelled following Cohen's (1988)\n#> recommendations.\n#> \n#> The One Sample t-test testing the difference between x\n#> (mean = 65.71) and mu = 65 suggests that the effect is\n#> positive, statistically not significant, and very small\n#> (difference = 0.71, 95% CI [-Inf, 66.58], t(63) = 1.38, p =\n#> 0.914; Cohen's d = 0.17, 95% CI [-Inf, 0.38])\nlibrary(tibble)\nlonger = tribble( \n  ~group, ~var,\n  \"a\",   10,\n  \"b\",   24, \n  \"a\",   31,\n  \"a\",   75,\n  \"b\",   26,\n  \"a\",   8,\n  \"b\",   98,\n  \"b\",   62,\n  )\nwider = tribble( \n  ~group_a, ~group_b,\n   10,       24,\n   31,       26,\n   75,       98,\n   8,        62\n  )\n\ntest_for_wider_format = t.test(var~group, data =  longer)\ntest_for_wider_format\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  var by group\n#> t = -0.91809, df = 5.9192, p-value = 0.3944\n#> alternative hypothesis: true difference in means between group a and group b is not equal to 0\n#> 95 percent confidence interval:\n#>  -78.99271  35.99271\n#> sample estimates:\n#> mean in group a mean in group b \n#>            31.0            52.5"},{"path":"int-prep.html","id":"further-exercies","chapter":"10 💻 Intermediate preparation","heading":"10.13 further exercies 🏋️","text":"Exercise 10.15  test scores students intervention. can assess intervention statistically significant effect scores? Specify type test use assumptions involved.Exercise 10.16  Using dataset mtcars, write R command calculate mean standard deviation disp variable grouped cyl.(#exr:mean_diff1) Explain assess mean difference two groups statistically significant, without running R code.Exercise 10.17  Use Boston dataset MASS create histogram variable crim. Report R code used.Exercise 10.18  Describe meaning p-value context hypothesis testing.(#exr:temp_diff1) Given dataset daily temperatures recorded two cities one year, write R code perform hypothesis test determine significant difference mean temperature cities. Specify test used.Exercise 10.19  Given X = c(10, 12, 15, 20) Y = c(11, 14, 16, 18), calculate Pearson correlation coefficient X Y using R.Exercise 10.20  Explain concept Type Type II errors hypothesis testing.Exercise 10.21  Using PlantGrowth dataset R, calculate mean weight level group plot boxplot weight grouped group.Exercise 10.1  Run one-sample t-test test mean hp mtcars different 120. Write R code.Exercise 10.22  Using dataset iris, write R code test significant difference average Sepal.Length setosa versicolor species.Exercise 10.23  Write R function simulate 500 observations Poisson distribution lambda 3 plot histogram.Exercise 10.24  Describe check multicollinearity multiple regression model R.Exercise 10.25  Using airquality dataset, calculate correlation matrix Ozone, Solar.R, Wind, Temp.Exercise 10.26  Perform paired t-test using variables = c(5, 7, 8, 6, 10) = c(6, 8, 9, 7, 12). Report p-value.Exercise 10.13  Using dataset mtcars, perform linear regression mpg dependent variable hp wt independent variables. Report adjusted R-squared.Exercise 10.27  Write R code create density plot variable Sepal.Length species iris dataset.Exercise 10.7  Using iris, perform one-way ANOVA test mean Sepal.Length differs across three species.Exercise 10.28  Define term “confidence interval” context statistical estimation.Exercise 10.29  Using mtcars dataset, refit multiple linear regression model mpg dependent variable hp, wt, drat independent variables. Use stepwise regression iteratively remove insignificant predictors. Report final model significant coefficients.Exercise 10.8  Using dataset ToothGrowth, perform one-way ANOVA function aov() test mean tooth length differs across supplement types doses.Exercise 10.9  recruit 90 people participate experiment randomly assign 30 people follow either program , program B, program C one month.plot boxplot weight_loss ~ program Hint: use boxplot() function specifying formula.fit 1 way anova test difference weight loss program.Answer Exercise 10.15:appropriate test use paired t-test. test used two related samples, measurements individuals, want determine statistically significant difference means. assumptions include differences normally distributed data paired.Answer Exercise 10.16:dplyr package used calculate mean standard deviation disp variable grouped cyl:Answer Exercise @ref(exr:mean_diff1):assess mean difference two groups statistically significant, can use hypothesis test independent t-test. involves setting null alternative hypotheses, calculating test statistic, comparing critical value using p-value determine significance, typically using significance level (e.g., 0.05).Answer Exercise 10.17:create histogram crim variable Boston dataset, use following code:Answer Exercise 10.18:p-value probability obtaining test results least extreme observed results, assumption null hypothesis true. smaller p-value indicates stronger evidence null hypothesis, p-value chosen significance level (e.g., 0.05), reject null hypothesis.Answer Exercise @ref(exr:temp_diff1):appropriate test use two-sample t-test, comparing means two independent groups. R code follows:Answer Exercise 10.19:calculate Pearson correlation coefficient vectors X Y:Answer Exercise 10.20:Type error occurs reject true null hypothesis (false positive), Type II error occurs fail reject false null hypothesis (false negative).Answer Exercise 10.21:calculate mean weight level group plot boxplot weight grouped group:Answer Exercise 10.1:run one-sample t-test test mean hp mtcars different 120:Answer Exercise 10.22:test significant difference average Sepal.Length setosa versicolor species:Answer Exercise 10.23:simulate 500 observations Poisson distribution lambda 3 plot histogram:Answer Exercise 10.24:check multicollinearity, can use Variance Inflation Factor (VIF). VIF value greater 10 indicates high multicollinearity:Answer Exercise 10.25:calculate correlation matrix Ozone, Solar.R, Wind, Temp airquality dataset:Answer Exercise 10.26:perform paired t-test using variables:Answer Exercise 10.13:perform linear regression mpg dependent variable hp wt independent variables, report adjusted R-squared:Answer Exercise 10.27:create density plot variable Sepal.Length species iris dataset:Answer Exercise 10.7:perform one-way ANOVA test mean Sepal.Length differs across three species:Answer Exercise 10.28:confidence interval range values, derived sample statistics, likely contain population parameter specified level confidence (e.g., 95%). provides estimated range expected include true parameter value.Answer Exercise 10.29:refit multiple linear regression model mpg dependent variable hp, wt, drat independent variables, using stepwise regression:Answer Exercise 10.8:perform one-way ANOVA using ToothGrowth dataset test mean tooth length differs across supplement types doses:Answer Exercise 10.9:plot boxplot weight_loss program fit one-way ANOVA test difference weight loss program:","code":"#make this example reproducible\nset.seed(0)\n\n#create data frame\ndata <- data.frame(program = rep(c(\"A\", \"B\", \"C\"), each = 30),\n                   weight_loss = c(runif(30, 0, 3),\n                                   runif(30, 0, 5),\n                                   runif(30, 1, 7)))library(dplyr)\nmtcars %>% group_by(cyl) %>% summarise(mean_disp = mean(disp), sd_disp = sd(disp))library(MASS)\ndata(Boston)\nhist(Boston$crim, main = \"Histogram of crim\", xlab = \"Crime rate per capita\")t.test(temp_city1, temp_city2, var.equal = TRUE)X <- c(10, 12, 15, 20)\nY <- c(11, 14, 16, 18)\ncor(X, Y)data(PlantGrowth)\naggregate(weight ~ group, data = PlantGrowth, mean)\nboxplot(weight ~ group, data = PlantGrowth, main = \"Boxplot of Weight by Group\")t.test(mtcars$hp, mu = 120)t.test(Sepal.Length ~ Species, data = subset(iris, Species %in% c(\"setosa\", \"versicolor\")))set.seed(0)\npoisson_data <- rpois(500, lambda = 3)\nhist(poisson_data, main = \"Histogram of Poisson Distribution\", xlab = \"Values\")library(car)\nvif(model)airquality_subset <- airquality[, c(\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\")]\ncor(airquality_subset, use = \"complete.obs\")before <- c(5, 7, 8, 6, 10)\nafter <- c(6, 8, 9, 7, 12)\nt.test(before, after, paired = TRUE)model <- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model)$adj.r.squaredlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Sepal.Length by Species\")aov_model <- aov(Sepal.Length ~ Species, data = iris)\nsummary(aov_model)library(MASS)\ninitial_model <- lm(mpg ~ hp + wt + drat, data = mtcars)\nstepwise_model <- stepAIC(initial_model, direction = \"both\")\nsummary(stepwise_model)data(ToothGrowth)\naov_model <- aov(len ~ supp * dose, data = ToothGrowth)\nsummary(aov_model)# Plot boxplot\nboxplot(weight_loss ~ program, data = data, main = \"Boxplot of Weight Loss by Program\", xlab = \"Program\", ylab = \"Weight Loss\")\n\n# Fit one-way ANOVA\naov_model <- aov(weight_loss ~ program, data = data)\nsummary(aov_model)"},{"path":"int-samp-q-second.html","id":"int-samp-q-second","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11 📚 Additional Practice Exercises for Intermediate Exam","text":"comprehensive collection practice exercises designed prepare intermediate exam. exercises cover major topics hypothesis testing advanced regression techniques, focus healthcare biomedical data analysis.Instructions: Try solve exercises first. Solutions provided end chapter.","code":""},{"path":"int-samp-q-second.html","id":"part-1-data-wrangling-and-descriptive-statistics","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.1 🏥 Part 1: Data Wrangling and Descriptive Statistics","text":"Exercise 11.1  Load built-dataset airquality remove rows missing values. many complete observations remain?Exercise 11.2  Using mtcars dataset, create new variable called efficiency categorizes cars “High” mpg > 20 “Low” otherwise. many cars “High” efficiency category?Exercise 11.3  Write R command calculate mean, median, standard deviation variable called blood_pressure dataset named health_data.Exercise 11.4  Given healthcare dataset variables patient_id, age, treatment (B), recovery_days, write R command create summary table showing mean recovery days treatment group.","code":""},{"path":"int-samp-q-second.html","id":"part-2-hypothesis-testing---single-population","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.2 🔬 Part 2: Hypothesis Testing - Single Population","text":"Exercise 11.5  hospital claims average waiting time emergency room 45 minutes. random sample 50 patients showed mean waiting time 52 minutes standard deviation 12 minutes. Test α = 0.05 actual waiting time significantly different claimed 45 minutes. Write R command perform test.Exercise 11.6  pharmaceutical company claims 75% patients experience relief headaches within 30 minutes taking new medication. sample 200 patients, 140 reported relief within 30 minutes. Test actual proportion significantly different 75%. Write appropriate R command.Exercise 11.7  Without using formulas, explain difference Type Type II errors hypothesis testing.Exercise 11.8  medical study tests whether average cholesterol level population greater 200 mg/dL. p-value obtained 0.032. significance level 0.05, decision conclusion?","code":""},{"path":"int-samp-q-second.html","id":"part-3-hypothesis-testing---two-populations","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.3 👥 Part 3: Hypothesis Testing - Two Populations","text":"Exercise 11.9  clinical trial compares two different treatments hypertension. Group (n=30) mean blood pressure reduction 15 mmHg (sd=5), Group B (n=35) mean reduction 12 mmHg (sd=6). Assume equal variances. Write R command test significant difference two treatments.Exercise 11.10  hospital testing new training program nurses. 20 nurses took competency test training. data follows:Test training program significantly improved test scores. Write appropriate R command state whether paired unpaired test.Exercise 11.11  Two hospitals comparing patient satisfaction rates. Hospital reports 180 250 patients (72%) satisfied, Hospital B reports 210 280 patients (75%) satisfied. Write R command test significant difference satisfaction rates two hospitals.Exercise 11.12  researcher wants compare effectiveness three different diets (, B, C) weight loss. Given following data, can determine requires t-test ANOVA? Explain .","code":"\nnurse_data <- data.frame(\n  nurse_id = 1:20,\n  before = c(72, 68, 75, 70, 73, 69, 71, 74, 68, 72, \n             70, 73, 69, 71, 74, 72, 70, 68, 73, 71),\n  after = c(78, 74, 80, 76, 79, 75, 77, 80, 74, 78, \n            76, 79, 75, 77, 80, 78, 76, 74, 79, 77)\n)\ndiet_study <- data.frame(\n  diet = rep(c(\"A\", \"B\", \"C\"), each = 15),\n  weight_loss = c(...)  # 45 observations total\n)"},{"path":"int-samp-q-second.html","id":"part-4-anova---comparing-multiple-groups","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.4 📊 Part 4: ANOVA - Comparing Multiple Groups","text":"Exercise 11.13  Using built-PlantGrowth dataset R, test significant difference plant weight across three treatment groups. Write R command report p-value.Exercise 11.14  medical study examines effect four different medications reducing fever. temperature reduction (degrees Celsius) patients group recorded:Perform ANOVA test determine significant differences among medications. F-statistic?Exercise 11.15  conducting ANOVA test shows significant differences among groups, additional analysis perform identify specific groups differ ?","code":"\nfever_data <- data.frame(\n  medication = rep(c(\"Med_A\", \"Med_B\", \"Med_C\", \"Med_D\"), each = 10),\n  temp_reduction = c(\n    1.2, 1.5, 1.3, 1.4, 1.6, 1.3, 1.5, 1.4, 1.2, 1.5,  # Med_A\n    1.8, 2.0, 1.9, 2.1, 1.8, 2.0, 1.9, 1.8, 2.0, 1.9,  # Med_B\n    1.0, 1.2, 1.1, 1.3, 1.0, 1.2, 1.1, 1.0, 1.2, 1.1,  # Med_C\n    2.3, 2.5, 2.4, 2.6, 2.3, 2.5, 2.4, 2.3, 2.5, 2.4   # Med_D\n  )\n)"},{"path":"int-samp-q-second.html","id":"part-5-simple-and-multiple-linear-regression","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.5 📈 Part 5: Simple and Multiple Linear Regression","text":"Exercise 11.16  Using mtcars dataset, estimate simple linear regression model mpg (miles per gallon) predicted wt (weight). interpretation slope coefficient?Exercise 11.17  researcher collects data hospital readmission rates. want predict readmission_rate (percentage) based bed_count, nurse_ratio (nurses per patient), avg_stay (average length stay). Write R command estimate multiple regression model.Exercise 11.18  multiple linear regression model predicting patient recovery time based age, BMI, exercise hours, obtain following VIF values:\n- age: VIF = 2.3\n- BMI: VIF = 12.5\n- exercise: VIF = 2.1Which variable(s) might problematic due multicollinearity, recommend?Exercise 11.19  Using mtcars dataset, estimate multiple regression model: mpg ~ wt + hp + cyl. checking significance multicollinearity, find cyl p-value 0.85 high VIF. next?Exercise 11.20  Explain difference R-squared Adjusted R-squared. Adjusted R-squared particularly important multiple regression?Exercise 11.21  Given following regression output predicting systolic blood pressure:predicted systolic blood pressure 50-year-old person weighing 75 kg exercises 5 hours per week?Exercise 11.22  regression output previous exercise, variable strongest effect blood pressure? determine ?","code":"Coefficients:\n              Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)   90.234    5.123       17.61    < 2e-16 ***\nage           0.652     0.085        7.67    1.2e-11 ***\nweight        0.234     0.045        5.20    8.3e-07 ***\nexercise     -1.234     0.312       -3.95    0.00015 ***\n\nResidual standard error: 8.5 on 146 degrees of freedom\nMultiple R-squared:  0.5234,    Adjusted R-squared:  0.5136\nF-statistic: 53.45 on 3 and 146 DF,  p-value: < 2.2e-16"},{"path":"int-samp-q-second.html","id":"part-6-nonlinear-regression-and-transformations","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.6 🔄 Part 6: Nonlinear Regression and Transformations","text":"Exercise 11.23  want model relationship BMI risk diabetes, relationship appears quadratic. Write R command estimate polynomial regression model BMI BMI² predictors.Exercise 11.24  Given variable income highly right-skewed, transformation typically apply make normally distributed regression analysis? Write R command create transformed variable.Exercise 11.25  Using mtcars dataset, create model predicting mpg using wt wt² (weight squared). Write complete R commands needed.","code":""},{"path":"int-samp-q-second.html","id":"part-7-logistic-regression","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.7 🎯 Part 7: Logistic Regression","text":"Exercise 11.26  study examines factors affecting whether patient develops certain disease (yes/). predictors age, BMI, smoking status (0=non-smoker, 1=smoker). Write R command estimate logistic regression model disease outcome variable.Exercise 11.27  logistic regression, odds ratio 2.5 variable “smoking” mean practical terms?Exercise 11.28  dataset binary outcome survived (1=yes, 0=) predictors age, treatment_type, severity_score. fitting logistic regression model, get coefficient -0.05 age. negative coefficient indicate?Exercise 11.29  key difference linear regression logistic regression terms predict?Exercise 11.30  Using built-mtcars dataset, create binary variable high_mpg (1 mpg > 20, 0 otherwise) estimate logistic regression model predicting high_mpg based wt hp. Write R commands.","code":""},{"path":"int-samp-q-second.html","id":"part-8-poisson-regression","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.8 📊 Part 8: Poisson Regression","text":"Exercise 11.31  hospital administrator wants model number emergency room visits per day based day week weather conditions. type regression model appropriate count data like ?Exercise 11.32  data number hospital-acquired infections (infection_count) want predict based bed_occupancy_rate nurse_staff_ratio. Write R command fit Poisson regression model.Exercise 11.33  primary assumption Poisson regression regarding relationship mean variance outcome variable?","code":""},{"path":"int-samp-q-second.html","id":"part-9-regression-with-dummy-variables","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.9 🏷️ Part 9: Regression with Dummy Variables","text":"Exercise 11.34  dataset contains variable blood_type categories: , B, AB, O. want include regression model predicting cholesterol levels. many dummy variables need create, ?Exercise 11.35  dataset following variables:\n- recovery_time (continuous outcome)\n- age (continuous)\n- treatment (categorical: “Standard”, “Experimental”, “Control”)Write R command create regression model includes categorical treatment variable.Exercise 11.36  regression model categorical variable “hospital_ward” (ICU, Surgery, General), create dummy variables. regression output shows:“General” reference category, predicted value patient Surgery ward (assuming predictors)?Exercise 11.37  Using mtcars dataset, variable binary (0=automatic, 1=manual). Create regression model predicting mpg based wt, hp, . Write R command.Exercise 11.38  model dummy variable gender (male=1, female=0) predicting salary, coefficient gender 5000. mean?","code":"Coefficients:\n                      Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)           12.5      1.2         10.42    < 2e-16 ***\nward_Surgery          -2.3      0.8         -2.88    0.0045 **\nward_ICU              4.7       0.9          5.22    < 0.0001 ***"},{"path":"int-samp-q-second.html","id":"part-10-model-diagnostics-and-interpretation","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.10 🧮 Part 10: Model Diagnostics and Interpretation","text":"Exercise 11.39  four main assumptions checked linear regression models?Exercise 11.40  fit regression model obtain R² 0.85 Adjusted R² 0.83. 100 observations. Approximately many predictors model? (Hint: use relationship R² Adjusted R²)Exercise 11.41  researcher finds regression model, residuals show clear funnel shape plotted fitted values. problem indicate, might solution?Exercise 11.42  Using mtcars dataset, fit model mpg ~ wt + hp create diagnostic plots using plot() function. Write R commands.","code":""},{"path":"int-samp-q-second.html","id":"part-11-big-data-concepts-and-data-collection","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.11 🔢 Part 11: Big Data Concepts and Data Collection","text":"Exercise 11.43  Explain words correct data collection particularly important era Big Data, even though data ever .Exercise 11.44  difference structured unstructured data? Give one example healthcare context.Exercise 11.45  Name least three potential sources bias data collection affect validity statistical analyses healthcare research.Exercise 11.46  hospital collects data patient satisfaction voluntary online survey. type bias might introduce, ?","code":""},{"path":"int-samp-q-second.html","id":"part-12-mixed-practical-exercises","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.12 💻 Part 12: Mixed Practical Exercises","text":"Exercise 11.47  Create simulated dataset 100 observations containing:\n- patient_id: sequential numbers 1 100\n- age: random values normal distribution mean 50 sd 15\n- treatment: randomly assign “” “B” patient\n- outcome: random values normal distribution mean 100 sd 20Write R commands create dataset.Exercise 11.48  Using dataset created previous exercise, perform following:\n1. Calculate mean age treatment group\n2. Test ’s significant difference outcome treatment groups\n3. Create linear model predicting outcome age treatmentWrite necessary R commands.Exercise 11.49  Load iris dataset :\n1. Filter “setosa” “versicolor” species\n2. Create logistic regression model predicting species based Sepal.Length Petal.Length\n3. Report coefficientsWrite R commands using dplyr filtering.Exercise 11.50  pharmaceutical company tests new drug 500 patients. record:\n- Patient demographics (age, gender, BMI)\n- Baseline health metrics (blood pressure, cholesterol, glucose)\n- Treatment assignment (drug vs. placebo)\n- Outcome 6 months (improved=1, improved=0)Describe complete analytical workflow follow, data cleaning final model interpretation. types analyses perform?","code":""},{"path":"int-samp-q-second.html","id":"solutions-1","chapter":"11 📚 Additional Practice Exercises for Intermediate Exam","heading":"11.13 ✅ SOLUTIONS","text":"Answer Exercise 11.1:Result: 111 complete observations remain.Answer Exercise 11.2:Result: 14 cars “High” efficiency category.Answer Exercise 11.3:Answer Exercise 11.4:Answer Exercise 11.5:one-sample t-test ’re comparing sample mean known population value (45 minutes).Answer Exercise 11.6:null hypothesis H0: p = 0.75. sample proportion 140/200 = 0.70 (70%).Answer Exercise 11.7:Type Error (α): Rejecting null hypothesis actually true. “false positive” - concluding effect isn’t one. probability Type error significance level (usually 0.05).Type II Error (β): Failing reject null hypothesis actually false. “false negative” - missing real effect. probability Type II error related power test (Power = 1 - β).Example healthcare: Type error concluding drug works doesn’t; Type II error concluding drug doesn’t work actually .Answer Exercise 11.8:Decision: Reject null hypothesis.Conclusion: Since p-value (0.032) < α (0.05), reject H0. sufficient evidence conclude average cholesterol level population significantly greater 200 mg/dL.Answer Exercise 11.9:unpaired (independent samples) t-test equal variances.Answer Exercise 11.10:PAIRED test nurses measured training (repeated measures subjects).Answer Exercise 11.11:tests proportions (0.72 vs 0.75) significantly different two hospitals.Answer Exercise 11.12:requires ANOVA (Analysis Variance), t-test.Reason: comparing means across three groups (diets , B, C). t-test can compare two groups time. ANOVA specifically designed test significant differences among three group means simultaneously.Answer Exercise 11.13:p-value approximately 0.0159, indicating significant differences among groups α = 0.05.Answer Exercise 11.14:F-statistic approximately 167.4, small p-value (< 2e-16), indicating highly significant differences among medications.Answer Exercise 11.15:significant ANOVA, perform post-hoc tests (pairwise comparisons) :Tukey’s HSD (Honestly Significant Difference) - commonBonferroni correctionScheffe’s testThese tests identify specific pairs groups differ significantly .Answer Exercise 11.16:Interpretation slope: coefficient wt approximately -5.34. means every 1,000 lb increase car weight, fuel efficiency (mpg) decreases 5.34 miles per gallon, average. negative sign indicates inverse relationship: heavier cars lower fuel efficiency.Answer Exercise 11.17:Answer Exercise 11.18:Problem: variable BMI VIF 12.5, exceeds common threshold 10, indicating problematic multicollinearity. Age exercise acceptable VIF values (< 10).Recommendation:\n1. Consider removing BMI model, \n2. Investigate variable(s) BMI highly correlated \n3. Consider combining correlated variables using principal component analysis (PCA)\n4. Check model performance interpretability actually affectedThe variables (age exercise) fine retained.Answer Exercise 11.19:remove cyl variable model.Reasoning:\n1. p-value 0.85 much larger reasonable significance level (e.g., 0.05), indicating cyl significantly associated mpg model\n2. High VIF suggests multicollinearity predictors\n3. Removing non-significant variables high VIF improves model parsimony interpretabilityNext step:Answer Exercise 11.20:R-squared (R²): Represents proportion variance dependent variable explained independent variable(s). ranges 0 1. However, R² always increases (stays ) add predictors, even ’re truly useful.Adjusted R-squared (R²adj): Adjusts R² number predictors model sample size. penalizes addition unhelpful variables.Formula: R²adj = 1 - [(1 - R²)(n - 1)/(n - k - 1)]\nn = sample size, k = number predictorsWhy important multiple regression:\n- Prevents overfitting\n- Allows fair comparison models different numbers predictors\n- Can decrease add predictors don’t improve model enough justify inclusion\n- realistic measure model qualityUse R²adj comparing models different numbers predictors; use R² evaluating single model’s goodness fit.Answer Exercise 11.21:regression equation :\nSystolic BP = 90.234 + 0.652(age) + 0.234(weight) - 1.234(exercise)50-year-old, 75 kg person exercises 5 hours/week:\nSystolic BP = 90.234 + 0.652(50) + 0.234(75) - 1.234(5)\nSystolic BP = 90.234 + 32.6 + 17.55 - 6.17\nSystolic BP = 134.214 mmHgApproximately 134.2 mmHgAnswer Exercise 11.22:variable strongest effect appears age, t-value 7.67 (highest absolute t-value).determine:Compare t-values (direct method): Larger absolute t-values indicate stronger effects\nage: t = 7.67\nweight: t = 5.20\nexercise: t = -3.95\nCompare t-values (direct method): Larger absolute t-values indicate stronger effectsage: t = 7.67weight: t = 5.20exercise: t = -3.95Compare p-values: highly significant, age smallest p-value (1.2e-11)Compare p-values: highly significant, age smallest p-value (1.2e-11)Standardized coefficients precise method (shown ), account different scales measurementStandardized coefficients precise method (shown ), account different scales measurementNote: simply compare raw coefficients (0.652, 0.234, -1.234) variables measured different units (years, kg, hours).Answer Exercise 11.23:first approach gives interpretable coefficients; second better avoiding multicollinearity BMI BMI².Answer Exercise 11.24:right-skewed variable like income, apply logarithmic transformation:log transformation:\n- Reduces right skewness\n- Makes distribution normal\n- Reduces influence extreme values\n- Often makes relationships linearOther options right-skewed data: square root transformation sqrt(income) inverse transformation 1/income.Answer Exercise 11.25:Method 2 recommended applications ’s concise easy interpret.Answer Exercise 11.26:Key points:\n- Use glm() instead lm()\n- Specify family = binomial logistic regression\n- outcome variable disease binary (0/1, factor 2 levels)\n- Results give log-odds; exponentiate coefficients get odds ratios: exp(coef(logistic_model))Answer Exercise 11.27:odds ratio 2.5 smoking means:odds developing disease 2.5 times higher smokers compared non-smokers, holding variables constant.specifically:\n- = 2.5, smokers 150% higher odds disease non-smokers\n- > 1: positive association (increased risk)\n- = 1: association\n- < 1: negative association (protective effect)Example interpretation: non-smokers 10% probability disease, smokers might approximately 22% probability (though exact conversion odds probability requires calculation).Note: Odds ratio relative risk, rare diseases (< 10% prevalence), approximately equal.Answer Exercise 11.28:negative coefficient -0.05 age indicates:Direction: age increases, log-odds survival decreasePractical meaning: Older patients lower probability survival, holding factors constantOdds ratio: exp(-0.05) = 0.951, meaning one-year increase age, odds survival multiply 0.951 (decrease 5%)10-year age difference: exp(-0.05 × 10) = exp(-0.5) = 0.606\npatient 10 years older 60.6% odds survival compared younger patient.Answer Exercise 11.29:Linear Regression:\n- Predicts actual value continuous outcome variable\n- Output: number continuous scale\n- Example: Predicting exact blood pressure (135 mmHg), salary ($45,000), temperature (98.6°F)\n- Uses normal distributionLogistic Regression:\n- Predicts probability belonging category (usually binary: yes/, 0/1)\n- Output: probability 0 1, can converted category\n- Example: Predicting probability disease (0.75 = 75% chance), probability survival, probability clicking ad\n- Uses binomial distribution logit link functionTechnical difference:\n- Linear regression: E(Y) = β₀ + β₁X₁ + β₂X₂ + …\n- Logistic regression: log(P/(1-P)) = β₀ + β₁X₁ + β₂X₂ + … (models log-odds)Answer Exercise 11.30:Answer Exercise 11.31:Poisson regression appropriate count data.Reasoning:\n- outcome (number ER visits per day) count variable (0, 1, 2, 3, …)\n- Count data discrete non-negative\n- Poisson distribution designed modeling count outcomes\n- models rate events occurringNote: variance much larger mean (overdispersion), consider using negative binomial regression instead.Answer Exercise 11.32:Answer Exercise 11.33:primary assumption Poisson regression mean equals variance (equidispersion).Specifically:\n- E(Y) = Var(Y) = λ (Poisson parameter)practice:\n- Var(Y) > E(Y): Overdispersion - common real data, violates assumption\n- Solution: Use negative binomial regression quasi-Poisson\n- Var(Y) < E(Y): Underdispersion - less common\n- May still use Poisson consider modelsOther important assumptions:\n1. Outcome variable count\n2. Observations independent\n3. Counts negative\n4. log mean linear function predictorsCheck overdispersion:ratio >> 1, overdispersion present.Answer Exercise 11.34:need create 3 dummy variables categorical variable 4 categories.Rule: categorical variable k categories, create (k-1) dummy variables.:\n- One category serves reference/baseline category\n- k-1 categories represented dummy variables\n- Including k categories cause perfect multicollinearity (dummy variable trap)Example blood_type:include type O well, model matrix singular inestimable.Answer Exercise 11.35:R automatically create 2 dummy variables (treatmentStandard treatmentExperimental) “Control” reference category.Answer Exercise 11.36:predicted value patient Surgery ward :Predicted value = 12.5 + (-2.3) = 10.2Explanation:\n- intercept (12.5) represents predicted value reference category (General ward)\n- Surgery ward: add Surgery coefficient (-2.3) intercept\n- ICU ward: add 4.7 get 12.5 + 4.7 = 17.2\n- General ward (reference): just use intercept = 12.5Interpretation: Compared General ward (baseline), patients Surgery 2.3 units lower outcome (average), ICU patients 4.7 units higher outcome.Answer Exercise 11.37:coefficient (transmissionManual) tells difference mpg manual automatic transmissions, holding weight horsepower constant.Answer Exercise 11.38:coefficient 5000 means:Males earn $5,000 females average, holding variables model constant.precisely:\n- gender = 1 (male), add $5,000 predicted salary\n- gender = 0 (female, reference), add $0\n- estimated gender pay gap modelExample:\nmodel : Salary = 40,000 + 5,000(gender) + terms\n- Female (gender=0): Predicted base salary = $40,000 + factors\n- Male (gender=1): Predicted base salary = $45,000 + factorsImportant notes:\n- correlation, necessarily causation\n- factors (education, experience, etc.) controlled \n- coefficient’s statistical significance (p-value) determines difference reliableAnswer Exercise 11.39:four main assumptions linear regression (LINE):Linearity: relationship predictors outcome linear\nCheck: Residuals vs. Fitted plot show random scatter\nCheck: Residuals vs. Fitted plot show random scatterIndependence: Observations independent \nCheck: Durbin-Watson test, knowledge data collection\nViolated : Time series, clustered data, repeated measures\nCheck: Durbin-Watson test, knowledge data collectionViolated : Time series, clustered data, repeated measuresNormality: Residuals normally distributed\nCheck: Q-Q plot, Shapiro-Wilk test, histogram residuals\nimportant inference (confidence intervals, p-values)\nCheck: Q-Q plot, Shapiro-Wilk test, histogram residualsMost important inference (confidence intervals, p-values)Equal variance (Homoscedasticity): Constant variance residuals across levels predictors\nCheck: Scale-Location plot, Breusch-Pagan test\nResiduals vs. Fitted show constant spread\nCheck: Scale-Location plot, Breusch-Pagan testResiduals vs. Fitted show constant spreadAdditional assumption often mentioned:\n5. multicollinearity (multiple regression): Predictors highly correlated\n- Check: VIF (Variance Inflation Factor)Answer Exercise 11.40:Using formula Adjusted R²:\nR²adj = 1 - [(1 - R²)(n - 1)/(n - k - 1)]Given:\n- R² = 0.85\n- R²adj = 0.83\n- n = 100Solving k:\n0.83 = 1 - [(1 - 0.85)(100 - 1)/(100 - k - 1)]\n0.83 = 1 - [0.15 × 99/(99 - k)]\n0.17 = 0.15 × 99/(99 - k)\n0.17(99 - k) = 14.85\n16.83 - 0.17k = 14.85\n-0.17k = -1.98\nk ≈ 11.6So approximately 11 12 predictors model.Quick approximation: R² R²adj close, typically relatively predictors. difference 0.02 n=100 suggests around 10-12 predictors.Answer Exercise 11.41:Problem identified: Heteroscedasticity (non-constant variance)funnel shape means:\n- Variance residuals increases (decreases) fitted values\n- Violates assumption homoscedasticity (equal variance)Potential solutions:Transform outcome variable:\n\n# Log transformation\nmodel_log <- lm(log(y) ~ x1 + x2, data = data)\n\n# Square root transformation\nmodel_sqrt <- lm(sqrt(y) ~ x1 + x2, data = data)Transform outcome variable:Use weighted least squares (WLS):\n\n# Weights inversely proportional variance\nwls_model <- lm(y ~ x1 + x2, data = data, weights = 1/residuals^2)Use weighted least squares (WLS):Use robust standard errors:\n\nlibrary(sandwich)\nlibrary(lmtest)\ncoeftest(model, vcov = vcovHC(model, type = \"HC1\"))Use robust standard errors:Add transform predictors: May indicate missing variable wrong functional formAdd transform predictors: May indicate missing variable wrong functional formUse generalized linear models (GLM): outcome count binaryUse generalized linear models (GLM): outcome count binaryCheck solution:Answer Exercise 11.42:Interpretation:\n- Plot 1: show random scatter around zero\n- Plot 2: Points follow diagonal line\n- Plot 3: show horizontal line equal spread\n- Plot 4: Points outside Cook’s distance indicate influential observationsAnswer Exercise 11.43:Correct data collection crucial Big Data several reasons:Volume doesn’t equal quality: data doesn’t automatically make better. Garbage , garbage - data collection flawed, just bad data, leading confident wrong conclusions.Volume doesn’t equal quality: data doesn’t automatically make better. Garbage , garbage - data collection flawed, just bad data, leading confident wrong conclusions.Bias amplification: Systematic errors data collection amplified large datasets. small sampling bias becomes huge problem scaled , potentially affecting millions decisions.Bias amplification: Systematic errors data collection amplified large datasets. small sampling bias becomes huge problem scaled , potentially affecting millions decisions.Representativeness: Big data often comes convenience samples (e.g., social media users, app users) may represent target population. Just millions data points doesn’t mean represent everyone.Representativeness: Big data often comes convenience samples (e.g., social media users, app users) may represent target population. Just millions data points doesn’t mean represent everyone.Missing data patterns: healthcare, missing data rarely random. data systematically missing certain groups (e.g., disadvantaged populations less likely electronic health records), analyses biased matter much data .Missing data patterns: healthcare, missing data rarely random. data systematically missing certain groups (e.g., disadvantaged populations less likely electronic health records), analyses biased matter much data .Measurement validity: Ensuring variables actually measure think measure critical. healthcare, diagnostic codes inconsistently applied symptoms differently reported, analyses unreliable.Measurement validity: Ensuring variables actually measure think measure critical. healthcare, diagnostic codes inconsistently applied symptoms differently reported, analyses unreliable.Ethical considerations: Poor data collection can perpetuate discrimination inequity, especially healthcare decisions affect lives.Ethical considerations: Poor data collection can perpetuate discrimination inequity, especially healthcare decisions affect lives.Example: hospital excellent electronic records insured patients poor documentation uninsured patients produce biased analyses treatment effectiveness, potentially harming vulnerable populations.Answer Exercise 11.44:Structured Data:\n- Organized predefined format (rows columns)\n- Easily searchable analyzable standard statistical tools\n- Stored relational databases spreadsheets\n- Healthcare example:\n- Electronic Health Records (EHRs) fields like patient ID, age, blood pressure readings, lab test results\n- Billing codes (ICD-10 diagnosis codes)\n- Appointment dates timesUnstructured Data:\n- predefined format organization\n- easily searchable without special tools (NLP, text mining)\n- Can include text, images, audio, video\n- Healthcare example:\n- Physician clinical notes (free text)\n- Medical imaging (X-rays, MRIs, CT scans)\n- Patient feedback survey comments\n- Audio recordings patient-doctor consultationsSemi-structured data (middle ground):\n- organizational properties doesn’t fit strict relational model\n- Healthcare example: XML JSON formatted health information exchange documentsChallenge: healthcare data (estimated 80%) unstructured, requiring advanced techniques extract value .Answer Exercise 11.45:Three major sources bias healthcare data collection:Selection Bias / Sampling Bias:\nProblem: sample doesn’t represent target population\nExamples:\nClinical trials exclude elderly patients, pregnant women, comorbidities\nStudies recruiting academic medical centers (sicker, complex patients)\nOnline health surveys capture tech-savvy, engaged patients\n\nImpact: Results don’t generalize broader populations\nProblem: sample doesn’t represent target populationExamples:\nClinical trials exclude elderly patients, pregnant women, comorbidities\nStudies recruiting academic medical centers (sicker, complex patients)\nOnline health surveys capture tech-savvy, engaged patients\nClinical trials exclude elderly patients, pregnant women, comorbiditiesStudies recruiting academic medical centers (sicker, complex patients)Online health surveys capture tech-savvy, engaged patientsImpact: Results don’t generalize broader populationsMeasurement Bias / Information Bias:\nProblem: Systematic errors variables measured recorded\nExamples:\nRecall bias: Patients disease remember exposures better healthy patients\nObserver bias: Researchers’ expectations influence record observations\nDiagnostic bias: Patients closer surveillance likely conditions detected\n\nImpact: Distorted associations variables\nProblem: Systematic errors variables measured recordedExamples:\nRecall bias: Patients disease remember exposures better healthy patients\nObserver bias: Researchers’ expectations influence record observations\nDiagnostic bias: Patients closer surveillance likely conditions detected\nRecall bias: Patients disease remember exposures better healthy patientsObserver bias: Researchers’ expectations influence record observationsDiagnostic bias: Patients closer surveillance likely conditions detectedImpact: Distorted associations variablesNon-Response Bias / Attrition Bias:\nProblem: Systematic differences responders non-responders\nExamples:\nHealthier patients likely complete follow-studies\nPatients experiencing side effects drop clinical trials\nLower response rates disadvantaged communities\n\nImpact: Results don’t reflect full population, often overestimating positive outcomes\nProblem: Systematic differences responders non-respondersExamples:\nHealthier patients likely complete follow-studies\nPatients experiencing side effects drop clinical trials\nLower response rates disadvantaged communities\nHealthier patients likely complete follow-studiesPatients experiencing side effects drop clinical trialsLower response rates disadvantaged communitiesImpact: Results don’t reflect full population, often overestimating positive outcomesOther important biases:\n- Confounding bias: Third variables influence exposure outcome\n- Survival bias: analyzing survivors excludes died\n- Temporal bias: Changes diagnostic criteria treatment timeAnswer Exercise 11.46:introduces non-response bias (also called self-selection bias).’s problematic:Voluntary participation: motivated patients respond\nsatisfied patients may likely respond (positive bias)\ndissatisfied patients may likely respond (negative bias)\n“moderately satisfied” majority may participate\nsatisfied patients may likely respond (positive bias)dissatisfied patients may likely respond (negative bias)“moderately satisfied” majority may participateDigital divide:\nExcludes patients without internet access (often elderly, low-income, less educated)\ngroups may different satisfaction levels\nTypically underrepresents vulnerable populations\nExcludes patients without internet access (often elderly, low-income, less educated)groups may different satisfaction levelsTypically underrepresents vulnerable populationsHealth status:\nSicker patients hospital may complete surveys\nHealthier, discharged patients likely respond\nCreates systematic difference responders non-responders\nSicker patients hospital may complete surveysHealthier, discharged patients likely respondCreates systematic difference responders non-respondersTiming: captures patients actively seeking care, missing avoid hospital due poor past experiencesConsequence: survey results accurately reflect true patient satisfaction across patient groups. Conclusions drawn biased policy decisions based data misguided.Better approach:\n- Random sampling patients\n- Multiple contact methods (phone, mail, -person)\n- Follow-non-responders\n- Weight responses match population demographicsAnswer Exercise 11.47:Alternative control:Answer Exercise 11.48:Answer Exercise 11.49:Interpretation: model likely show Petal.Length strong predictor (high coefficient) distinguishing setosa versicolor.Answer Exercise 11.50:Complete Analytical Workflow:1. Data Cleaning & Preparation (Exploratory Phase)2. Descriptive Statistics & Initial Exploration3. Primary Analysis - Treatment Effect4. Adjusted Analysis - Control Confounders5. Subgroup Analyses6. Model Diagnostics7. Sensitivity Analyses8. Results Interpretation & ReportingKey Analyses Perform:\n1. ✓ Descriptive statistics (demographics, baseline characteristics)\n2. ✓ Balance check (treatment groups comparable baseline?)\n3. ✓ Crude association (treatment vs. outcome)\n4. ✓ Adjusted association (control confounders)\n5. ✓ Interaction testing (effect modification)\n6. ✓ Subgroup analyses\n7. ✓ Model diagnostics validation\n8. ✓ Sensitivity analysesFinal Deliverables:\n- Table 1: Baseline characteristics treatment group\n- Table 2: Crude adjusted odds ratios\n- Figure 1: Forest plot effect estimates\n- Figure 2: ROC curve\n- Supplementary: Sensitivity analyses results","code":"\ndata(airquality)\ncomplete_data <- na.omit(airquality)\nnrow(complete_data)\ndata(mtcars)\nmtcars$efficiency <- ifelse(mtcars$mpg > 20, \"High\", \"Low\")\ntable(mtcars$efficiency)\nmean(health_data$blood_pressure, na.rm = TRUE)\nmedian(health_data$blood_pressure, na.rm = TRUE)\nsd(health_data$blood_pressure, na.rm = TRUE)\n\n# Or all at once:\nsummary(health_data$blood_pressure)\nlibrary(dplyr)\nhealth_data %>%\n  group_by(treatment) %>%\n  summarise(mean_recovery = mean(recovery_days, na.rm = TRUE))\n\n# Or base R:\naggregate(recovery_days ~ treatment, data = health_data, FUN = mean)\n# Create the sample data\nwaiting_time <- rnorm(50, mean = 52, sd = 12)  # Simulated data\n\n# Perform one-sample t-test\nt.test(waiting_time, mu = 45, alternative = \"two.sided\")\n\n# Or if you have the actual data vector:\n# t.test(sample_waiting_times, mu = 45, alternative = \"two.sided\")\n# Proportion test\nprop.test(x = 140, n = 200, p = 0.75, alternative = \"two.sided\")\n# Create sample data (in practice, you'd have actual data)\ngroup_A <- rnorm(30, mean = 15, sd = 5)\ngroup_B <- rnorm(35, mean = 12, sd = 6)\n\n# Two-sample t-test assuming equal variances\nt.test(group_A, group_B, var.equal = TRUE, alternative = \"two.sided\")\nnurse_data <- data.frame(\n  nurse_id = 1:20,\n  before = c(72, 68, 75, 70, 73, 69, 71, 74, 68, 72, \n             70, 73, 69, 71, 74, 72, 70, 68, 73, 71),\n  after = c(78, 74, 80, 76, 79, 75, 77, 80, 74, 78, \n            76, 79, 75, 77, 80, 78, 76, 74, 79, 77)\n)\n\n# Paired t-test\nt.test(nurse_data$after, nurse_data$before, \n       paired = TRUE, alternative = \"greater\")\n# Two-sample proportion test\nprop.test(x = c(180, 210), n = c(250, 280), alternative = \"two.sided\")\n# The appropriate test would be:\nanova_result <- aov(weight_loss ~ diet, data = diet_study)\nsummary(anova_result)\ndata(PlantGrowth)\nplant_model <- aov(weight ~ group, data = PlantGrowth)\nsummary(plant_model)\nfever_data <- data.frame(\n  medication = rep(c(\"Med_A\", \"Med_B\", \"Med_C\", \"Med_D\"), each = 10),\n  temp_reduction = c(\n    1.2, 1.5, 1.3, 1.4, 1.6, 1.3, 1.5, 1.4, 1.2, 1.5,\n    1.8, 2.0, 1.9, 2.1, 1.8, 2.0, 1.9, 1.8, 2.0, 1.9,\n    1.0, 1.2, 1.1, 1.3, 1.0, 1.2, 1.1, 1.0, 1.2, 1.1,\n    2.3, 2.5, 2.4, 2.6, 2.3, 2.5, 2.4, 2.3, 2.5, 2.4\n  )\n)\n\nfever_anova <- aov(temp_reduction ~ medication, data = fever_data)\nsummary(fever_anova)\n# Example using Tukey's HSD:\nTukeyHSD(anova_model)\n\n# Or using the multcomp package:\nlibrary(multcomp)\npost_hoc <- glht(anova_model, linfct = mcp(group_variable = \"Tukey\"))\nsummary(post_hoc)\ndata(mtcars)\nmpg_model <- lm(mpg ~ wt, data = mtcars)\nsummary(mpg_model)\nreadmission_model <- lm(readmission_rate ~ bed_count + nurse_ratio + avg_stay, \n                        data = hospital_data)\nsummary(readmission_model)\n# Refit the model without cyl\nimproved_model <- lm(mpg ~ wt + hp, data = mtcars)\nsummary(improved_model)\n# Check VIF again\nlibrary(car)\nvif(improved_model)\n# Create the polynomial term\nmodel_data$BMI_squared <- model_data$BMI^2\n\n# Fit polynomial regression\npoly_model <- lm(diabetes_risk ~ BMI + BMI_squared, data = model_data)\nsummary(poly_model)\n\n# Alternative using poly() function (creates orthogonal polynomials):\npoly_model2 <- lm(diabetes_risk ~ poly(BMI, 2), data = model_data)\nsummary(poly_model2)\n# Natural log transformation\ndata$log_income <- log(data$income)\n\n# Or log base 10\ndata$log10_income <- log10(data$income)\n\n# Make sure there are no zero or negative values first:\ndata$log_income <- log(data$income + 1)  # Adding 1 if zeros exist\ndata(mtcars)\n\n# Method 1: Create the squared term explicitly\nmtcars$wt_squared <- mtcars$wt^2\npoly_model <- lm(mpg ~ wt + wt_squared, data = mtcars)\nsummary(poly_model)\n\n# Method 2: Using I() function (cleaner)\npoly_model2 <- lm(mpg ~ wt + I(wt^2), data = mtcars)\nsummary(poly_model2)\n\n# Method 3: Using poly() for orthogonal polynomials\npoly_model3 <- lm(mpg ~ poly(wt, 2), data = mtcars)\nsummary(poly_model3)\nlogistic_model <- glm(disease ~ age + BMI + smoking, \n                      data = health_data, \n                      family = binomial(link = \"logit\"))\nsummary(logistic_model)\ndata(mtcars)\n\n# Create binary outcome variable\nmtcars$high_mpg <- ifelse(mtcars$mpg > 20, 1, 0)\n\n# Fit logistic regression model\nlogit_model <- glm(high_mpg ~ wt + hp, \n                   data = mtcars, \n                   family = binomial(link = \"logit\"))\n\n# View results\nsummary(logit_model)\n\n# Get odds ratios\nexp(coef(logit_model))\n\n# Predict probabilities for original data\nmtcars$predicted_prob <- predict(logit_model, type = \"response\")\n# Example Poisson regression\npoisson_model <- glm(er_visits ~ day_of_week + weather, \n                     data = hospital_data, \n                     family = poisson(link = \"log\"))\npoisson_model <- glm(infection_count ~ bed_occupancy_rate + nurse_staff_ratio, \n                     data = hospital_data, \n                     family = poisson(link = \"log\"))\n\nsummary(poisson_model)\n\n# Check for overdispersion\nlibrary(AER)\ndispersiontest(poisson_model)\n\n# If overdispersion detected, use negative binomial:\nlibrary(MASS)\nnb_model <- glm.nb(infection_count ~ bed_occupancy_rate + nurse_staff_ratio, \n                   data = hospital_data)\n# Deviance/df should be close to 1\nmodel$deviance / model$df.residual\n# R does this automatically in regression, but manual creation:\ndata$type_A <- ifelse(data$blood_type == \"A\", 1, 0)\ndata$type_B <- ifelse(data$blood_type == \"B\", 1, 0)\ndata$type_AB <- ifelse(data$blood_type == \"AB\", 1, 0)\n# Type O is the reference category (all three dummies = 0)\n\n# In regression, R handles this automatically:\nmodel <- lm(cholesterol ~ blood_type, data = data)\n# R will create the dummy variables automatically\n# R automatically creates dummy variables for factors\nrecovery_model <- lm(recovery_time ~ age + treatment, data = patient_data)\nsummary(recovery_model)\n\n# To explicitly set the reference category:\npatient_data$treatment <- factor(patient_data$treatment, \n                                  levels = c(\"Control\", \"Standard\", \"Experimental\"))\n# Now \"Control\" is the reference\n\n# Alternatively, using relevel():\npatient_data$treatment <- relevel(factor(patient_data$treatment), ref = \"Control\")\n\nrecovery_model <- lm(recovery_time ~ age + treatment, data = patient_data)\nsummary(recovery_model)\ndata(mtcars)\n\n# The variable am is already coded as 0/1\nmpg_model <- lm(mpg ~ wt + hp + am, data = mtcars)\nsummary(mpg_model)\n\n# To make it more explicit:\nmtcars$transmission <- factor(mtcars$am, levels = c(0, 1), \n                               labels = c(\"Automatic\", \"Manual\"))\nmpg_model2 <- lm(mpg ~ wt + hp + transmission, data = mtcars)\nsummary(mpg_model2)\n# Check assumptions:\nplot(model)  # Produces 4 diagnostic plots\nlibrary(car)\nvif(model)  # Check multicollinearity\n# Log transformation\nmodel_log <- lm(log(y) ~ x1 + x2, data = data)\n\n# Square root transformation\nmodel_sqrt <- lm(sqrt(y) ~ x1 + x2, data = data)\n# Weights inversely proportional to variance\nwls_model <- lm(y ~ x1 + x2, data = data, weights = 1/residuals^2)\nlibrary(sandwich)\nlibrary(lmtest)\ncoeftest(model, vcov = vcovHC(model, type = \"HC1\"))\nplot(fitted(new_model), residuals(new_model))\n# Should show random scatter, not a pattern\ndata(mtcars)\n\n# Fit the model\nmpg_model <- lm(mpg ~ wt + hp, data = mtcars)\n\n# Create diagnostic plots\npar(mfrow = c(2, 2))  # Set up 2x2 plot layout\nplot(mpg_model)\npar(mfrow = c(1, 1))  # Reset to default\n\n# Individual plots:\n# 1. Residuals vs Fitted (checks linearity and homoscedasticity)\nplot(mpg_model, which = 1)\n\n# 2. Q-Q plot (checks normality of residuals)\nplot(mpg_model, which = 2)\n\n# 3. Scale-Location (checks homoscedasticity)\nplot(mpg_model, which = 3)\n\n# 4. Residuals vs Leverage (identifies influential points)\nplot(mpg_model, which = 5)\n\n# Additional useful diagnostics:\n# Histogram of residuals\nhist(residuals(mpg_model), main = \"Histogram of Residuals\")\n\n# Shapiro-Wilk test for normality\nshapiro.test(residuals(mpg_model))\n# Set seed for reproducibility\nset.seed(123)\n\n# Create simulated dataset\nsimulated_data <- data.frame(\n  patient_id = 1:100,\n  age = rnorm(100, mean = 50, sd = 15),\n  treatment = sample(c(\"A\", \"B\"), size = 100, replace = TRUE),\n  outcome = rnorm(100, mean = 100, sd = 20)\n)\n\n# View first few rows\nhead(simulated_data)\n\n# Check structure\nstr(simulated_data)\n\n# Summary statistics\nsummary(simulated_data)\nset.seed(456)\n\n# Ensure exactly 50 patients in each treatment group\nsimulated_data2 <- data.frame(\n  patient_id = 1:100,\n  age = rnorm(100, mean = 50, sd = 15),\n  treatment = rep(c(\"A\", \"B\"), each = 50),\n  outcome = rnorm(100, mean = 100, sd = 20)\n)\n# Using the simulated_data from previous exercise\nset.seed(123)\nsimulated_data <- data.frame(\n  patient_id = 1:100,\n  age = rnorm(100, mean = 50, sd = 15),\n  treatment = sample(c(\"A\", \"B\"), size = 100, replace = TRUE),\n  outcome = rnorm(100, mean = 100, sd = 20)\n)\n\n# 1. Calculate mean age for each treatment group\nlibrary(dplyr)\nage_by_treatment <- simulated_data %>%\n  group_by(treatment) %>%\n  summarise(\n    mean_age = mean(age),\n    sd_age = sd(age),\n    n = n()\n  )\nprint(age_by_treatment)\n\n# Base R alternative:\naggregate(age ~ treatment, data = simulated_data, FUN = mean)\n\n# 2. Test if there's significant difference in outcome between treatments\noutcome_test <- t.test(outcome ~ treatment, data = simulated_data)\nprint(outcome_test)\n\n# 3. Create linear model predicting outcome from age and treatment\noutcome_model <- lm(outcome ~ age + treatment, data = simulated_data)\nsummary(outcome_model)\n\n# Additional useful analyses:\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(outcome_model)\n\n# Get confidence intervals\nconfint(outcome_model)\n# Load necessary packages\nlibrary(dplyr)\n\n# Load iris dataset\ndata(iris)\n\n# 1. Filter only setosa and versicolor species\niris_filtered <- iris %>%\n  filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n# Alternative base R:\niris_filtered <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]\n\n# Create binary outcome (0/1)\niris_filtered$species_binary <- ifelse(iris_filtered$Species == \"versicolor\", 1, 0)\n\n# 2. Fit logistic regression model\nlogit_model <- glm(species_binary ~ Sepal.Length + Petal.Length, \n                   data = iris_filtered, \n                   family = binomial(link = \"logit\"))\n\n# 3. Report coefficients\nsummary(logit_model)\ncoef(logit_model)\n\n# Get odds ratios\nexp(coef(logit_model))\n\n# Predictions\niris_filtered$predicted_prob <- predict(logit_model, type = \"response\")\niris_filtered$predicted_species <- ifelse(iris_filtered$predicted_prob > 0.5, \n                                           \"versicolor\", \"setosa\")\n\n# Confusion matrix\ntable(Predicted = iris_filtered$predicted_species, \n      Actual = iris_filtered$Species)\n# Load and inspect data\nsummary(pharma_data)\nstr(pharma_data)\n\n# Check for missing values\nsum(is.na(pharma_data))\nlibrary(naniar)\nvis_miss(pharma_data)\n\n# Handle missing data\n# - Document patterns of missingness\n# - Decide on imputation vs. deletion\n# - Use multiple imputation if appropriate\n\n# Check for outliers\nboxplot(pharma_data[, c(\"age\", \"BMI\", \"blood_pressure\")])\n\n# Create derived variables if needed\npharma_data$age_group <- cut(pharma_data$age, breaks = c(0, 40, 60, 100))\n# Demographics summary\ntable(pharma_data$gender, pharma_data$treatment)\n\n# Baseline characteristics by treatment group\npharma_data %>%\n  group_by(treatment) %>%\n  summarise(\n    mean_age = mean(age),\n    mean_BP = mean(blood_pressure),\n    mean_cholesterol = mean(cholesterol)\n  )\n\n# Check randomization success\nt.test(age ~ treatment, data = pharma_data)\nt.test(BMI ~ treatment, data = pharma_data)\n# Crude comparison of outcome\nprop.table(table(pharma_data$treatment, pharma_data$improved), 1)\n\n# Chi-square test\nchisq.test(pharma_data$treatment, pharma_data$improved)\n\n# Logistic regression - unadjusted\nmodel_crude <- glm(improved ~ treatment, \n                   data = pharma_data, \n                   family = binomial)\nsummary(model_crude)\n# Multivariable logistic regression\nmodel_adjusted <- glm(improved ~ treatment + age + gender + BMI + \n                                 blood_pressure + cholesterol + glucose, \n                     data = pharma_data, \n                     family = binomial)\nsummary(model_adjusted)\n\n# Check for multicollinearity\nlibrary(car)\nvif(model_adjusted)\n\n# Model selection if needed\nmodel_final <- step(model_adjusted, direction = \"backward\")\n# Test for interaction with gender\nmodel_interaction <- glm(improved ~ treatment * gender + age + BMI, \n                         data = pharma_data, \n                         family = binomial)\n\n# Stratified analyses\n# By age group\npharma_data %>%\n  filter(age_group == \"young\") %>%\n  glm(improved ~ treatment, data = ., family = binomial) %>%\n  summary()\n# Check assumptions\nplot(model_final)\n\n# Assess model fit\nlibrary(ResourceSelection)\nhoslem.test(model_final$y, fitted(model_final))\n\n# Calculate AUC\nlibrary(pROC)\nroc_curve <- roc(pharma_data$improved, \n                 fitted(model_final))\nauc(roc_curve)\nplot(roc_curve)\n# Complete case vs. multiple imputation\n# Different model specifications\n# Exclude influential outliers\n# Alternative statistical methods\n# Calculate odds ratios with 95% CI\nexp(cbind(OR = coef(model_final), confint(model_final)))\n\n# Number needed to treat (NNT)\n# Predicted probabilities for typical patients\n# Clinical significance vs. statistical significance"},{"path":"int-samp-q.html","id":"int-samp-q","chapter":"12 💻 Intermediate Sample Questions","heading":"12 💻 Intermediate Sample Questions","text":"Hi guys, favourite TA, just aggregating questions asked previous exam sessions previous years .e. 2020/2021 2021/2022. representative actual exam, know, take like grain salt.also make sure provide exercises still anxious.","code":""},{"path":"int-samp-q.html","id":"section","chapter":"12 💻 Intermediate Sample Questions","heading":"12.1 👨‍🎓 2020/2021","text":"Exercise 12.1  Write line R command use produce boxplot variable XExercise 12.2  want test statistically hypothesis performances students UCSC Rome graduated last year better graduated year. Can say paired sample test ?Exercise 12.3  Without using formulae, describe can calculate test statistics hypothesis testing procedure single mean known variance.Exercise 12.4  Using dataset Boston downloaded library spdep, write correlation matrix variables MEDV, NOX CRIM.Exercise 12.5  define confidence statistical test?Exercise 12.6  Given following 2 variables X = (1,5,3,3,5,5) Y= (4,4,6,3,2,3), write cross-tabulation X Y.Exercise 12.7  Write line R command use simulate 1000 random observation normal distribution 0 mean variance = 0.5.Exercise 12.8  law company evaluating performances two departments measuring terms time required solving conflict last year. observed values reported following table:…can accept hypothesis H0: (mean Dept 1 equal mean Dept 2) versus bilateral alternative hypothesis? (F)Exercise 12.9  company recorded number costumers 10 sample stores (variable X) (Variable Y) new advertising campaign introduced. observed values reported following table…write p-value test H0: (mean X equal mean Y) versus bilateral alternative hypothesis. ( 0,000341138)Exercise 12.10  HR office cleaning company wants test gender discrimination employees. Call X = income set 20 male workers Y = income set 35 female workers. Write line R command run appropriate test hypothesis.Exercise 12.11  power statistical test?Exercise 12.12  Using dataset boston.c downloaded library spdep, calculate coefficient skewness variable RM.Answer Exercise 12.12:Exercise 12.13  define significance statistical test?","code":"library(moments)\nskewness(boston.c$RM)\n\n0,4024147"},{"path":"int-samp-q.html","id":"section-1","chapter":"12 💻 Intermediate Sample Questions","heading":"12.2 👨‍🎓 2021/2022","text":"Exercise 12.14  Given dataset “Duncan” library “carData” estimate regression model variable prestige regressed variables income Looking following information,residuals display.Exercise 12.15  consequences collinearity among regressors?Estimators become biasedEstimators become inefficientEstimators become inconsistentEstimators become unstableExercise 12.16  correct definition variance inflation factor .e. VIF?\\(1-R2\\)\\(\\frac{1}{R2}\\)\\(\\frac{1}{1-R2}\\)\\(1-\\frac{1}{R2}\\)Answer Exercise 12.16:general guideline VIF larger 5 10 large, indicating model problems estimating coefficient. However, general degrade quality predictions. VIF larger 1/(1-R2), R2 Multiple R-squared regression, predictor related predictors response.alternatively can use library car use vif() functionExercise 12.17  Using following variables minority , crime , poverty , language highschool housing Ericksen data library carData, run factor analysis. percentage explained first two factors?risposta: 90.130.001Exercise 12.18  multiple linear regression model y= +bx1+cx2, Correlation(x1,x2)=0.9, discard one two variables collinearity?risposta: FExercise 12.19  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education. variable significant?EducationincomeAnswer Exercise 12.19:first load data Duncan datasetThen specify model produce sumamries:look pvalues andeducation significant income since 0.00000173 < 0.00001053Exercise 12.20  multiple linear regression model y= +bx1+cx2, level correlation x1 x2 beyond discard one two variables collinearity?risposta: 0.948Exercise 12.21  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education. p-value coefficient variable education?Answer Exercise 12.21:first load data Duncan datasetThen specify model produce sumamries:look pvalues andThe pvalue coefficient 0.00000173you may want directly access instead just copying pasting console sumamry outputExercise 12.22  reason adjusting R2 multiple regressionTo account number degrees freedomTo account number parametersTo reduce uncertaintyTo adjust variance inflation factorrispoasta: account number degrees freedomExercise 12.23  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income. Using VIF, exclude variable due collinearity?result: FAnswer Exercise 12.23:first load data Duncan datasetThen specify model produce sumamries:output look like something like.Since 10 rule thumb gave assess multicollinearity conclude neither income education collinear.Exercise 12.24  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income. value t value coefficient variable education?Answer Exercise 12.24:first load data Duncan datasetThen specify model produce sumamries:output look like something like.inspecting summary wee obtain t value (t value column summary) dor variable education 5.555Exercise 12.24  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using k-means method. divide observations 4 classes frequency largest class ?result: 26Exercise 12.25  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using k-means method. percentage explained first factor?risposta: 7.391.719Exercise 12.26  Using following variables minority , crime , poverty , language, highschool housing Ericksen data library carData, run cluster analysis using hierarchical method. divide observations 10 classes frequency largest class ?risposta: 27Exercise 12.27  Given dataset Duncan library carData estimate regression model variable prestige regressed variables income education report \\(R^2\\).Answer Exercise 12.27:first load data Duncan datasetThen specify model produce sumamries:output look like something like.inspecting lowe end summary obtain R2 (multiple) model 0.8282, high.","code":"Residuals:\n\nMin      1Q  Median      3Q     Max\n\n-29.538  -6.417   0.655   6.605  34.641install.packages(\"regclass\")\nlibrary(regclass)\nVIF(modello_regressione)install.packges(\"car\")\nlibrary(car)\nvif(modello_regressione)library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\nlibrary(car)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nvif(duncan_regression) income education \n 2.1049    2.1049 library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Coefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420      0.163    \nincome       0.59873    0.11967   5.003 0.00001053 ***\neducation    0.54583    0.09825   5.555 0.00000173 ***library(carData)\ndata(\"Duncan\")duncan_regression = lm(prestige~ income + education, data= Duncan)\nsummary(duncan_regression)Residual standard error: 13.37 on 42 degrees of freedom\nMultiple R-squared:  0.8282,    Adjusted R-squared:   0.82 \nF-statistic: 101.2 on 2 and 42 DF,  p-value: < 0.00000000000000022\n    "},{"path":"int-samp-q.html","id":"section-2","chapter":"12 💻 Intermediate Sample Questions","heading":"12.3 👨‍🎓 2022/2023","text":"Exercise 12.28  Using dataset Boston downloaded library spdep,\ncalculate coefficient skewness variable RM.Exercise 12.29  define significance statistical test?Exercise 12.30  power statistical test?Exercise 12.31  define confidence statistical test?Exercise 12.32  law company evaluating performances two departments\nmeasuring terms time required solving conflict \nlast year. observed values reported following table:can reject hypothesis H0: (mean Dept 1 equal \nmean Dept 2) versus bilateral alternative hypothesis?Exercise 12.33  company recorded number costumers 10 sample stores\n(variable X) (Variable Y) new advertising campaign\nintroduced. observed values reported following table:can reject hypothesis H0: (mean X, .e. equal \nmean Y, .e. ) versus bilateral alternative hypothesis?Exercise 12.34  Write line R command use simulate 2000 random\nobservation normal distribution 0 mean variance = 0.1Many fall trap!. Tip: always use “tab” \nautomatic suggestion also check arguments. case\nexercise wants sample normal distribution 2000\ninstances (data points), 0 mean variance = 0.1. argument\nrnorm sd var, apply square root!Answer Question 12.34:Exercise 12.35  Write line R command use produce boxplot \nvariable XExercise 12.36  Given following 2 variables X = (5,5,3,3,5,5) \nY= (4,4,3,3,3,3), test mean X significantly different\nmean Y. Report p-value appropriate test \ndecision.Exercise 12.37  Using dataset boston.c downloaded library spdep, write\nelements correlation matrix variables MEDV, NOX \nCRIM.Exercise 12.38  Without using formulae, describe can calculate test\nstatistics hypothesis testing procedure single mean known\nvariance.Exercise 12.39  HR office cleaning company wants test significant difference salary males females. Call X = salary set 2000 male workers Y = salary set 150 female workers. previous survey know variances two groups equal. Write line R command run appropriate test hypothesis.Exercise 12.40  want test statistically hypothesis students UCSC\nRome better performances second year first year\nyear. Can say paired sample test?Exercise 12.41  Using dataset iris test significant difference\nmean Petal.Length mean Sepal.Width \nreport outcome value t-test.Exercise 12.42  Using dataset iris calculate correlation \nSepal.Length Sepal.Width.Exercise 12.43  Using dataset iris report highest correlation coefficient \nfind four variables.Exercise 12.44  Using dataset iris report highest correlation coefficient \nfind four variables.Exercise 12.45  Using dataset iris report variance Sepal.LengthExercise 12.46  Using dataset iris report third quartile Sepal.LengthExercise 12.47  reason adjusting R2 multiple regression?Exercise 12.48  correct definition variance inflation factor?Exercise 12.49  consequences collinearity among regressors?Exercise 12.50  Using dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.variables retained model? (retained means\ntratteresti)Exercise 12.51  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.value adjusted R squared best modelExercise 12.52  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.estimated coefficient variable duration best\nmodel?Exercise 12.53  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.estimated value intercept best model?Exercise 12.54  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.p-value variable duration best model?Exercise 12.55  sing dataset Wong R library carData, estimate \nmultiple linear regression variable piq expressed \nfunction age, days duration.check collinearity significance choose best\nmodel.value R square best modelExercise 12.56  Using dataset iris, test average variable\nSepal.Length changes significantly three Species considered.\nReport p-value appropiate test.look Species (already gone lecture)\ninspecting dataset. see Species three\ncategories setosa, versicolor virginica. like \ncompare means across 3 different categories can’t use\nt.test() since 3. Instead use ANOVA aov().\nSintax similar linear models. saw trying \ntackle “long” format data vs. “wide” format data.Answer Question 12.56:resulting 0.0000000000000002, significant. can conclude\n: ANOVA (formula: Sepal.Length ~ Species) suggests \nmain effect Species statistically significant large.Exercise 12.57  Using dataset iris, test average variable\nSepal.Length differs significantly three Species, Report \nvalue test statistic.","code":"perf_table = data.frame(\n  stringsAsFactors = FALSE,\n             month = c(\"january\",\"febraury\",\"march\",\n                       \"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\n                       \"october\",\"november\",\"december\"),\n            dept_1 = c(NA, NA, NA, 3L, 6L, 9L, 7L, 5L, 7L, 3L, 4L, 6L),\n            dept_2 = c(4L, 3L, 9L, 5L, 7L, 2L, 6L, 3L, 6L, 7L, 4L, 1L)\n)\n)stores = data.frame(\n     n_store = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L),\n      before = c(113L, 110L, 108L, 108L, 103L, 101L, 96L, 101L, 104L, 98L),\n       after = c(125L, 113L, 115L, 117L, 105L, 112L, 100L, 103L, 116L, 104L)\n)rnorm(n = 2000, mean = 0, sd = 0.1^(1/2))test_species = aov(Sepal.Length~Species, data = iris)\nsummary(test_species)"},{"path":"int-samp-q.html","id":"solutions-2","chapter":"12 💻 Intermediate Sample Questions","heading":"12.4 solutions","text":"Answer Question 12.28:required load data case since package already\n. just need type boston.c find .\nneed extract RMThere nunber packages makes able compute skewness,\n: e1071, moments, PerformanceAnalytics etc. \nsuggest use moments. dont installed execute:use teh function skewnes RM_varAnswer Question 12.29:probability type error, probability rejecting H0 H0\ntrueAnswer Question 12.30:1 minus probability type II error, probability accepting\nH0 H0 true.Answer Question 12.31:probability accepting H0 H0 true.Answer Question 12.32:H0: \\(\\mu_{dept1} = \\mu_{dept2}\\) H1: \\(\\mu_{dept1} \\neq \\mu_{dept2}\\)Remember always test alternative hypothesis H1. pvalue\nt test statistically significant reject H1 \nconversely accept H0, case means (\ndifferent ’s randomness data, .e. sampling\nvariation).look p-value test see something like:\n0.4076, can conclude Two Sample t-test testing \ndifference dept_1 dept_2 (mean dept_1 = 5.56, mean \ndept_2 = 4.75) suggests effect positive, statistically \nsignificant, small. reject alt. hypo H1 accept H0.question tells can reject Null Hypo, \ncase since just accepted !Answer Question 12.33:exactly reasoning except \npaired t test. “talking individuals? \nchecking individuals pre treatment?” YESLook p-value: p-value = 0.0004646.really small. \ncan conclude Paired t-test testing difference \n(mean difference = -6.80) suggests effect \nnegative, statistically significant, large.Answer Question 12.34:Answer Question 12.35:Answer Question 12.36:first define vectors X Y executing:answer may look something like: (Welch, remember \ncheck variance rely default R behavior applying \ntransformation t.test) Sample t-test testing difference X\nY (mean x = 4.33, mean y = 3.33) suggests effect \npositive, statistically significant, large given pvalue\n0.0697. However also significant alpha\nlevel significance 10%.Answer Question 12.37:Note principal diag matrix 1s. \nvariables perfect correlation . just\ninterested upper triangle. might also interested \nvisualizing corrplot. Install \ninstall.packages(\"corrplot\") pass matrix argument\ncorrplot(cor(new_boston))Answer Question 12.38:test statistic calculated seeing, example, many times \nabsolute difference sample mean population mean\n(sm-mu) embodies standard error = sqrt[(known variance)/n]. \nvalue allow us standardize distribution allocate value \nNormal distribution (variance known) T di Student\ndistribution (variance unknown) - looking value can now\ncalculate probability within range values\nestablished level confidence statistical test.Answer Question 12.39:Answer Question 12.40:FALSEAnswer Question 12.42:simple correlation aight?!Answer Question 12.43:Please note correlation cor() can computed \nnumeric values. Looking iris see variable species \nfactor (aka grouping variable) used ANOVA aov() \ninterested comparing means across 2 groups. \nresult need select variables Species \ncor().’s another way filtering stuff, just deselect\nSpecies :also might want visualize correlation (advanced trick):Answer Question 12.44:’s another way filtering stuff, just deselect\nSpecies :Answer Question 12.45:Answer Question 12.46:Answer Question 12.47:Adjusted R2 corrected goodness--fit (model accuracy) measure linear models. identifies percentage\nvariance target field explained input \ninputs. R2 tends optimistically estimate fit linear\nregression. always increases number effects included \nmodel. Adjusted R2 attempts correct overestimation.\nAdjusted R2 might decrease specific effect improve \nmodel. guessed account number parameters \nalso get points, precisely talking \ndegrees freedom.\\(R_{adj}^2 = 1- \\frac{(1-R^2)(n-1)}{n-k-1}\\)account number degrees freedom!Answer Question 12.48:may know Multicollinearity problem can run \n’re fitting regression model, linear model. refers \npredictors correlated predictors model.\nUnfortunately, effects multicollinearity can feel murky \nintangible, makes unclear whether ’s important fix.\nMulticollinearity results unstable parameter estimates makes \ndifficult assess effect independent variables \ndependent variables.Let’s see another pov:Consider simplest case \\(Y\\) regressed \\(X\\) \\(Z\\)\n\\(Y = \\alpha + \\beta_1X +\\beta_2Z + \\epsilon\\) \\(Z\\) \n\\(Z\\) highly positively correlated. effect \\(X\\) \\(Y\\) \nhard distinguish effect \\(Z\\) \\(Y\\) increase\n\\(X\\) tends associated increase \\(Z\\). Now let’s also\nconsider th pathological case \\(X = Z\\) highlights .\n\\(Y = \\alpha + \\beta_1X + \\beta_2Z + \\epsilon\\) ->\n\\(Y = \\alpha + (\\beta_1 + \\beta_2)X + 0Z + \\epsilon\\) two\nvariables indistinguishable.\\(\\frac{1}{1-R^2}\\)Answer Question 12.49:Estimators become unstableAnswer Question 12.50:first attempt:see age days significant, indeed\nduration . However days havign .13 p values much \nsignificant age accounts .38 Let’s also check\ncollinearity uncorrectly specified model. look\ngood since values <10.mnay want see model, behaves cancelling age\nkeeping days, :iteration verify duration becomes even important\nsince now ***. However days just got worst. finally remove\n. don’t check vif() already done \nexpect subset non collinear varibales () now become\ncollinear. result:ùIn end retain durationAnswer Question 12.51:resulting 0.02618Answer Question 12.52:resulting -0.09918208you can also directly look summaryAnswer Question 12.53:resulting 88.97380549you can also directly look summaryAnswer Question 12.54:resulting duration p-value: 0.00183Answer Question 12.55:result : 0.02913Answer Question 12.56:look Species (already gone lecture)\ninspecting dataset. see Species three\ncategories setosa, versicolor virginica. like \ncompare means across 3 different categories can’t use\nt.test() since 3. Instead use ANOVA aov().\nSintax similar linear models. saw trying \ntackle “long” format data vs. “wide” format data.resulting 0.0000000000000002, significant. can conclude\n: ANOVA (formula: Sepal.Length ~ Species) suggests \nmain effect Species statistically significant large.Answer Question 12.57:take exact test look statisticHere look F statistics: F = 119.3","code":"library(spdep)RM_var = boston.c$RMinstall.packages(\"moments\")\nlibrary(moments)skewness(RM_var)t.test(x = perf_table$dept_1, y = perf_table$dept_2, paired = F, alternative = \"two.sided\")t.test(x = stores$before, y = stores$after, paired = T, alternative = \"two.sided\")rnorm(n = 2000, mean = 0, sd = 0.1^(1/2))boxplot(X)X = c(5,5,3,3,5,5)\nY = c(4,4,3,3,3,3)\nt.test(X, Y, alternative = \"two.sided\", paired = F)library(spdep)\nlibrary(dplyr)\nnew_boston = select(boston.c, MEDV, NOX, CRIM)\ncor(new_boston)t.test(X, Y, paired = F, alternative =\"less\", var.equal = T)cor(x = iris$Sepal.Length, y = iris$Sepal.Width)iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\ncor(iris_filtr)iris_filtr2 = select(iris, -Species)\ncor(iris_filtr2)library(corrplot)\niris_filtr2 = select(iris, -Species)\ncorrplot::corrplot(cor(iris_filtr2))\n    iris_filtr = select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\ncor(iris_filtr)iris_filtr2 = select(iris, -Species)\ncor(iris_filtr2)\n    var(iris$Sepal.Length)summary(iris$Sepal.Length)library(carData)\nlibrary(car)\ndata(\"Wong\")\nwong_regression = lm(piq ~ age + days + duration, data = Wong)\nsummary(wong_regression)wong_regression_2 = lm(piq ~ days + duration, data = Wong)wong_regression_3 = lm(piq ~ duration, data = Wong)summary(wong_regression_3)wong_regression_3$coefficients[2]summary(wong_regression_3)wong_regression_3$coefficients[1]summary(wong_regression_3)summary(wong_regression_3)summary(wong_regression_3)test_species = aov(Sepal.Length~Species, data = iris)\nsummary(test_species)summary(test_species)"},{"path":"course-performance.html","id":"course-performance","chapter":"13 ⚡ Course Performance","heading":"13 ⚡ Course Performance","text":"place can grasp previous colleagues performed past years course. Hopefully inspire better challenge grade expectation.\nData extracted Blackboard carefully anonymized securely stored preserve privacy.","code":""},{"path":"course-performance.html","id":"students-performance-analysis","chapter":"13 ⚡ Course Performance","heading":"13.1 🎓 24-25 Students Performance Analysis","text":"Based data 2024-2025 academic year, ’s comprehensive analysis student performance:","code":""},{"path":"course-performance.html","id":"overall-statistics","chapter":"13 ⚡ Course Performance","heading":"13.1.1 Overall Statistics","text":"\nTable 13.1: Summary Statistics 2024-2025 Academic Year\n","code":""},{"path":"course-performance.html","id":"grade-distribution","chapter":"13 ⚡ Course Performance","heading":"13.1.2 Grade Distribution","text":"","code":""},{"path":"course-performance.html","id":"intermediate-exam-performance","chapter":"13 ⚡ Course Performance","heading":"13.1.3 Intermediate Exam Performance","text":"\nTable 13.2: Intermediate Exam Performance\n","code":""},{"path":"course-performance.html","id":"performance-by-exam-type","chapter":"13 ⚡ Course Performance","heading":"13.1.4 Performance by Exam Type","text":"","code":""},{"path":"course-performance.html","id":"key-insights","chapter":"13 ⚡ Course Performance","heading":"13.1.5 Key Insights","text":"Overall Performance: mean final grade 2024-2025 academic year 23.2 30.Overall Performance: mean final grade 2024-2025 academic year 23.2 30.Pass Rate: 77.6% students achieved passing grade (≥18).Pass Rate: 77.6% students achieved passing grade (≥18).Grade Range: Final grades ranged 1.5 32.25.Grade Range: Final grades ranged 1.5 32.25.Intermediate Exams: Students performed similarly intermediate exams, slight variations.Intermediate Exams: Students performed similarly intermediate exams, slight variations.Distribution: grade distribution shows normal distribution students scoring middle range.Distribution: grade distribution shows normal distribution students scoring middle range.","code":""},{"path":"course-performance.html","id":"students","chapter":"13 ⚡ Course Performance","heading":"13.2 🎓 20-21 Students","text":"","code":""},{"path":"course-performance.html","id":"students-1","chapter":"13 ⚡ Course Performance","heading":"13.3 🎓 21-22 Students","text":"… soon analysed …","code":""},{"path":"course-performance.html","id":"students-2","chapter":"13 ⚡ Course Performance","heading":"13.4 🎓 22-23 Students","text":"… soon analysed …","code":""},{"path":"course-performance.html","id":"students-3","chapter":"13 ⚡ Course Performance","heading":"13.5 🎓 23-24 Students","text":"… soon analysed …","code":""},{"path":"frequently-asked-questions.html","id":"frequently-asked-questions","chapter":"14 ❓ Frequently Asked Questions","heading":"14 ❓ Frequently Asked Questions","text":"number frequent troubles students get frequently stucked . collection relative answer. generally suggest look online answer just coping pasting error message console Google. respected resource look trouble Stackovverflow.","code":""},{"path":"frequently-asked-questions.html","id":"q-na","chapter":"14 ❓ Frequently Asked Questions","heading":"14.1 Q n’A","text":"can’t really install  via install.packages(\"<package>\"), gives weird stuff cant undestrand.check syntax? spell correctly package name, capital letters matters know ?\nstill problem? try install source, cases packages source code lies GitHub. Look package author name : library(devtools) (still already installed package devtools run install.packages(\"devtools) execute library(devtools)). point run install_github(\"<github package author username>/<package name>\"). mentioned give fro granted: PLEASE SURE INTERNET CONNECTION ACCESS otherwise going dowload anything.file local inside R project execute read.csv(\"<path file>\") says R cant find files, ’s ?likely problem Working Directory ( check Working Directory means 4.3). Load library() (still already installed package , execute install.packages(\"\") library(devtools)). point run () verify file lies directory output.lectures recorded?\n> Generally Yes, lectures recorded made available enrolled students. might happen sometimes teacher forget start registration, notice please raise hand say !lectures recorded?\n> Generally Yes, lectures recorded made available enrolled students. might happen sometimes teacher forget start registration, notice please raise hand say !videos made available publicly?\n> can’t , sorry internal policyWill videos made available publicly?\n> can’t , sorry internal policyIs attendance mandatory?\n> won’t taking attendance expect see often class. love talking students understand , make sure get class, get feedback improve materials. class relatively small probably get know well.\ntime conflict can’t attend online lectures, please send us email let us know!attendance mandatory?\n> won’t taking attendance expect see often class. love talking students understand , make sure get class, get feedback improve materials. class relatively small probably get know well.\ntime conflict can’t attend online lectures, please send us email let us know!format class?\n> lectures, laboratories, discussion. Occasionally depending fast lectures might industry experts giving us tutorials themed speeches.format class?\n> lectures, laboratories, discussion. Occasionally depending fast lectures might industry experts giving us tutorials themed speeches.need know R course?\n> Since R gold standard statistics people, expect tutorials R even though fluency isn’t required, make life much easier course.need know R course?\n> Since R gold standard statistics people, expect tutorials R even though fluency isn’t required, make life much easier course.(group) assignments?\n> still discussing , evemntually make clear one month maxAre (group) assignments?\n> still discussing , evemntually make clear one month maxI question class. best way reach course staff?\n> please email teaching assistant, Dr. Niccolo Salvini related R laboratories, case drop email Prof. Giuseppe ArbiaI question class. best way reach course staff?\n> please email teaching assistant, Dr. Niccolo Salvini related R laboratories, case drop email Prof. Giuseppe ArbiaI can’t install package OneTwoSamples, ?\n> something incopatible R version built time current R version. cunfuses don’t bother going use . Instead going use base R hyp testing stats::t.test() make use infer, tidy statistical inferece.can’t install package OneTwoSamples, ?\n> something incopatible R version built time current R version. cunfuses don’t bother going use . Instead going use base R hyp testing stats::t.test() make use infer, tidy statistical inferece.","code":""},{"path":"StatisticsAndParameters.html","id":"StatisticsAndParameters","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15 Symbols, formulas, statistics and parameters","text":"","code":""},{"path":"StatisticsAndParameters.html","id":"symbols-and-standard-errors","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15.1 Symbols and standard errors","text":"\nTable 15.1: sample statistics used estimate population parameters. Empty table cells means studied textbook. dashes means formula given textbook.\n","code":""},{"path":"StatisticsAndParameters.html","id":"confidence-intervals","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15.2 Confidence intervals","text":"Almost confidence intervals form\\[\n    \\text{statistic} \\pm ( \\text{multiplier} \\times \\text{s.e.}(\\text{statistic})).\n\\]Notes:multiplier approximately 2 approximate 95% CI (based 68–95–99.7 rule).\\(\\text{multiplier} \\times \\text{s.e.}(\\text{statistic})\\) called margin error.Confidence intervals odds ratios slightly different, formula apply odds ratios.\nreason, standard error ORs given.","code":""},{"path":"StatisticsAndParameters.html","id":"hypothesis-testing","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15.3 Hypothesis testing","text":"many hypothesis tests, test statistic \\(t\\)-score, form:\\[\n  t = \\frac{\\text{statistic} - \\text{parameter}}{\\text{s.e.}(\\text{statistic})}.\n\\]Notes:Since \\(t\\)-scores little like \\(z\\)-scores, 68–95–99.7 rule can used approximate \\(P\\)-values.Tests involving odds ratios use \\(t\\)-scores, formula apply tests involving odds ratios.tests involving odds ratios, test statistic\n\\(\\chi^2\\) score \\(t\\)-score.\nreason, standard error ORs given.\\(\\chi^2\\) statistic approximately like \\(z\\)-score value (\\(\\text{df}\\) ‘degrees freedom’ given software output):\\[\n   \\sqrt{\\frac{\\chi^2}{\\text{df}}}.\n\\]","code":""},{"path":"StatisticsAndParameters.html","id":"other-formulas","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15.4 Other formulas","text":"estimate sample size needed estimating proportion: \\(\\displaystyle n = \\frac{1}{(\\text{Margin error})^2}\\).estimate sample size needed estimating mean: \\(\\displaystyle n = \\left( \\frac{2\\times s}{\\text{Margin error}}\\right)^2\\).calculate \\(z\\)-scores: \\(\\displaystyle z = \\frac{x - \\mu}{\\sigma}\\) , generally, \\(\\displaystyle z = \\frac{\\text{specific value variable} - \\text{mean variable}}{\\text{measure variable's variation}}\\).unstandardizing formula: \\(x = \\mu + (z\\times \\sigma)\\).Notes:sample size calculations, always round sample size found formulas.","code":""},{"path":"StatisticsAndParameters.html","id":"other-symbols-used","chapter":"15 Symbols, formulas, statistics and parameters","heading":"15.5 Other symbols used","text":"\nTable 15.2: symbols used\n","code":""},{"path":"appendixdatasets.html","id":"appendixdatasets","chapter":"16 Datasets","heading":"16 Datasets","text":"section provides information datasets used throughout course. datasets come various R packages commonly used statistical analysis data science education.","code":""},{"path":"appendixdatasets.html","id":"built-in-r-datasets","chapter":"16 Datasets","heading":"16.1 Built-in R Datasets","text":"","code":""},{"path":"appendixdatasets.html","id":"mtcars---motor-trend-car-road-tests","chapter":"16 Datasets","heading":"16.1.1 mtcars - Motor Trend Car Road Tests","text":"Package: datasetsDescription: Data extracted 1974 Motor Trend US magazine, comprises fuel consumption 10 aspects automobile design performance 32 automobiles (1973–74 models).Variables:\n- mpg: Miles/(US) gallon\n- cyl: Number cylinders\n- disp: Displacement (cu..)\n- hp: Gross horsepower\n- drat: Rear axle ratio\n- wt: Weight (1000 lbs)\n- qsec: 1/4 mile time\n- vs: Engine (0 = V-shaped, 1 = straight)\n- : Transmission (0 = automatic, 1 = manual)\n- gear: Number forward gears\n- carb: Number carburetors","code":""},{"path":"appendixdatasets.html","id":"iris---edgar-andersons-iris-data","chapter":"16 Datasets","heading":"16.1.2 iris - Edgar Anderson’s Iris Data","text":"Package: datasetsDescription: Famous dataset giving measurements centimeters variables sepal length width petal length width, respectively, 50 flowers 3 species iris.Variables:\n- Sepal.Length: Sepal length cm\n- Sepal.Width: Sepal width cm\n- Petal.Length: Petal length cm\n- Petal.Width: Petal width cm\n- Species: Species iris (setosa, versicolor, virginica)","code":""},{"path":"appendixdatasets.html","id":"cars---speed-and-stopping-distances-of-cars","chapter":"16 Datasets","heading":"16.1.3 cars - Speed and Stopping Distances of Cars","text":"Package: datasetsDescription: data give speed cars distances taken stop.Variables:\n- speed: Speed (mph)\n- dist: Stopping distance (ft)","code":""},{"path":"appendixdatasets.html","id":"course-specific-datasets","chapter":"16 Datasets","heading":"16.2 Course-Specific Datasets","text":"","code":""},{"path":"appendixdatasets.html","id":"bikeshare.csv---bike-sharing-data","chapter":"16 Datasets","heading":"16.2.1 bikeshare.csv - Bike Sharing Data","text":"File: data/bikeshare.csvDescription: Dataset containing bike sharing information various weather temporal features.Variables:\n- datetime: Date time\n- season: Season (1:spring, 2:summer, 3:fall, 4:winter)\n- holiday: Whether day holiday \n- workingday: day neither weekend holiday\n- weather: Weather situation (1: Clear, 2: Mist, 3: Light Snow/Rain, 4: Heavy Rain)\n- temp: Temperature Celsius\n- atemp: “Feels like” temperature Celsius\n- humidity: Relative humidity\n- windspeed: Wind speed\n- casual: Count casual users\n- registered: Count registered users\n- count: Count total rental bikes","code":""},{"path":"appendixdatasets.html","id":"stroke_data.csv---stroke-prediction-data","chapter":"16 Datasets","heading":"16.2.2 stroke_data.csv - Stroke Prediction Data","text":"File: data/stroke_data.csvDescription: Dataset predicting stroke occurrence based various health indicators.Variables:\n- id: Unique identifier\n- gender: Gender\n- age: Age\n- hypertension: Hypertension (0: , 1: Yes)\n- heart_disease: Heart disease (0: , 1: Yes)\n- ever_married: Ever married (, Yes)\n- work_type: Type work\n- Residence_type: Residence type (Rural, Urban)\n- avg_glucose_level: Average glucose level\n- bmi: Body mass index\n- smoking_status: Smoking status\n- stroke: Stroke (0: , 1: Yes)","code":""},{"path":"appendixdatasets.html","id":"coronary.dta---coronary-heart-disease-data","chapter":"16 Datasets","heading":"16.2.3 coronary.dta - Coronary Heart Disease Data","text":"File: data/coronary.dtaDescription: Dataset containing information coronary heart disease patients.Variables:\n- Various medical demographic variables related coronary heart disease\n- Used survival analysis medical statistics","code":""},{"path":"appendixdatasets.html","id":"how-to-access-datasets","chapter":"16 Datasets","heading":"16.3 How to Access Datasets","text":"","code":""},{"path":"appendixdatasets.html","id":"built-in-r-datasets-1","chapter":"16 Datasets","heading":"16.3.1 Built-in R Datasets","text":"","code":"\n# Load built-in datasets\ndata(mtcars)\ndata(iris)\ndata(cars)\n\n# View dataset structure\nstr(mtcars)\nhead(mtcars)"},{"path":"appendixdatasets.html","id":"course-datasets","chapter":"16 Datasets","heading":"16.3.2 Course Datasets","text":"","code":"\n# Load course datasets\nbikeshare <- read.csv(\"data/bikeshare.csv\")\nstroke_data <- read.csv(\"data/stroke_data.csv\")\n\n# View dataset structure\nstr(bikeshare)\nhead(bikeshare)"},{"path":"appendixdatasets.html","id":"dataset-usage-in-course","chapter":"16 Datasets","heading":"16.4 Dataset Usage in Course","text":"mtcars: Used linear regression examples correlation analysisiris: Used classification, clustering, ANOVA examplescars: Used simple linear regression demonstrationsbikeshare: Used time series analysis multiple regressionstroke_data: Used logistic regression classification examplescoronary: Used survival analysis examples","code":""},{"path":"appendixdatasets.html","id":"additional-resources","chapter":"16 Datasets","heading":"16.5 Additional Resources","text":"information datasets:\n- R documentation: ?mtcars, ?iris, ?cars\n- Course materials exercises\n- Statistical analysis examples course chapters","code":""},{"path":"references-2.html","id":"references-2","chapter":"References","heading":"References","text":"","code":""},{"path":"references-2.html","id":"statistical-methods-and-r-programming","chapter":"References","heading":"16.6 Statistical Methods and R Programming","text":"Everitt, B., Hothorn, T. (2011). Introduction Applied Multivariate Analysis R. Springer-Verlag.Everitt, B., Hothorn, T. (2011). Introduction Applied Multivariate Analysis R. Springer-Verlag.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2015). Introduction Statistical Learning, Applications R. Springer.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2015). Introduction Statistical Learning, Applications R. Springer.Timbers, T., Campbell, T., & Lee, M. (2022). Data Science: First Introduction. Online versionTimbers, T., Campbell, T., & Lee, M. (2022). Data Science: First Introduction. Online versionWickham, H., & Grolemund, G. (2018). R Data Science. O’Reilly. Freely available onlineWickham, H., & Grolemund, G. (2018). R Data Science. O’Reilly. Freely available onlineDauber, D. (2022). R non-programmers. Free bookDauber, D. (2022). R non-programmers. Free book","code":""},{"path":"references-2.html","id":"advanced-r-programming","chapter":"References","heading":"16.7 Advanced R Programming","text":"Higgins, P. D. R. (2022). Reproducible Medical Research R. Free bookHiggins, P. D. R. (2022). Reproducible Medical Research R. Free bookArmstrong, J. K. (2022). Fundamentals Wrangling Healthcare Data R. Free bookArmstrong, J. K. (2022). Fundamentals Wrangling Healthcare Data R. Free bookWickham, H. (2015). Advanced R. CRC Press. Free bookWickham, H. (2015). Advanced R. CRC Press. Free book","code":""},{"path":"references-2.html","id":"statistical-software-and-tools","chapter":"References","heading":"16.8 Statistical Software and Tools","text":"R Core Team (2024). R: language environment statistical computing. R Foundation Statistical Computing, Vienna, Austria.R Core Team (2024). R: language environment statistical computing. R Foundation Statistical Computing, Vienna, Austria.RStudio Team (2024). RStudio: Integrated Development Environment R. RStudio, PBC, Boston, MA.RStudio Team (2024). RStudio: Integrated Development Environment R. RStudio, PBC, Boston, MA.Xie, Y., Allaire, J. J., & Grolemund, G. (2018). R Markdown: Definitive Guide. Chapman Hall/CRC.Xie, Y., Allaire, J. J., & Grolemund, G. (2018). R Markdown: Definitive Guide. Chapman Hall/CRC.","code":""},{"path":"references-2.html","id":"course-materials-1","chapter":"References","heading":"16.9 Course Materials","text":"Salvini, N. (2025). Statistics & Big Data 25-26 Labs. Course website materials.Salvini, N. (2025). Statistics & Big Data 25-26 Labs. Course website materials.Dabo-Niang, S. (2025). Advanced Modeling Techniques. Intensive session materials.Dabo-Niang, S. (2025). Advanced Modeling Techniques. Intensive session materials.","code":""},{"path":"references-2.html","id":"additional-resources-1","chapter":"References","heading":"16.10 Additional Resources","text":"CRAN Task Views: https://cran.r-project.org/web/views/CRAN Task Views: https://cran.r-project.org/web/views/R-bloggers: https://www.r-bloggers.com/R-bloggers: https://www.r-bloggers.com/Stack Overflow R Tag: https://stackoverflow.com/questions/tagged/rStack Overflow R Tag: https://stackoverflow.com/questions/tagged/rR Documentation: https://www.rdocumentation.org/R Documentation: https://www.rdocumentation.org/","code":""}]
