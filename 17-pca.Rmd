# üíª PCA {#pca}

> **‚ö†Ô∏è Note on Slides**: If you encounter broken links in the PDF slides (`module_2_PCA_&_CA.pdf`), please refer to the "Online Resources and References" section at the end of this chapter for updated and working links.

## Introduction to Principal Component Analysis

Principal Component Analysis (PCA) is a powerful dimensionality reduction technique used to transform a large set of variables into a smaller set of uncorrelated variables called principal components. These components capture the maximum variance in the data while reducing complexity.

### Key Concepts

- **Dimensionality Reduction**: Reduce the number of variables while retaining most of the information
- **Variance Preservation**: Principal components are ordered by the amount of variance they explain
- **Uncorrelated Components**: Each principal component is orthogonal (uncorrelated) to the others
- **Data Standardization**: Often necessary when variables are on different scales

### When to Use PCA

- When you have many correlated variables
- To visualize high-dimensional data in 2D or 3D
- To reduce noise in the data
- Before applying other machine learning algorithms
- To identify the most important features in your dataset

## Required Packages

We will use the following packages throughout this chapter:

```{r setup-pca, message=FALSE, warning=FALSE}
library(FactoMineR)  # For PCA analysis
library(factoextra)  # For visualization
library(ISLR2)       # For datasets
```

## 17.1 PCA with FactoMineR Package

### 17.1.1 Example 1: Simple 2D Dataset

Let's start with a simple example to understand the fundamentals of PCA.

#### Step 1: Create and Explore the Data

```{r exercise1-data}
X <- data.frame(
  Var1 = c(2, 1, -1, -2),
  Var2 = c(2, -1, 1, -2)
)
rownames(X) <- c("i1", "i2", "i3", "i4")
X
```

#### Step 2: Calculate Covariance Matrix and Inertia

```{r covariance}
# Calculate means
mean(X[,1]) # mean of Var1
mean(X[,2]) # mean of Var2

# Variance-covariance matrix
# The constant (n-1)/n adjusts for the variance-covariance matrix used in lectures
S <- var(X) * (3/4)
S

# Inertia (sum of diagonal elements = sum of variances)
Inertia <- sum(diag(S))
Inertia
```

#### Step 3: Eigen-analysis on Covariance Matrix

```{r eigen-analysis}
eigen(S) # gives the eigen-values and eigen-vectors
```

#### Step 4: Eigen-analysis on Correlation Matrix

```{r eigen-correlation}
# Correlation matrix
R <- cor(X)
eigenan <- eigen(R) # eigen analysis of R
eigenan

# Sum of eigenvalues equals p (number of variables)
sum(eigenan$values)

# Normalized data (centered and scaled)
Z <- scale(X)
var(Z) # is the correlation matrix
```

#### Step 5: Perform PCA on Covariance Matrix

PCA with the covariance matrix uses only centered data. For PCA on the correlation matrix (normed PCA), use `scale.unit = TRUE` (default option).

**Note on Correlation**: The correlation between two variables $X_1$ and $X_2$ is:
$$\rho = \frac{cov(X_1, X_2)}{\sigma_{X_1} \sigma_{X_2}}$$

where $cov(X_1, X_2)$ is the covariance, and $\sigma_{X_1} = \sqrt{Var(X_1)}$ is the standard deviation of $X_1$.

```{r pca-covariance}
res.pca.cov <- PCA(X, scale.unit = FALSE, graph = FALSE)
print(res.pca.cov)
```

#### Step 6: Examine Eigenvalues

We have $p = 2 = min(n-1, p) = min(3, 2)$ eigenvalues: 4 and 1. The inertia $Inertia = 4 + 1 = 5$ is the sum of the variances of the variables.

```{r eigenvalues}
res.pca.cov$eig
```

#### Step 7: Examine Variables and Individuals

```{r variables}
res.pca.cov$var
```

```{r individuals}
res.pca.cov$ind
```

### 17.1.2 Example 2: Student Grades Dataset

Now let's work with a more realistic example: student grades in three subjects.

#### Step 1: Prepare the Data

```{r another-example}
A <- matrix(c(9,12,10,15,9,10,5,10,8,11,13,14,11,13,8,3,15,10), 
            nrow=6, byrow=TRUE)
A

Nframe <- as.data.frame(A)

# Set row and column names
m1 <- c("Alex", "Bea", "Claudio", "Damien", "Emilie", "Fran")
m2 <- c("Biostatistics", "Economics", "English")

rownames(A) <- m1
colnames(A) <- m2
head(A)
```

#### Step 2: Perform PCA on Correlation Matrix

```{r pca-correlation}
res.pca.cor <- PCA(A, scale.unit = TRUE, graph = FALSE)
print(res.pca.cor)
```

#### Step 3: Examine Eigenvalues

```{r eigenvalues-correlation}
res.pca.cor$eig
get_eigenvalue(res.pca.cor)
```

**Interpretation**: Kaiser's rule suggests $q = 2$ components because the eigenvalue mean is 1 (with 89% of explained variance). The rule of thumb gives $q = 2$ because the first 2 dimensions explain 89% of the variance/inertia.

#### Step 4: Examine Variables

```{r variables-correlation}
res.pca.cor$var
```

#### Step 5: Correlations Between Variables and Components

```{r correlations}
res.pca.cor$var$cor
```

**Interpretation**: The first axis is correlated with Biostatistics (+0.86) and Economics (-0.86). The second axis is correlated to English (0.96). The two components are correlated with at least one variable, so $q = 2$ may be considered to reduce the dimension from $p = 3$.

#### Step 6: Coordinates of Variables

```{r coordinates-vars}
res.pca.cor$var$coord
```

#### Step 7: Quality of Representation (cos¬≤)

```{r quality-vars}
res.pca.cor$var$cos2
```

**Interpretation**: Biostatistics and Economics are well represented in the first dimension (75%), while English is very well represented in the second axis (93%). In the first plane (Dim.1 and Dim.2), Biostatistics and Economics are well represented (75% + 12.5% = 87.5%), and English is very well represented (93% + 0% = 93%).

#### Step 8: Contributions of Variables

```{r contributions-vars}
res.pca.cor$var$contrib
```

**Interpretation**: Biostatistics and Economics contribute to the construction of the first dimension (50%), while English contributes highly to the construction of the second axis (78.8%). In the first plane, the contribution of Biostatistics and Economics is 50% + 10.5% = 60.5%, and that of English is 78.8%.

#### Step 9: Description of Dimensions

```{r description-dim1}
dimdesc(res.pca.cor, axes = 1)
```

#### Step 10: Contributions of First Two Dimensions

```{r contributions-first-two}
res.pca.cor$var$contrib[, 1:2]
```

### 17.1.3 Visualizations

#### Variables Plot

```{r plot-vars, fig.width=8, fig.height=6}
fviz_pca_var(res.pca.cor, col.var = "black")
```

#### Variables Plot with Quality of Representation

```{r plot-vars-quality, fig.width=8, fig.height=6}
fviz_pca_var(res.pca.cor, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

#### Variables with High Quality of Representation (cos¬≤ > 0.6)

```{r plot-vars-quality-filter, fig.width=8, fig.height=6}
fviz_pca_var(res.pca.cor, select.var = list(cos2 = 0.6))
```

#### Variables Plot with Contributions

```{r plot-vars-contrib, fig.width=8, fig.height=6}
fviz_pca_var(res.pca.cor, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```

#### Biplot: Variables and Individuals

```{r plot-biplot, fig.width=8, fig.height=6}
fviz_pca_biplot(res.pca.cor, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
)
```

### 17.1.4 Analysis of Individuals

#### Individuals Information

```{r individuals-correlation}
res.pca.cor$ind
```

#### Individuals: Coordinates

```{r individuals-coords}
res.pca.cor$ind$coord
```

#### Individuals: Quality of Representation

```{r individuals-quality}
res.pca.cor$ind$cos2
```

#### Individuals: Contributions

```{r individuals-contrib}
res.pca.cor$ind$contrib
```

#### Individuals Plot with Contributions

```{r plot-ind-contrib, fig.width=8, fig.height=6}
fviz_pca_ind(res.pca.cor, col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```

#### Individuals Plot with Quality of Representation

```{r plot-ind-quality, fig.width=8, fig.height=6}
fviz_pca_ind(res.pca.cor, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

### 17.1.5 Exporting Results

#### Save Figures as PDF

```{r save-pdf, eval=FALSE}
pdf("PCA.pdf")
fviz_pca_var(res.pca.cor)
fviz_pca_ind(res.pca.cor)
dev.off()
```

#### Save Figures as PNG

```{r save-png, eval=FALSE}
png("PCA%03d.png", width = 800, height = 600)
fviz_pca_var(res.pca.cor)
fviz_pca_ind(res.pca.cor)
dev.off()
```

#### Export Results to Text Files

```{r export-txt, eval=FALSE}
write.table(res.pca.cor$eig, "eigenvalues.txt", sep = "\t")
write.table(res.pca.cor$var$coord, "variables_coordinates.txt", sep = "\t")
write.table(res.pca.cor$ind$coord, "individuals_coordinates.txt", sep = "\t")
```

## 17.2 PCA with Base R: prcomp() Function

The `prcomp()` function is part of base R and provides an alternative way to perform PCA. Let's use the `USArrests` dataset, which contains crime statistics for the 50 US states.

### 17.2.1 Data Exploration

```{r usarrests-data}
# Load and examine the data
head(USArrests)
rownames(USArrests)
```

The dataset contains four variables:

- **Murder**: Murder arrests (per 100,000)
- **Assault**: Assault arrests (per 100,000)
- **UrbanPop**: Percent urban population
- **Rape**: Rape arrests (per 100,000)

```{r usarrests-vars}
colnames(USArrests)
```

### 17.2.2 Data Preprocessing: Why Standardization Matters

Notice that the variables have vastly different means. The `apply()` function with option `2` calculates statistics for each column (option `1` does it by row):

```{r usarrests-means}
apply(USArrests, 2, mean)
```

There are on average three times as many rapes as murders, and more than eight times as many assaults as rapes. Let's examine the variances:

```{r usarrests-variances}
apply(USArrests, 2, var)
```

**Important**: The variables have very different variances. The `UrbanPop` variable (percentage) is not comparable to the number of arrests per 100,000. If we don't scale the variables before performing PCA, most principal components would be driven by the `Assault` variable, since it has the largest mean and variance. **It is crucial to standardize variables to have mean zero and standard deviation one before performing PCA.**

### 17.2.3 Performing PCA with prcomp()

The option `scale = TRUE` scales the variables to have standard deviation one.

```{r prcomp}
pr.out <- prcomp(USArrests, scale = TRUE)
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard deviations used for scaling:

```{r prcomp-center-scale}
pr.out$center
pr.out$scale
```

### 17.2.4 Principal Component Loadings

The rotation matrix provides the principal component loadings; each column contains the corresponding principal component loading vector.

```{r prcomp-rotation}
pr.out$rotation
```

We see that there are four distinct principal components. This is expected because there are generally $min(n-1, p)$ informative principal components in a dataset with $n$ observations and $p$ variables.

### 17.2.5 Principal Component Scores

Using `pr.out$x`, we have the $50 \times 4$ matrix of principal component score vectors. The $k$th column is the $k$th principal component score vector.

```{r prcomp-scores}
dim(pr.out$x)
head(pr.out$x)
```

### 17.2.6 Visualizing Results

We can plot the first two principal components as follows:

```{r plot-prcomp, fig.width=8, fig.height=6}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument ensures that the arrows are scaled to represent the loadings; other values give slightly different biplots with different interpretations.

**Note**: Principal components are only unique up to a sign change. We can reproduce the figure by changing signs:

```{r prcomp-sign-change}
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0)
```

### 17.2.7 Variance Explained

The standard deviation (square root of the corresponding eigenvalue) of each principal component:

```{r prcomp-sdev}
pr.out$sdev
```

The variance explained by each principal component (corresponding eigenvalue) is obtained by squaring these:

```{r prcomp-variance}
pr.var <- pr.out$sdev^2
pr.var
```

Compute the proportion of variance explained by each principal component:

```{r prcomp-pve}
pve <- pr.var / sum(pr.var)
pve
```

We see that the first principal component explains 62.0% of the variance, and the next explains 24.7%. Let's plot the PVE (Proportion of Variance Explained) and cumulative PVE:

```{r plot-pve, fig.width=10, fig.height=5}
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
par(mfrow = c(1, 1))
```

The function `cumsum()` computes the cumulative sum of the elements of a numeric vector.

## 17.3 Practical Exercises

### Exercise 1: US Cereals Dataset

Consider the `UScereal` dataset from the `MASS` package (65 rows and 11 columns), which contains nutritional and marketing information from the 1993 ASA Statistical Graphics Exposition. The data have been normalized to a portion of one American cup.

**Tasks:**

1. Load the dataset and examine its structure
2. Identify which variables are quantitative and suitable for PCA
3. Perform PCA on the quantitative variables
4. Interpret the results: how many components should be retained?
5. Create visualizations to understand the relationships between variables

```{r exercise-cereal, eval=FALSE}
library(MASS)
data(UScereal)
# Note: Some variables are not quantitative
# Select only quantitative variables before performing PCA
# res.pca.cereal <- PCA(UScereal[, quantitative_cols], scale.unit = TRUE, graph = FALSE)
```

### Exercise 2: NCI Cancer Cell Line Data

Consider the NCI cancer cell line microarray data from `ISLR2`, which consists of 6,830 gene expression measurements on 64 cancer cell lines.

**Tasks:**

1. Load the dataset
2. Perform PCA on the gene expression data
3. Visualize the first few principal components
4. Check if the cancer types (given in `nci.labs`) cluster together in the PCA space

```{r exercise-nci, eval=FALSE}
library(ISLR2)
data(NCI60)
# Each cell line is labeled with a cancer type, given in nci.labs
# Perform PCA and check if cancer types cluster in the first two dimensions
```

### Exercise 3: Wine Quality Analysis

Consider the wine dataset from the `gclus` package, which contains chemical analyses of wines grown in the same region in Italy but derived from three different cultivars. The dataset has 13 variables and over 170 observations.

**Tasks:**

(a) Perform PCA on the wine dataset. Remember to standardize the variables as they are on different scales.

```{r exercise-wine, eval=FALSE}
library(gclus)
data(wine)
library(FactoMineR)
# Perform PCA
res.pca.wine <- PCA(wine, scale.unit = TRUE, graph = FALSE)
```

(b) Interpret the PCA results. Focus on:
- Which chemical properties contribute most to the variance?
- Do the wines cluster by cultivar?
- How many principal components should be retained?

### Exercise 4: Boston Housing Data

Consider the Boston dataset from the `MASS` package, which contains information collected by the U.S. Census Service concerning housing in the area of Boston, Massachusetts. It has 506 rows and 14 columns.

**Tasks:**

(a) Conduct PCA on the Boston housing dataset. Before performing PCA:
- Assess which variables are most suitable for the analysis
- Handle any missing values
- Preprocess the data accordingly

```{r exercise-boston, eval=FALSE}
library(MASS)
data(Boston)
library(FactoMineR)
# Select appropriate variables and perform PCA
res.pca.boston <- PCA(Boston, scale.unit = TRUE, graph = FALSE)
```

(b) Interpret the results. Look for patterns indicating relationships between:
- Crime rates
- Property tax
- Median value of owner-occupied homes
- Other housing characteristics

### General Guidelines for Solving Exercises

- **Data Preprocessing**: Before performing PCA, it's crucial to:
  - Handle missing values appropriately
  - Standardize the data (especially when variables are on different scales)
  - Select relevant quantitative variables
  
- **PCA Interpretation**: When interpreting results, focus on:
  - Eigenvalues and proportion of variance explained
  - Loadings of variables on principal components
  - Quality of representation (cos¬≤)
  - Contributions of variables and individuals
  
- **Visualization**: Use plots to aid interpretation:
  - Scree plots (eigenvalues)
  - Biplots (variables and individuals together)
  - Variable plots (with contributions or quality)
  - Individual plots
  
- **Contextual Understanding**: Understanding the domain context can significantly help in interpreting results meaningfully. Always relate statistical findings back to the real-world problem you're solving.

## Online Resources and References

### Official Documentation

- **FactoMineR Package**: 
  - [CRAN page](https://cran.r-project.org/package=FactoMineR)
  - [GitHub repository](https://github.com/husson/FactoMineR)
  - [Official website](http://factominer.free.fr/)
  
- **factoextra Package**: 
  - [CRAN page](https://cran.r-project.org/package=factoextra)
  - [Documentation](http://www.sthda.com/english/rpkgs/factoextra/)

### R Markdown Resources (as mentioned in slides)

- **R Markdown Cheat Sheet**: [RStudio Cheatsheets](https://www.rstudio.com/resources/cheatsheets/) - Download the R Markdown cheat sheet
- **R Markdown Guide**: [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/)
- **RStudio**: [Download RStudio](https://www.rstudio.com/products/rstudio/download/)
- **R Markdown Tutorial**: [R Markdown Tutorial](https://rmarkdown.rstudio.com/lesson-1.html)

### Tutorials and Guides

- **PCA Tutorial**: [STHDA - Principal Component Analysis in R](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)
- **CA Tutorial**: [STHDA - Correspondence Analysis in R](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/113-ca-correspondence-analysis-in-r-the-essentials/)
- **R-bloggers**: Search for PCA and CA articles at [R-bloggers.com](https://www.r-bloggers.com/)

### Interactive Tools

- **PCA Visualization**: [PCA Explorer](https://huygens.science.uva.nl/) - Interactive PCA visualization tool
- **R Documentation**: [rdocumentation.org](https://www.rdocumentation.org/) - Search for PCA and CA functions

### Books and Academic Resources

- **An Introduction to Applied Multivariate Analysis with R** by Everitt & Hothorn
- **Principal Component Analysis** by Jolliffe (classic textbook)
- **Correspondence Analysis in Practice** by Greenacre

### Quick Reference

- **FactoMineR Help**: Type `?PCA` or `?CA` in R console after loading the package
- **factoextra Help**: Type `?fviz_pca_var` or `?fviz_ca_biplot` in R console

**‚ö†Ô∏è Important**: If you encounter broken links in the PDF slides (`module_2_PCA_&_CA.pdf`), all the links above are verified and working. Use this section as your reference for online resources.
